{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf2e3b77",
   "metadata": {},
   "source": [
    "# Exercise 1 — Starter\n",
    "\n",
    "Build on the **demo** by exploring: (A) generation length, (B) numerical precision, and (C) KV cache."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b760f2f5",
   "metadata": {},
   "source": [
    "## 0) Environment guard\n",
    "\n",
    "#### Built on top of the Demo\n",
    "Avoid NumPy 2.x + TensorFlow wheel conflicts by disabling optional TF/Flax imports in `transformers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5c1574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c5335a",
   "metadata": {},
   "source": [
    "## 1) Imports & globals\n",
    "\n",
    "#### Built on top of the Demo\n",
    "If the preferred model isn't available, we fall back to a tiny model to keep the notebook runnable everywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ba73ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Dict, Any, List\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device={device}; NumPy={np.__version__}; Torch={torch.__version__}\")\n",
    "\n",
    "PREFERRED_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "DEFAULT_PROMPT = (\n",
    "        \"Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast to the natural \"\n",
    "        \"intelligence displayed by humans and animals. Leading AI textbooks define the field as the study \"\n",
    "        \"of intelligent agents: any system that perceives its environment and takes actions that maximize \"\n",
    "        \"its chance of achieving its goals. Colloquially, the term \\\"artificial intelligence\\\" is often \"\n",
    "        \"used to describe machines that mimic cognitive functions that humans associate with the human mind, \"\n",
    "        \"such as learning and problem-solving.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447515ab",
   "metadata": {},
   "source": [
    "## 2) Helpers \n",
    "\n",
    "#### Built on top of the Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb97ec0-bd7e-45e7-ae20-357059dc0685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _resolve_dtype(dtype_str: str, device: torch.device):\n",
    "    \"\"\"\n",
    "      1) Normalize/validate `dtype_str` (accept 'float16' or 'float32'; case-insensitive is fine).\n",
    "         - If unsupported, raise ValueError with a clear message.\n",
    "\n",
    "      2) CPU guard:\n",
    "         - If device.type == 'cpu' and requested dtype is float16, fall back to torch.float32\n",
    "           (most CPUs don’t support fp16 efficiently).\n",
    "         - Return both: (torch.float32, \"float32 (forced on CPU)\").\n",
    "\n",
    "      3) Otherwise map the string to the torch dtype:\n",
    "         - 'float16' -> torch.float16\n",
    "         - 'float32' -> torch.float32\n",
    "\n",
    "      4) Return a tuple: (resolved_torch_dtype, label_string), where label_string is\n",
    "         a human-readable descriptor like 'float16' or 'float32 (forced on CPU)'.\n",
    "    \"\"\"\n",
    "    if device.type == 'cpu' and dtype_str == 'float16':\n",
    "        return torch.float32, 'float32 (forced on CPU)'\n",
    "    return (torch.float16 if dtype_str == 'float16' else torch.float32), dtype_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4270be2e-48d2-4359-9044-e6cacd4c1a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name: str, dtype_str: str = \"float16\"):\n",
    "    \"\"\"\n",
    "      1) Resolve the requested precision:\n",
    "         - Call `_resolve_dtype(dtype_str, device)` to get `(torch_dtype, dtype_label)`.\n",
    "         - Ensure CPU requests for 'float16' are coerced to float32.\n",
    "\n",
    "      2) Build a candidate list of model ids:\n",
    "         - Start with `[model_name]`.\n",
    "         - (Optional) Append `FALLBACK_MODELS` so the function still works if the preferred model\n",
    "           isn’t available in the environment.\n",
    "\n",
    "      3) Try candidates in order:\n",
    "         for name in candidates:\n",
    "           try:\n",
    "             - tokenizer = AutoTokenizer.from_pretrained(name, use_fast=True)\n",
    "             - model = AutoModelForCausalLM.from_pretrained(name, torch_dtype=torch_dtype)\n",
    "             - Move model to `device` and call `model.eval()`\n",
    "             - If `tokenizer.pad_token_id` is None and `tokenizer.eos_token_id` exists,\n",
    "               set `tokenizer.pad_token = tokenizer.eos_token` to avoid padding warnings.\n",
    "             - RETURN `(tokenizer, model, name, dtype_label)`\n",
    "           except Exception as e:\n",
    "             - Collect the error and continue to the next candidate.\n",
    "\n",
    "      4) If all candidates fail:\n",
    "         - Raise `RuntimeError` with a concise message and include the first/last error\n",
    "           to aid debugging.\n",
    "\n",
    "    Notes:\n",
    "      - You may pass `low_cpu_mem_usage=True` or `device_map=\"auto\"` (if appropriate for your runtime).\n",
    "      - Keep `trust_remote_code=False` unless you explicitly need custom modeling code.\n",
    "    \"\"\"\n",
    "    dtype, label = _resolve_dtype(dtype_str, device)\n",
    "    for name in [model_name]:\n",
    "        try:\n",
    "            tok = AutoTokenizer.from_pretrained(name, use_fast=True)\n",
    "            mdl = AutoModelForCausalLM.from_pretrained(name, torch_dtype=dtype)\n",
    "            mdl.to(device)\n",
    "            mdl.eval()\n",
    "            if tok.pad_token_id is None and tok.eos_token_id is not None:\n",
    "                tok.pad_token = tok.eos_token\n",
    "            return tok, mdl, name, label\n",
    "        except Exception:\n",
    "            continue\n",
    "    raise RuntimeError('Could not load any model from preferred/fallback list.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6de2f0-b0b2-4ca0-b2f5-242259adca33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 50,\n",
    "    use_cache: bool = True,\n",
    "    num_warmup: int = 1,\n",
    "    num_runs: int = 3,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "      1) Tokenize the prompt:\n",
    "         - Create `input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(model.device)`.\n",
    "\n",
    "      2) Warm up (no timing):\n",
    "         - Under `torch.no_grad()`, run `model.generate(input_ids, max_new_tokens=8, use_cache=use_cache)`\n",
    "           `num_warmup` times to stabilize kernels/clocks.\n",
    "         - If CUDA, you may `torch.cuda.synchronize()` after warmup.\n",
    "\n",
    "      3) Timed runs:\n",
    "         - Initialize `latencies = []` and `tokens_out = []`.\n",
    "         - Under `torch.no_grad()`, loop `num_runs` times:\n",
    "             * If CUDA, `torch.cuda.synchronize()` before starting the timer.\n",
    "             * Record `t0 = time.perf_counter()`.\n",
    "             * Call `out = model.generate(input_ids, max_new_tokens=max_new_tokens, use_cache=use_cache)`.\n",
    "             * If CUDA, `torch.cuda.synchronize()` to ensure all work has finished.\n",
    "             * Record `t1 = time.perf_counter()` and append `(t1 - t0)` to `latencies`.\n",
    "             * Compute generated token count: `gen = out.shape[-1] - input_ids.shape[-1]`\n",
    "               and append to `tokens_out`.\n",
    "\n",
    "      4) Aggregate metrics:\n",
    "         - `L = average(latencies)` (seconds)\n",
    "         - `TG = average(tokens_out)` (tokens)\n",
    "         - `TPS = TG / L` (tokens/sec), guard divide-by-zero\n",
    "         - `LPT = (L / max(TG, 1e-9)) * 1000.0` (ms/token)\n",
    "\n",
    "      5) Return a dict with keys:\n",
    "         - 'total_latency_s', 'tokens_generated', 'tokens_per_sec', 'avg_latency_per_token_ms'\n",
    "\n",
    "      Notes:\n",
    "        - Ensure `model.eval()` was called beforehand.\n",
    "        - Do all forwards under `torch.no_grad()` to avoid autograd overhead.\n",
    "        - We measure **end-to-end** generation time for the given `max_new_tokens`.\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(model.device)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_warmup):\n",
    "            _ = model.generate(input_ids, max_new_tokens=8, use_cache=use_cache)\n",
    "    latencies, tokens_out = [], []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            t0 = time.perf_counter()\n",
    "            out = model.generate(input_ids, max_new_tokens=max_new_tokens, use_cache=use_cache)\n",
    "            t1 = time.perf_counter()\n",
    "            latencies.append(t1 - t0)\n",
    "            tokens_out.append(out.shape[-1] - input_ids.shape[-1])\n",
    "    L = sum(latencies) / max(len(latencies), 1)\n",
    "    TG = sum(tokens_out) / max(len(tokens_out), 1)\n",
    "    TPS = (TG / L) if L > 0 else float('nan')\n",
    "    LPT = (L / max(TG, 1e-9)) * 1000.0\n",
    "    return {'total_latency_s': L, 'tokens_generated': TG, 'tokens_per_sec': TPS, 'avg_latency_per_token_ms': LPT}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb22eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_xy(xs: List[float], ys: List[float], xlabel: str, ylabel: str, title: str):\n",
    "    plt.figure()\n",
    "    plt.plot(xs, ys, marker='o')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf2b066",
   "metadata": {},
   "source": [
    "## 3) Baseline\n",
    "Run one fixed setup as baseline: `max_new_tokens=50`, `float16`, `use_cache=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013825a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model, MODEL_USED, DTYPE_USED = load_model_and_tokenizer(PREFERRED_MODEL, dtype_str='float16')\n",
    "print(f\"Loaded: {MODEL_USED} | dtype={DTYPE_USED}\")\n",
    "baseline = time_generate(model, tokenizer, DEFAULT_PROMPT, max_new_tokens=50, use_cache=True)\n",
    "assert all(k in baseline for k in ['total_latency_s','tokens_generated','tokens_per_sec','avg_latency_per_token_ms'])\n",
    "baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc54275b",
   "metadata": {},
   "source": [
    "## 4) Exercise A — Generation Length vs Cost\n",
    "Vary `max_new_tokens` and plot latency/throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e6ad25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_for_varied_lengths():\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      1) Define a list of generation lengths, e.g. lengths = [10, 50, 100, 250].\n",
    "\n",
    "      2) For each L in lengths:\n",
    "         - Call time_generate(model, tokenizer, DEFAULT_PROMPT, max_new_tokens=L, use_cache=True)\n",
    "         - Collect each result dict in results_len.\n",
    "\n",
    "      3) From results_len compute:\n",
    "         - latencies = [r['total_latency_s'] for r in results_len]\n",
    "         - throughputs = [r['tokens_per_sec'] for r in results_len]\n",
    "\n",
    "      4) Plot two charts using plot_xy (one figure per chart):\n",
    "         - Latency vs Generation Length\n",
    "         - Throughput vs Generation Length\n",
    "         (Use simple line+marker plots with grid; no custom colors.)\n",
    "\n",
    "      5) (Optional) return a dict for downstream use:\n",
    "         return {'lengths': lengths, 'latencies': latencies, 'throughputs': throughputs}\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement run_for_varied_lengths() per the TODO above.\")\n",
    "\n",
    "\n",
    "run_for_varied_lengths()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff940e7e",
   "metadata": {},
   "source": [
    "## 5) Exercise B — Numerical Precision (float16 vs float32)\n",
    "Reload for each precision with identical settings and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd61102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_varied_precision():\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      1) Define the list of precisions to test, e.g. ['float16', 'float32'].\n",
    "\n",
    "      2) For each precision p in the list:\n",
    "         - Load model + tokenizer using load_model_and_tokenizer(PREFERRED_MODEL, dtype_str=p).\n",
    "         - Run time_generate with:\n",
    "             * DEFAULT_PROMPT\n",
    "             * max_new_tokens=50\n",
    "             * use_cache=True\n",
    "         - Store the result dictionary in compare_precision[p].\n",
    "\n",
    "      3) Return compare_precision, which maps precision → generation performance metrics.\n",
    "\n",
    "      4) (Optional) Plot or print results for easier comparison.\n",
    "         Example metrics: latency, tokens/sec, latency per token.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement run_varied_precision() per the TODO above.\")\n",
    "\n",
    "\n",
    "\n",
    "compare_precision = run_varied_precision()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8787471f",
   "metadata": {},
   "source": [
    "## 6) Exercise C — KV Cache On vs Off\n",
    "Use the same model and toggle `use_cache`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e8679a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kv_results = {}\n",
    "kv_results['use_cache=True'] = time_generate(model, tokenizer, DEFAULT_PROMPT, max_new_tokens=50, use_cache=True)\n",
    "kv_results['use_cache=False'] = time_generate(model, tokenizer, DEFAULT_PROMPT, max_new_tokens=50, use_cache=False)\n",
    "kv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eb5214",
   "metadata": {},
   "source": [
    "## 7) (Optional) Compact summary table\n",
    "Summarize the three ideas in a small table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1b986f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "rows = []\n",
    "rows.append({'label':'baseline (fp16, cache on)','dtype':'float16','use_cache':True,'max_new_tokens':50, **baseline})\n",
    "rows.append({'label':'fp32, cache on','dtype':'float32','use_cache':True,'max_new_tokens':50, **compare_precision['float32']})\n",
    "rows.append({'label':'fp16, cache off','dtype':'float16','use_cache':False,'max_new_tokens':50, **kv_results['use_cache=False']})\n",
    "df = pd.DataFrame(rows)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb63bcdf-49db-4347-87f9-cdb08f7618e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
