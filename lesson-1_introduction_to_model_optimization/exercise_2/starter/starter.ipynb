{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c4b7ff-8007-42d3-b546-161ea0311854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505c6a6b-781d-479c-9da4-0bc638cbb1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── SETTINGS ────────────────────────────────────────────────────────────────\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7594d665-4e84-450e-b99e-4f314b3e2843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name: str):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Load tokenizer & model from `model_name`\n",
    "      - Move model to GPU if available, choose float16 for CUDA else float32\n",
    "      - Set model to eval mode\n",
    "      - Return (tokenizer, model, device)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86d4106-cefb-4fbc-98ce-a706826987f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_text():\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Pick or paste a ~100-token passage (e.g. a Wiki snippet)\n",
    "      - Return (text_str)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd910644-75ec-4fe7-a543-5ffc1b859572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_labels(tokenizer, text: str):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Tokenize `text` with return_tensors=\"pt\"\n",
    "      - Prepare inputs and set labels = input_ids\n",
    "      - Return (inputs_dict, input_len)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a79bc77-fcab-4626-82a4-1f6478754e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_peak_memory_and_loss(model, inputs, device):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Reset peak memory stats if CUDA\n",
    "      - Run model(**inputs) under torch.no_grad()\n",
    "      - Sync CUDA if needed\n",
    "      - Retrieve peak memory via torch.cuda.max_memory_allocated (in MiB)\n",
    "      - Return (peak_mem_mib, loss_value)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678ae06b-a587-4b21-8c88-75144b24dd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(loss: float):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Compute and return math.exp(loss)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1bb526-1947-498e-98e5-7d806b5b3940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_forward(model, inputs: Dict[str, torch.Tensor], device, num_warmup: int = 1, num_runs: int = 3):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      1) Move all tensors in `inputs` to `device`.\n",
    "      2) Warmup (no timing): under torch.no_grad(), run a small number of forward passes\n",
    "         (num_warmup times) to stabilize kernels/clocks:\n",
    "             _ = model(**inputs_on_device)\n",
    "         If CUDA, call torch.cuda.synchronize() after warmup.\n",
    "      3) Timed runs: create an empty list `latencies = []`.\n",
    "         For _ in range(num_runs):\n",
    "           - Record start time with time.perf_counter()\n",
    "           - Under torch.no_grad(), run a forward pass:\n",
    "                 _ = model(**inputs_on_device)\n",
    "           - If CUDA, call torch.cuda.synchronize()\n",
    "           - Record end time with time.perf_counter()\n",
    "           - Append (end - start) to `latencies`.\n",
    "      4) Return the average latency in seconds:\n",
    "             return sum(latencies) / max(len(latencies), 1)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a379759-ab45-43e2-bbea-996ef3a721f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sweep_context_length(\n",
    "    tokenizer,\n",
    "    model,\n",
    "    device,\n",
    "    targets=(64, 256, 1024),\n",
    "    base_text=None,\n",
    "    build_text_fn=None,\n",
    "    warmup=1,\n",
    "    runs=3,\n",
    "    plot=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      1) Determine the model's maximum context length:\n",
    "         - Read `max_position_embeddings` from `model.config` (fallback to 2048).\n",
    "         - Filter `targets` to those <= `max_len - 2` (leave room for special tokens).\n",
    "         - If no valid targets remain, raise a ValueError.\n",
    "\n",
    "      2) Choose a base paragraph:\n",
    "         - If `base_text` is None, call `select_text()`; if unavailable, raise with a helpful message.\n",
    "\n",
    "      3) Build texts of desired token lengths:\n",
    "         - Implement a builder that repeats `base_text` until the tokenized length >= `target_tokens`.\n",
    "         - Allow overriding via `build_text_fn(tokenizer, base_text, target_tokens)`.\n",
    "\n",
    "      4) For each target length:\n",
    "         - Build the text and tokenize with `tokenize_with_labels(tokenizer, text)` to get `(inputs, input_len)`.\n",
    "         - Move all tensors in `inputs` to `device`.\n",
    "         - Compute peak memory (MiB) and loss using `compute_peak_memory_and_loss(model, inputs, device)`.\n",
    "         - Measure forward (prefill) latency using `time_forward(model, inputs, device, num_warmup=warmup, num_runs=runs)`.\n",
    "         - Append `input_len`, `peak_mib`, and `latency_s` to running lists.\n",
    "         - Print a concise summary line for each target.\n",
    "\n",
    "      5) If `plot` is True, create two matplotlib plots (one per figure):\n",
    "         - Peak memory (MiB) vs input tokens.\n",
    "         - Forward latency (s) vs input tokens.\n",
    "         - Use simple line plots with markers and grid; do not specify custom colors.\n",
    "\n",
    "      6) Return a results dictionary with:\n",
    "         - 'lengths': list of actual input token counts\n",
    "         - 'peak_mib': list of peak memory values (MiB)\n",
    "         - 'latency_s': list of forward latencies (s)\n",
    "         - 'max_ctx': the model’s max context length\n",
    "         - 'targets_used': the filtered list of targets\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77147a80-e26b-437d-b80a-ae3810b264b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Iterable, Callable, Tuple\n",
    "\n",
    "def compare_precision_memory_perplexity(\n",
    "    model_name: str,\n",
    "    tokenizer,\n",
    "    load_with_dtype_fn: Callable[[str], Tuple],  # expected to return (tokenizer, model, device)\n",
    "    precisions: Iterable[str] = (\"float16\", \"float32\"),\n",
    "    base_text: str = None,\n",
    ") -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      1) Prepare a short evaluation paragraph:\n",
    "         - If `base_text` is None, call `select_text()` to get a default paragraph.\n",
    "\n",
    "      2) Tokenize once with the provided `tokenizer`:\n",
    "         - Use `tokenize_with_labels(tokenizer, text)` to get `(inputs_s, Ls)`.\n",
    "\n",
    "      3) For each precision in `precisions`:\n",
    "         - Reload the model with that dtype via: tok_p, mod_p, dev_p = load_with_dtype_fn(precision)\n",
    "           (Assume this function *honors* dtype; on CPU it may force float32.)\n",
    "         - Move pre-tokenized inputs to the appropriate device:\n",
    "               inp = {k: v.to(dev_p) for k, v in inputs_s.items()}\n",
    "         - Measure peak memory (MiB) and loss:\n",
    "               peak_mib, loss_val = compute_peak_memory_and_loss(mod_p, inp, dev_p)\n",
    "         - Convert loss to perplexity:\n",
    "               ppl = compute_perplexity(loss_val)\n",
    "         - Save into a results dict keyed by precision:\n",
    "               results[precision] = {\"peak_mib\": peak_mib, \"ppl\": ppl}\n",
    "\n",
    "      4) Return the results dict.\n",
    "\n",
    "    Returns:\n",
    "      Dict[str, Dict[str, float]] like:\n",
    "        {\"float16\": {\"peak_mib\": 123.4, \"ppl\": 15.2}, \"float32\": {...}}\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89d3869-d723-4080-b6e6-5fd32e387b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep_batch_memory(\n",
    "    tokenizer,\n",
    "    model,\n",
    "    device,\n",
    "    batch_sizes=(1, 2, 4),\n",
    "    base_text=None,\n",
    "    target_len=128,\n",
    "    plot=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      1) Choose a base paragraph:\n",
    "         - If `base_text` is None, call `select_text()` to get a default paragraph.\n",
    "\n",
    "      2) Implement a helper to build batched inputs:\n",
    "         - Given (tokenizer, text, bs, max_len), tokenize a list of the same `text`\n",
    "           repeated `bs` times with:\n",
    "               return_tensors='pt', padding='longest', truncation=True, max_length=target_len\n",
    "         - Set enc['labels'] = enc['input_ids'].clone()\n",
    "\n",
    "      3) For each batch size in `batch_sizes`:\n",
    "         - Build the batch with the helper.\n",
    "         - Move all tensors to `device`.\n",
    "         - Measure peak memory (MiB) using `compute_peak_memory_and_loss(model, enc, device)`.\n",
    "         - Record (bs, peak_mib) and print a concise summary line.\n",
    "\n",
    "      4) If `plot` is True, produce a single matplotlib figure of:\n",
    "         - x = batch size\n",
    "         - y = peak MiB\n",
    "         - Use a simple line plot with markers and grid; do not set custom colors.\n",
    "\n",
    "      5) Return a results dictionary:\n",
    "         {\n",
    "           \"batch_sizes\": [...],\n",
    "           \"peak_mib\": [...],\n",
    "           \"target_len\": target_len,\n",
    "         }\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473365bf-4f75-467b-a7cf-d6fe1e67868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start():\n",
    "    if \"load_with_dtype\" not in globals():\n",
    "        def load_with_dtype(dtype_str: str):\n",
    "            tok_p, mod_p, dev_p = load_model_and_tokenizer(MODEL_NAME)\n",
    "            return tok_p, mod_p, dev_p\n",
    "\n",
    "\n",
    "    # --- 1) Load model/tokenizer ---\n",
    "    tokenizer, model, device = load_model_and_tokenizer(MODEL_NAME)\n",
    "    print(f\"[load] device={device}\")\n",
    "\n",
    "    # --- 2) Select & tokenize a baseline paragraph ---\n",
    "    text = select_text()\n",
    "    inputs, input_len = tokenize_with_labels(tokenizer, text)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    print(f\"[tokenize] tokens={input_len}\")\n",
    "\n",
    "    # --- 3) Baseline: peak memory & loss ---\n",
    "    peak_mib, loss = compute_peak_memory_and_loss(model, inputs, device)\n",
    "    ppl = compute_perplexity(loss)\n",
    "    fwd_lat = time_forward(model, inputs, device)\n",
    "    print(f\"[baseline] peak={peak_mib:.1f} MiB | loss={loss:.3f} | ppl={ppl:.3f} | fwd={fwd_lat:.3f}s\")\n",
    "\n",
    "    # --- A) Context-length sweep ---\n",
    "    ctx_results = sweep_context_length(\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # --- B) Precision comparison ---\n",
    "    prec_results = compare_precision_memory_perplexity(\n",
    "        model_name=MODEL_NAME,\n",
    "        tokenizer=tokenizer,\n",
    "        load_with_dtype_fn=load_with_dtype,\n",
    "        precisions=(\"float16\", \"float32\"),\n",
    "        base_text=text,\n",
    "    )\n",
    "\n",
    "    # --- C) Batch-size sweep ---\n",
    "    batch_results = sweep_batch_memory(\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"baseline\": {\n",
    "            \"tokens\": int(input_len),\n",
    "            \"peak_mib\": float(peak_mib),\n",
    "            \"loss\": float(loss),\n",
    "            \"ppl\": float(ppl),\n",
    "            \"forward_latency_s\": float(fwd_lat),\n",
    "        },\n",
    "        \"context_sweep\": ctx_results,\n",
    "        \"precision_compare\": prec_results,\n",
    "        \"batch_sweep\": batch_results,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d528dcca-537b-43d1-a05a-088cf6d16708",
   "metadata": {},
   "outputs": [],
   "source": [
    "start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
