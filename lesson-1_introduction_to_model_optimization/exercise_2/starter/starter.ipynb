{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8538ff09",
   "metadata": {},
   "source": [
    "> **Importing Core Libraries for Model Evaluation**\n",
    "\n",
    "This cell imports the essential libraries and modules needed for the experiments.\n",
    "\n",
    "**What to check:**\n",
    "\n",
    "- `math` for mathematical operations.\n",
    "- `torch` for deep learning and tensor operations.\n",
    "- `AutoTokenizer` and `AutoModelForCausalLM` from `transformers` for model and tokenizer loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2fc4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1fbe81",
   "metadata": {},
   "source": [
    "> **Model Selection: Choosing the LLM for Experiments**\n",
    "\n",
    "This cell specifies the model name to be used throughout the notebook. The chosen model should be small enough to run on limited hardware but representative for scaling experiments.\n",
    "\n",
    "**What to check:**\n",
    "\n",
    "- Assign a string with the Hugging Face model name (e.g., `'TinyLlama/TinyLlama-1.1B-Chat-v1.0'`) to a variable like `MODEL_NAME`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548fa4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── SETTINGS ────────────────────────────────────────────────────────────────\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8735de2",
   "metadata": {},
   "source": [
    "> **Model and Tokenizer Loader: Device and Precision Handling**\n",
    "\n",
    "This function loads the model and tokenizer, moves the model to the correct device (GPU if available, otherwise CPU), and sets the appropriate numerical precision. The model is set to evaluation mode.\n",
    "\n",
    "**What to implement:**\n",
    "\n",
    "- Detect if CUDA (GPU) is available and set the device accordingly.\n",
    "- Choose `torch.float16` if using CUDA, otherwise use `torch.float32`.\n",
    "- Load the tokenizer and model from the specified model name.\n",
    "- Move the model to the selected device and set it to evaluation mode.\n",
    "- Return the tokenizer, model, and device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70cd37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name: str):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Load tokenizer & model from `model_name`\n",
    "      - Move model to GPU if available, choose float16 for CUDA else float32\n",
    "      - Set model to eval mode\n",
    "      - Return (tokenizer, model, device)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d8521d",
   "metadata": {},
   "source": [
    "> **Text Selection: Providing a Sample Passage**\n",
    "\n",
    "This function returns a sample passage of about 100 tokens, ideally from Wikipedia or a similar source. This passage will be used as the input prompt for all experiments.\n",
    "\n",
    "**What to implement:**\n",
    "\n",
    "- Choose or paste a paragraph of about 100 tokens.\n",
    "- Return the text as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae23f2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_text():\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Pick or paste a ~100-token passage (e.g. a Wiki snippet)\n",
    "      - Return (text_str)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf85270",
   "metadata": {},
   "source": [
    "> **Tokenization with Labels: Preparing Model Inputs**\n",
    "\n",
    "This function tokenizes the selected text and prepares the input tensors for the model, including setting up the labels for language modeling.\n",
    "\n",
    "**What to implement:**\n",
    "\n",
    "- Tokenize the text using the provided tokenizer with `return_tensors='pt'`.\n",
    "- Set `labels = input_ids` in the returned dictionary.\n",
    "- Return the inputs dictionary and the input length (number of tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8c4291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_labels(tokenizer, text: str):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Tokenize `text` with return_tensors=\"pt\"\n",
    "      - Prepare inputs and set labels = input_ids\n",
    "      - Return (inputs_dict, input_len)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b879ad55",
   "metadata": {},
   "source": [
    "> **Peak Memory and Loss Calculation: Measuring Resource Usage**\n",
    "\n",
    "This function runs a forward pass of the model on the provided inputs and measures the peak GPU memory usage (if CUDA is available) and the loss value.\n",
    "\n",
    "**What to implement:**\n",
    "\n",
    "- If using CUDA, reset peak memory stats before running the model.\n",
    "- Run the model in `torch.no_grad()` mode to avoid gradient computation.\n",
    "- After the forward pass, if using CUDA, synchronize and get the peak memory allocated (in MiB).\n",
    "- Return the peak memory (MiB) and the loss value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4b84c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_peak_memory_and_loss(model, inputs, device):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Reset peak memory stats if CUDA\n",
    "      - Run model(**inputs) under torch.no_grad()\n",
    "      - Sync CUDA if needed\n",
    "      - Retrieve peak memory via torch.cuda.max_memory_allocated (in MiB)\n",
    "      - Return (peak_mem_mib, loss_value)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29d0a31",
   "metadata": {},
   "source": [
    "> **Perplexity Computation: Interpreting Model Loss**\n",
    "\n",
    "This function converts the model's loss value into perplexity, a standard metric for evaluating language models.\n",
    "\n",
    "**What to implement:**\n",
    "\n",
    "- Use `math.exp(loss)` to compute perplexity from the loss value.\n",
    "- Return the perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c00c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(loss: float):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Compute and return math.exp(loss)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d69381",
   "metadata": {},
   "source": [
    "> **Forward Pass Timing: Measuring Inference Latency**\n",
    "\n",
    "This function measures the average time taken for the model to perform a forward pass (inference) on the input data. It includes warmup runs to stabilize performance and reports the mean latency over several timed runs.\n",
    "\n",
    "**What to implement:**\n",
    "\n",
    "- Move all input tensors to the correct device.\n",
    "- Run a few warmup passes (not timed) to stabilize performance.\n",
    "- For each timed run:\n",
    "\n",
    "  - Record the start time.\n",
    "  - Run the model in `torch.no_grad()` mode.\n",
    "  - If using CUDA, synchronize after each run.\n",
    "  - Record the end time and compute the latency.\n",
    "- Return the average latency in seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e25f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_forward(model, inputs: Dict[str, torch.Tensor], device, num_warmup: int = 1, num_runs: int = 3):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      1) Move all tensors in `inputs` to `device`.\n",
    "      2) Warmup (no timing): under torch.no_grad(), run a small number of forward passes\n",
    "         (num_warmup times) to stabilize kernels/clocks:\n",
    "             _ = model(**inputs_on_device)\n",
    "         If CUDA, call torch.cuda.synchronize() after warmup.\n",
    "      3) Timed runs: create an empty list `latencies = []`.\n",
    "         For _ in range(num_runs):\n",
    "           - Record start time with time.perf_counter()\n",
    "           - Under torch.no_grad(), run a forward pass:\n",
    "                 _ = model(**inputs_on_device)\n",
    "           - If CUDA, call torch.cuda.synchronize()\n",
    "           - Record end time with time.perf_counter()\n",
    "           - Append (end - start) to `latencies`.\n",
    "      4) Return the average latency in seconds:\n",
    "             return sum(latencies) / max(len(latencies), 1)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56f3cf7",
   "metadata": {},
   "source": [
    "> **Context Length Sweep: Scaling Analysis with Input Size**\n",
    "\n",
    "This function systematically increases the input (context) length and measures how peak memory usage and inference latency scale. It builds longer input texts, runs the model, and visualizes the results.\n",
    "\n",
    "**What to implement:**\n",
    "\n",
    "- Determine the model's maximum context length from `model.config` (default to 2048 if unavailable).\n",
    "\n",
    "- Filter the target context lengths to those that fit within the model's window.\n",
    "- Build input texts of increasing length by repeating the base passage.\n",
    "- For each target length:\n",
    "\n",
    "  - Tokenize and prepare inputs.\n",
    "  - Move tensors to the correct device.\n",
    "  - Measure peak memory and latency using the functions above.\n",
    "  - Print a summary for each run.\n",
    "- Plot memory and latency vs. input length using matplotlib.\n",
    "- Return a dictionary with the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ab004f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sweep_context_length(\n",
    "    tokenizer,\n",
    "    model,\n",
    "    device,\n",
    "    targets=(64, 256, 1024),\n",
    "    base_text=None,\n",
    "    build_text_fn=None,\n",
    "    warmup=1,\n",
    "    runs=3,\n",
    "    plot=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      1) Determine the model's maximum context length:\n",
    "         - Read `max_position_embeddings` from `model.config` (fallback to 2048).\n",
    "         - Filter `targets` to those <= `max_len - 2` (leave room for special tokens).\n",
    "         - If no valid targets remain, raise a ValueError.\n",
    "\n",
    "      2) Choose a base paragraph:\n",
    "         - If `base_text` is None, call `select_text()`; if unavailable, raise with a helpful message.\n",
    "\n",
    "      3) Build texts of desired token lengths:\n",
    "         - Implement a builder that repeats `base_text` until the tokenized length >= `target_tokens`.\n",
    "         - Allow overriding via `build_text_fn(tokenizer, base_text, target_tokens)`.\n",
    "\n",
    "      4) For each target length:\n",
    "         - Build the text and tokenize with `tokenize_with_labels(tokenizer, text)` to get `(inputs, input_len)`.\n",
    "         - Move all tensors in `inputs` to `device`.\n",
    "         - Compute peak memory (MiB) and loss using `compute_peak_memory_and_loss(model, inputs, device)`.\n",
    "         - Measure forward (prefill) latency using `time_forward(model, inputs, device, num_warmup=warmup, num_runs=runs)`.\n",
    "         - Append `input_len`, `peak_mib`, and `latency_s` to running lists.\n",
    "         - Print a concise summary line for each target.\n",
    "\n",
    "      5) If `plot` is True, create two matplotlib plots (one per figure):\n",
    "         - Peak memory (MiB) vs input tokens.\n",
    "         - Forward latency (s) vs input tokens.\n",
    "         - Use simple line plots with markers and grid; do not specify custom colors.\n",
    "\n",
    "      6) Return a results dictionary with:\n",
    "         - 'lengths': list of actual input token counts\n",
    "         - 'peak_mib': list of peak memory values (MiB)\n",
    "         - 'latency_s': list of forward latencies (s)\n",
    "         - 'max_ctx': the model’s max context length\n",
    "         - 'targets_used': the filtered list of targets\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c6d49d",
   "metadata": {},
   "source": [
    "> **Precision Comparison: Memory and Perplexity Across dtypes**\n",
    "\n",
    "This function compares the model's memory usage and perplexity when run with different numerical precisions (float16 and float32). It reloads the model for each precision, runs evaluation, and summarizes the trade-offs.\n",
    "\n",
    "**What to implement:**\n",
    "\n",
    "- Prepare a short evaluation paragraph (use the text selection function).\n",
    "\n",
    "- Tokenize the text once for all precisions.\n",
    "- For each precision:\n",
    "\n",
    "  - Reload the model and tokenizer with the correct dtype.\n",
    "  - Move inputs to the correct device.\n",
    "  - Measure peak memory and loss, then compute perplexity.\n",
    "  - Store results in a dictionary.\n",
    "- Return the results dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11686a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Iterable, Callable, Tuple\n",
    "\n",
    "def compare_precision_memory_perplexity(\n",
    "    model_name: str,\n",
    "    tokenizer,\n",
    "    load_with_dtype_fn: Callable[[str], Tuple],  # expected to return (tokenizer, model, device)\n",
    "    precisions: Iterable[str] = (\"float16\", \"float32\"),\n",
    "    base_text: str = None,\n",
    ") -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      1) Prepare a short evaluation paragraph:\n",
    "         - If `base_text` is None, call `select_text()` to get a default paragraph.\n",
    "\n",
    "      2) Tokenize once with the provided `tokenizer`:\n",
    "         - Use `tokenize_with_labels(tokenizer, text)` to get `(inputs_s, Ls)`.\n",
    "\n",
    "      3) For each precision in `precisions`:\n",
    "         - Reload the model with that dtype via: tok_p, mod_p, dev_p = load_with_dtype_fn(precision)\n",
    "           (Assume this function *honors* dtype; on CPU it may force float32.)\n",
    "         - Move pre-tokenized inputs to the appropriate device:\n",
    "               inp = {k: v.to(dev_p) for k, v in inputs_s.items()}\n",
    "         - Measure peak memory (MiB) and loss:\n",
    "               peak_mib, loss_val = compute_peak_memory_and_loss(mod_p, inp, dev_p)\n",
    "         - Convert loss to perplexity:\n",
    "               ppl = compute_perplexity(loss_val)\n",
    "         - Save into a results dict keyed by precision:\n",
    "               results[precision] = {\"peak_mib\": peak_mib, \"ppl\": ppl}\n",
    "\n",
    "      4) Return the results dict.\n",
    "\n",
    "    Returns:\n",
    "      Dict[str, Dict[str, float]] like:\n",
    "        {\"float16\": {\"peak_mib\": 123.4, \"ppl\": 15.2}, \"float32\": {...}}\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f698e5f",
   "metadata": {},
   "source": [
    "> **Batch Size Sweep: Analyzing Memory Scaling with Batch Size**\n",
    "\n",
    "This function evaluates how peak memory usage changes as the batch size increases. It creates batches of repeated input text, runs the model, and plots the relationship between batch size and memory consumption.\n",
    "\n",
    "**What to implement:**\n",
    "\n",
    "- Prepare a base paragraph for batching.\n",
    "- For each batch size:\n",
    "\n",
    "  - Create a batch of repeated texts and tokenize with padding and truncation.\n",
    "  - Move tensors to the correct device.\n",
    "  - Measure peak memory using the earlier function.\n",
    "  - Print a summary for each batch size.\n",
    "- Plot memory vs. batch size using matplotlib.\n",
    "- Return a dictionary with the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d77821d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep_batch_memory(\n",
    "    tokenizer,\n",
    "    model,\n",
    "    device,\n",
    "    batch_sizes=(1, 2, 4),\n",
    "    base_text=None,\n",
    "    target_len=128,\n",
    "    plot=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      1) Choose a base paragraph:\n",
    "         - If `base_text` is None, call `select_text()` to get a default paragraph.\n",
    "\n",
    "      2) Implement a helper to build batched inputs:\n",
    "         - Given (tokenizer, text, bs, max_len), tokenize a list of the same `text`\n",
    "           repeated `bs` times with:\n",
    "               return_tensors='pt', padding='longest', truncation=True, max_length=target_len\n",
    "         - Set enc['labels'] = enc['input_ids'].clone()\n",
    "\n",
    "      3) For each batch size in `batch_sizes`:\n",
    "         - Build the batch with the helper.\n",
    "         - Move all tensors to `device`.\n",
    "         - Measure peak memory (MiB) using `compute_peak_memory_and_loss(model, enc, device)`.\n",
    "         - Record (bs, peak_mib) and print a concise summary line.\n",
    "\n",
    "      4) If `plot` is True, produce a single matplotlib figure of:\n",
    "         - x = batch size\n",
    "         - y = peak MiB\n",
    "         - Use a simple line plot with markers and grid; do not set custom colors.\n",
    "\n",
    "      5) Return a results dictionary:\n",
    "         {\n",
    "           \"batch_sizes\": [...],\n",
    "           \"peak_mib\": [...],\n",
    "           \"target_len\": target_len,\n",
    "         }\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730c8810",
   "metadata": {},
   "source": [
    "> **Experiment Runner: Orchestrating All Analyses**\n",
    "\n",
    "This function coordinates the entire experiment workflow. It loads the model and tokenizer, prepares the input, runs baseline measurements, and then executes the context length sweep, precision comparison, and batch size sweep. The results are returned in a structured dictionary for further analysis or reporting.\n",
    "\n",
    "**What to implement:**\n",
    "\n",
    "- Load the model and tokenizer using your loader function.\n",
    "- Select and tokenize a baseline paragraph.\n",
    "- Run baseline measurements: peak memory, loss, perplexity, and forward latency.\n",
    "- Run the context length sweep, precision comparison, and batch size sweep functions.\n",
    "- Return a dictionary with all results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064e7f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start():\n",
    "    if \"load_with_dtype\" not in globals():\n",
    "        def load_with_dtype(dtype_str: str):\n",
    "            tok_p, mod_p, dev_p = load_model_and_tokenizer(MODEL_NAME)\n",
    "            return tok_p, mod_p, dev_p\n",
    "\n",
    "\n",
    "    # --- 1) Load model/tokenizer ---\n",
    "    tokenizer, model, device = load_model_and_tokenizer(MODEL_NAME)\n",
    "    print(f\"[load] device={device}\")\n",
    "\n",
    "    # --- 2) Select & tokenize a baseline paragraph ---\n",
    "    text = select_text()\n",
    "    inputs, input_len = tokenize_with_labels(tokenizer, text)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    print(f\"[tokenize] tokens={input_len}\")\n",
    "\n",
    "    # --- 3) Baseline: peak memory & loss ---\n",
    "    peak_mib, loss = compute_peak_memory_and_loss(model, inputs, device)\n",
    "    ppl = compute_perplexity(loss)\n",
    "    fwd_lat = time_forward(model, inputs, device)\n",
    "    print(f\"[baseline] peak={peak_mib:.1f} MiB | loss={loss:.3f} | ppl={ppl:.3f} | fwd={fwd_lat:.3f}s\")\n",
    "\n",
    "    # --- A) Context-length sweep ---\n",
    "    ctx_results = sweep_context_length(\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # --- B) Precision comparison ---\n",
    "    prec_results = compare_precision_memory_perplexity(\n",
    "        model_name=MODEL_NAME,\n",
    "        tokenizer=tokenizer,\n",
    "        load_with_dtype_fn=load_with_dtype,\n",
    "        precisions=(\"float16\", \"float32\"),\n",
    "        base_text=text,\n",
    "    )\n",
    "\n",
    "    # --- C) Batch-size sweep ---\n",
    "    batch_results = sweep_batch_memory(\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"baseline\": {\n",
    "            \"tokens\": int(input_len),\n",
    "            \"peak_mib\": float(peak_mib),\n",
    "            \"loss\": float(loss),\n",
    "            \"ppl\": float(ppl),\n",
    "            \"forward_latency_s\": float(fwd_lat),\n",
    "        },\n",
    "        \"context_sweep\": ctx_results,\n",
    "        \"precision_compare\": prec_results,\n",
    "        \"batch_sweep\": batch_results,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d528dcca-537b-43d1-a05a-088cf6d16708",
   "metadata": {},
   "outputs": [],
   "source": [
    "start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
