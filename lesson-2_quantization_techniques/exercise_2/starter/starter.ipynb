{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77b3712d",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "This cell:\n",
    "- Imports standard libraries for file handling, timing, and mathematical operations.\n",
    "- Imports PyTorch for deep learning operations.\n",
    "- Imports NumPy for numerical computations.\n",
    "- Imports Hugging Face Transformers for model and tokenizer handling.\n",
    "- Imports Optimum Intel and Neural Compressor for quantization-aware training (QAT).\n",
    "- Imports the `evaluate` library for evaluation metrics.\n",
    "- Sets up the necessary modules for dataset loading and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb2b1242-8e58-4d60-82d1-3972b6186d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-03 23:43:00.992905: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-03 23:43:01.006372: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-09-03 23:43:01.023874: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-09-03 23:43:01.029380: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-09-03 23:43:01.041980: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-03 23:43:06 [WARNING][auto_accelerator.py:454] Auto detect accelerator: CUDA_Accelerator.\n",
      "2025-09-03 23:43:06 [WARNING][modeling_auto.py:807] please install transformers>=4.46 for quantizing Qwen2VLForConditionalGeneration.\n",
      "2025-09-03 23:43:06 [WARNING][modeling_auto.py:814] please install transformers>=4.46 for quantizing MllamaForConditionalGeneration.\n",
      "2025-09-03 23:43:06 [WARNING][modeling_auto.py:821] please install transformers>=4.46 for quantizing LlavaForConditionalGeneration.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import shutil\n",
    "import torch\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from datasets import load_dataset, DownloadConfig\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    EvalPrediction,\n",
    ")\n",
    "from optimum.intel import INCTrainer, INCModelForCausalLM\n",
    "from neural_compressor import QuantizationAwareTrainingConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4d1df0",
   "metadata": {},
   "source": [
    "# Define Settings and Constants\n",
    "This cell:\n",
    "- Defines the model name to be used for training and evaluation.\n",
    "- Sets up a sample prompt for benchmarking latency and throughput.\n",
    "- Defines a sample text for perplexity evaluation.\n",
    "- Specifies the maximum number of tokens to generate during inference.\n",
    "- Sets the output directory for saving the quantized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afdb9262-9057-474c-a135-fcf214fd5ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── SETTINGS ────────────────────────────────────────────────────────────────\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "PROMPT = (\n",
    "    \"Over the next decade, sustainable energy solutions will revolutionize \"\n",
    "    \"global power grids, reducing carbon footprints and fostering resilient \"\n",
    "    \"communities through innovative storage and distribution technologies.\"\n",
    ")\n",
    "PERP_TEXT = (\n",
    "    \"Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast \"\n",
    "    \"to the natural intelligence displayed by humans and animals. Leading AI textbooks \"\n",
    "    \"define the field as the study of intelligent agents: any system that perceives \"\n",
    "    \"its environment and takes actions that maximize its chance of achieving its goals.\"\n",
    ")\n",
    "MAX_NEW_TOKENS = 50\n",
    "OUTPUT_DIR = \"qat_tinyllama\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff8d4da",
   "metadata": {},
   "source": [
    "# Load Model and Tokenizer\n",
    "This function:\n",
    "- Loads the specified model using Hugging Face Transformers.\n",
    "- Enables gradient checkpointing to save memory during training.\n",
    "- Loads the tokenizer associated with the model.\n",
    "- Ensures that the tokenizer has a pad token, setting it to the EOS token if missing.\n",
    "- Returns the loaded model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c3d52ef-d54d-416f-9ac9-68088d41f182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_id: str):\n",
    "    # 1. Load the model\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "    # (Optional) save memory during training\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    # 2. Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "    # 3. Make sure we have a pad token\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de66e39",
   "metadata": {},
   "source": [
    "# Prepare Datasets\n",
    "This function:\n",
    "- Loads the Wikitext-2 dataset for training and evaluation.\n",
    "- Tokenizes the dataset and prepares labels for causal language modeling.\n",
    "- Truncates and pads the tokenized data to a fixed block size.\n",
    "- Selects a subset of the dataset for training and evaluation to keep the process lightweight.\n",
    "- Returns the prepared training and evaluation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9c7f232-bc3c-45a3-a97e-ebe0eaffa447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(tokenizer, block_size=8, train_size=1000, eval_size=500):\n",
    "    # 1) Load the raw Wikitext-2 dataset\n",
    "    raw = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "    # 2) Tokenization + label prep\n",
    "    def _tokenize(examples):\n",
    "        out = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=block_size,\n",
    "        )\n",
    "        out[\"labels\"] = out[\"input_ids\"].copy()\n",
    "        return out\n",
    "\n",
    "    tokenized = raw.map(\n",
    "        _tokenize,\n",
    "        batched=True,\n",
    "        remove_columns=[\"text\"],\n",
    "    )\n",
    "\n",
    "    # 3) Select the subsets\n",
    "    train_ds = tokenized[\"train\"].select(range(train_size))\n",
    "    eval_ds  = tokenized[\"validation\"].select(range(eval_size))\n",
    "\n",
    "    return train_ds, eval_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f70f8ed",
   "metadata": {},
   "source": [
    "# Create INCTrainer for QAT\n",
    "This function:\n",
    "- Sets up training arguments for quantization-aware training (QAT).\n",
    "  - Configures batch size, gradient accumulation, and optimizer.\n",
    "  - Disables mid-training checkpoints to save space.\n",
    "  - Enables mixed precision (FP16) if CUDA is available.\n",
    "- Creates an INCTrainer instance with the specified model, tokenizer, and datasets.\n",
    "- Returns the configured trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "080245c8-386e-43c1-b949-fa2c6879d276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inc_trainer(model, tokenizer, train_ds, quant_config, output_dir):\n",
    "    from transformers import TrainingArguments\n",
    "    from optimum.intel import INCTrainer\n",
    "    from transformers import default_data_collator\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        optim=\"adamw_bnb_8bit\",\n",
    "        eval_strategy=\"no\",          # <- replaces deprecated evaluation_strategy\n",
    "        save_strategy=\"no\",          # <- NO mid-training checkpoints\n",
    "        save_only_model=True,        # <- smaller, model-only saves when we do save\n",
    "        logging_steps=50,\n",
    "        save_total_limit=1,\n",
    "        report_to=[],                # (quiet logs; optional)\n",
    "    )\n",
    "\n",
    "    trainer = INCTrainer(\n",
    "        model=model,\n",
    "        quantization_config=quant_config,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=default_data_collator,\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd7e02d",
   "metadata": {},
   "source": [
    "# Compute Perplexity Function\n",
    "This function:\n",
    "- Defines a closure to compute perplexity during evaluation.\n",
    "- Calculates cross-entropy loss while ignoring padding tokens.\n",
    "- Converts logits and labels to PyTorch tensors for loss computation.\n",
    "- Returns the perplexity as an evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75af9f3b-c3e2-406c-856d-a743aa5e2eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_compute_ppl_fn(pad_token_id: int):\n",
    "    \"\"\"\n",
    "    Returns a compute_metrics function that knows the pad_token_id.\n",
    "    \"\"\"\n",
    "    def compute_ppl(pred: EvalPrediction):\n",
    "        # 1) Unpack\n",
    "        logits = pred.predictions         # np array (batch, seq_len, vocab_size)\n",
    "        labels = pred.label_ids           # np array (batch, seq_len)\n",
    "\n",
    "        # 2) Shift so each token predicts the next one\n",
    "        shift_logits = logits[..., :-1, :]\n",
    "        shift_labels = labels[..., 1:]\n",
    "\n",
    "        # 3) Flatten\n",
    "        flat_logits = shift_logits.reshape(-1, shift_logits.shape[-1])\n",
    "        flat_labels = shift_labels.reshape(-1)\n",
    "\n",
    "        # 4) To torch\n",
    "        logits_t = torch.from_numpy(flat_logits)\n",
    "        labels_t = torch.from_numpy(flat_labels)\n",
    "\n",
    "        # 5) CE loss ignoring pad_token_id\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "        loss = loss_fct(logits_t, labels_t)\n",
    "\n",
    "        # 6) Return perplexity\n",
    "        return {\"eval_perplexity\": torch.exp(loss).item()}\n",
    "    return compute_ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecde143f",
   "metadata": {},
   "source": [
    "# Run QAT and Evaluate\n",
    "This function:\n",
    "- Trains the model using quantization-aware training (QAT).\n",
    "- Evaluates the model on a small subset of the evaluation dataset.\n",
    "- Computes perplexity using the `compute_metrics` function.\n",
    "- Returns the evaluation metrics, including perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1896d4b-50bd-4197-a100-2a424ab3a94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_qat_and_evaluate(trainer, eval_ds, tokenizer):\n",
    "    # 1) Train with QAT\n",
    "    trainer.train()\n",
    "\n",
    "    # 2) Small slice for eval\n",
    "    small_eval = eval_ds.select(range(min(len(eval_ds), 100)))\n",
    "\n",
    "    # 3) Build a ppl_fn that closes over pad_token_id\n",
    "    ppl_fn = make_compute_ppl_fn(tokenizer.pad_token_id)\n",
    "\n",
    "    # 4) Set up evaluation INCTrainer\n",
    "    eval_args = TrainingArguments(\n",
    "        output_dir=trainer.args.output_dir,\n",
    "        per_device_eval_batch_size=1,\n",
    "        fp16=True,\n",
    "        eval_strategy=\"no\",\n",
    "        save_strategy=\"no\",\n",
    "        logging_steps=50,\n",
    "    )\n",
    "    eval_trainer = INCTrainer(\n",
    "        model=trainer.model,\n",
    "        args=eval_args,\n",
    "        eval_dataset=small_eval,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=default_data_collator,\n",
    "        compute_metrics=ppl_fn,  # << use the closure here\n",
    "    )\n",
    "\n",
    "    # 5) Evaluate and return metrics\n",
    "    return eval_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d634206e",
   "metadata": {},
   "source": [
    "# Save and Load QAT Model\n",
    "This function:\n",
    "- Saves the quantized model to the specified output directory.\n",
    "- Reloads the saved model as an INCModelForCausalLM.\n",
    "- Returns the reloaded model for further evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c43f6335-f9fd-48c8-870d-df9a6da99969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_load_qat_model(trainer, output_dir):\n",
    "    # 1) Save to the specified directory\n",
    "    trainer.save_model(output_dir)\n",
    "\n",
    "    # 2) Load it back as an INCModelForCausalLM\n",
    "    loaded_model = INCModelForCausalLM.from_pretrained(output_dir)\n",
    "\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bde59d5",
   "metadata": {},
   "source": [
    "# Measure Latency, Throughput, and Peak Memory\n",
    "This function:\n",
    "- Measures the latency and throughput of the model during text generation.\n",
    "- Evaluates the peak GPU memory usage and perplexity for a given text.\n",
    "- Returns the latency, throughput, peak memory usage, and perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67f7f1f8-3f95-47c1-9299-826e281c6db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_latency_and_throughput(model, tokenizer, prompt: str, device, max_new_tokens=50):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_len = inputs[\"input_ids\"].size(1)\n",
    "\n",
    "    # warm-up\n",
    "    _ = model.generate(**inputs, max_new_tokens=5)\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    # timed generation\n",
    "    start = time.time()\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "\n",
    "    latency = end - start\n",
    "    gen_tokens = outputs.size(1) - input_len\n",
    "    return latency, gen_tokens / latency\n",
    "\n",
    "def measure_peak_mem_and_perplexity(model, tokenizer, text: str, device):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    torch.cuda.synchronize()\n",
    "    peak_mem_mib = torch.cuda.max_memory_allocated(device) / 1024**2\n",
    "    perplexity = math.exp(out.loss.item())\n",
    "    return peak_mem_mib, perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1becd685",
   "metadata": {},
   "source": [
    "# Main Execution Flow\n",
    "This cell:\n",
    "- Loads the model and tokenizer.\n",
    "- Prepares the training and evaluation datasets.\n",
    "- Configures QAT settings and creates a trainer.\n",
    "- Runs QAT and evaluates the model's perplexity.\n",
    "- Saves and reloads the quantized model.\n",
    "- Benchmarks the quantized model for latency, throughput, and memory usage.\n",
    "- Prints the final evaluation metrics and model size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26545b47-4068-45c7-8b15-1c9b568d3725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 13:46, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.397400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.348700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.351600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.072600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.096600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final perplexity: 55.14\n",
      "Latency     : 1.180 s\n",
      "Throughput  : 42.4 tokens/s\n",
      "Peak GPU memory     : 10660.4 MiB\n",
      "Next-token perplexity: 10.961\n",
      "Quantized model size : 4198.67 MB\n",
      "Average latency      : 1179.8 ms\n",
      "Throughput           : 42.4 tokens/sec\n",
      "Peak GPU memory      : 10660.4 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1) Load model & tokenizer\n",
    "model, tokenizer = load_model_and_tokenizer(MODEL_NAME)\n",
    "\n",
    "# 2) Prepare datasets\n",
    "train_ds, eval_ds = prepare_datasets(tokenizer)\n",
    "\n",
    "# 3) QAT configuration\n",
    "quant_config = QuantizationAwareTrainingConfig()\n",
    "\n",
    "# 4) Create and run QAT trainer\n",
    "qat_trainer = create_inc_trainer(model, tokenizer, train_ds, quant_config, OUTPUT_DIR)\n",
    "metrics = run_qat_and_evaluate(qat_trainer, eval_ds, tokenizer)\n",
    "print(f\"Final perplexity: {metrics['eval_perplexity']:.2f}\")\n",
    "\n",
    "# 5) Save & load quantized model\n",
    "qat_model = save_and_load_qat_model(qat_trainer, OUTPUT_DIR)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "qat_model.to(device)\n",
    "\n",
    "# 6) Benchmarks\n",
    "latency, throughput = measure_latency_and_throughput(qat_model, tokenizer, PROMPT, device)\n",
    "print(f\"Latency     : {latency:.3f} s\")\n",
    "print(f\"Throughput  : {throughput:.1f} tokens/s\")\n",
    "\n",
    "peak_mem, ppl = measure_peak_mem_and_perplexity(qat_model, tokenizer, PERP_TEXT, device)\n",
    "print(f\"Peak GPU memory     : {peak_mem:.1f} MiB\")\n",
    "print(f\"Next-token perplexity: {ppl:.3f}\")\n",
    "size_mb = sum(os.path.getsize(os.path.join(OUTPUT_DIR, f)) for f in os.listdir(OUTPUT_DIR)) / 1024**2\n",
    "\n",
    "print(f\"Quantized model size : {size_mb:.2f} MB\")\n",
    "print(f\"Average latency      : {latency*1000:.1f} ms\")\n",
    "print(f\"Throughput           : {throughput:.1f} tokens/sec\")\n",
    "if peak_mem is not None:\n",
    "    print(f\"Peak GPU memory      : {peak_mem:.1f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5777a580",
   "metadata": {},
   "source": [
    "# Evaluate Perplexity on CPU\n",
    "This function:\n",
    "- Evaluates the perplexity of the model on a list of texts using CPU.\n",
    "- Ignores padding tokens during loss computation.\n",
    "- Returns the computed perplexity for the given texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f2adf24-e9bc-441f-b92e-badfaf9570b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_ppl_cpu_texts(model, tokenizer, texts, max_length=64, batch_size=1):\n",
    "    import math, torch\n",
    "    model.eval().to(\"cpu\")\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    with torch.inference_mode():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            enc = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "            labels = enc[\"input_ids\"].clone()\n",
    "            # ignore pad positions in loss\n",
    "            pad_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "            attention = enc[\"attention_mask\"]\n",
    "            labels[attention == 0] = -100\n",
    "            out = model(**enc, labels=labels)\n",
    "            # HF loss is mean over non -100 positions; weight by #tokens to combine batches\n",
    "            n_tokens = (labels != -100).sum().item()\n",
    "            total_loss += float(out.loss) * max(1, n_tokens)\n",
    "            total_tokens += max(1, n_tokens)\n",
    "    return math.exp(total_loss / max(1, total_tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a9f8a4-4e19-4a4d-923f-32c397c4c94b",
   "metadata": {},
   "source": [
    "# Exercise 1 — Activation Fake-Quant Sensitivity (CPU, Per-Layer, No Training)\n",
    "This exercise:\n",
    "- Simulates QAT-style activation quantization for each layer using a forward pre-hook.\n",
    "- Evaluates the impact of fake quantization on perplexity for each layer.\n",
    "- Measures the perplexity drift compared to the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6577de13-a3ff-47b1-b11d-c7b1c2b1ff34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_fake_quant_sensitivity_cpu(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    texts=None,\n",
    "    max_length=64,\n",
    "    batch_size=1,\n",
    "    last_k_linear=8,   # test only the last K Linear layers to keep it light\n",
    "):\n",
    "    \"\"\"\n",
    "    For each of the last K nn.Linear layers:\n",
    "      - add a forward pre-hook that fake-quantizes the *input activation* to int8 (symmetric)\n",
    "      - run a tiny CPU perplexity eval\n",
    "      - remove the hook\n",
    "    Returns a DataFrame with baseline and per-layer PPL deltas.\n",
    "\n",
    "    Student TODOs:\n",
    "      - Ensure the model is on CPU and in eval() mode.\n",
    "      - Find the last K nn.Linear layers to test.\n",
    "      - Calculate a baseline perplexity score with the original model.\n",
    "      - Implement a symmetric int8 fake-quantization helper function.\n",
    "      - Loop through target layers:\n",
    "          - Register a forward pre-hook to apply fake-quant on layer inputs.\n",
    "          - Evaluate perplexity with the hook active.\n",
    "          - Remove the hook to clean up.\n",
    "          - Store results.\n",
    "      - Return a tidy DataFrame with the results.\n",
    "    \"\"\"\n",
    "    # --- TODO 1: Imports (keep local so function is self-contained) ---\n",
    "    import torch\n",
    "    import pandas as pd\n",
    "\n",
    "    # Some helper functions you'll need. Assume these are defined elsewhere:\n",
    "    #   - eval_ppl_cpu_texts(model, tokenizer, texts, ...): returns a float PPL\n",
    "    #   - PERP_TEXT: a default string for perplexity evaluation\n",
    "    from fns import eval_ppl_cpu_texts, PERP_TEXT\n",
    "\n",
    "\n",
    "    # --- TODO 2: Set up model for CPU inference ---\n",
    "    # Hints:\n",
    "    #   - Use model.to(\"cpu\") to move all parameters and buffers to the CPU.\n",
    "    #   - Disable gradient calculations globally with torch.set_grad_enabled(False).\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "\n",
    "    # --- TODO 3: Prepare evaluation data ---\n",
    "    # If the `texts` argument is None, create a small default list for evaluation.\n",
    "    # A list of 16 identical strings is sufficient to get a stable PPL estimate.\n",
    "    # Hint: `if texts is None: texts = [PERP_TEXT] * 16`\n",
    "    if texts is None:\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "\n",
    "    # --- TODO 4: Find candidate nn.Linear layers ---\n",
    "    # Create a list of (name, module) tuples for all nn.Linear layers in the model.\n",
    "    # Hint: `linears = [(n, m) for (n, m) in model.named_modules() if isinstance(m, torch.nn.Linear)]`\n",
    "    # Then, select only the last `last_k_linear` layers for the analysis.\n",
    "    linears_to_test = [] # This should be a slice of the full list of linears\n",
    "    if not linears_to_test:\n",
    "        raise RuntimeError(\"No nn.Linear modules found to test.\")\n",
    "\n",
    "\n",
    "    # --- TODO 5: Calculate baseline perplexity ---\n",
    "    # Before applying any modifications, evaluate the perplexity of the original model.\n",
    "    # This will be your reference point for measuring the impact of quantization.\n",
    "    baseline_ppl = 0.0 # Call eval_ppl_cpu_texts(...)\n",
    "\n",
    "\n",
    "    # --- TODO 6: Implement a fake-quantization helper function ---\n",
    "    # This function will simulate the effect of int8 quantization on a float tensor.\n",
    "    # It should perform symmetric, per-tensor quantization.\n",
    "    # Steps:\n",
    "    #   1. Find the absolute maximum value of the input tensor `x`.\n",
    "    #   2. Calculate the `scale` factor: `scale = max_abs / 127.0`.\n",
    "    #   3. Quantize: `q = torch.round(x / scale)`.\n",
    "    #   4. Clip the quantized values to the int8 range [-127, 127].\n",
    "    #   5. De-quantize: return `q * scale`.\n",
    "    #   - Add a small epsilon to the max_abs to avoid division by zero.\n",
    "    def fake_quant_sym_int8(x, eps=1e-8):\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "\n",
    "    # --- TODO 7: Main loop for sensitivity analysis ---\n",
    "    # Iterate through each `name` and `module` in your `linears_to_test` list.\n",
    "    # Inside the loop, you will add a hook, evaluate, and then remove the hook.\n",
    "    results = []\n",
    "    for name, module in linears_to_test:\n",
    "\n",
    "        # --- TODO 8: Register a forward pre-hook ---\n",
    "        # A forward pre-hook runs before the layer's forward pass. It's perfect\n",
    "        # for modifying the *inputs* to a layer.\n",
    "        # The hook function should take (module, inputs) and return a new tuple of inputs.\n",
    "        # Here, you only want to modify the first input tensor (the activations).\n",
    "        #\n",
    "        # hook_fn = lambda mod, inputs: (fake_quant_sym_int8(inputs[0]),) + tuple(inputs[1:])\n",
    "        # hook = module.register_forward_pre_hook(hook_fn)\n",
    "        hook = None\n",
    "\n",
    "\n",
    "        # --- TODO 9: Evaluate perplexity with the hook active ---\n",
    "        # With the hook in place, any call to the model will use fake-quantized\n",
    "        # activations for the target layer. Run the PPL evaluation again.\n",
    "        ppl = 0.0 # Call eval_ppl_cpu_texts(...)\n",
    "\n",
    "\n",
    "        # --- TODO 10: CRITICAL: Remove the hook ---\n",
    "        # If you don't remove the hook, it will persist and affect subsequent\n",
    "        # evaluations for other layers.\n",
    "        # Hint: `hook.remove()`\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "\n",
    "        # --- TODO 11: Store the results ---\n",
    "        # Append a dictionary to the `results` list containing:\n",
    "        #   - \"layer\": the layer name\n",
    "        #   - \"ppl_with_act_fakequant\": the PPL you just calculated\n",
    "        #   - \"ppl_baseline\": the baseline PPL\n",
    "        #   - \"delta_ppl\": the difference between the new PPL and baseline\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "\n",
    "    # --- TODO 12: Create and return a tidy DataFrame ---\n",
    "    # Convert the `results` list into a pandas DataFrame.\n",
    "    # Sort the DataFrame by \"delta_ppl\" to easily see which layers are most sensitive.\n",
    "    # Reset the index for clean presentation.\n",
    "    # df = pd.DataFrame(results).sort_values(\"delta_ppl\").reset_index(drop=True)\n",
    "    # return df\n",
    "    raise NotImplementedError(\"Return the final DataFrame once all TODOs are implemented.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e0ccb0",
   "metadata": {},
   "source": [
    "# Run Activation Fake-Quant Sensitivity Analysis\n",
    "This cell:\n",
    "- Runs the activation fake-quant sensitivity analysis for the last 8 linear layers.\n",
    "- Outputs a DataFrame showing the perplexity drift for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afa1dd63-82b6-42bb-bd2a-729fd080bf49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer</th>\n",
       "      <th>ppl_with_act_fakequant</th>\n",
       "      <th>ppl_baseline</th>\n",
       "      <th>delta_ppl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>model.layers.21.mlp.up_proj</td>\n",
       "      <td>10.717280</td>\n",
       "      <td>10.961409</td>\n",
       "      <td>-0.244129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>model.layers.21.mlp.gate_proj</td>\n",
       "      <td>10.731192</td>\n",
       "      <td>10.961409</td>\n",
       "      <td>-0.230217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lm_head</td>\n",
       "      <td>10.904405</td>\n",
       "      <td>10.961409</td>\n",
       "      <td>-0.057004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>model.layers.21.self_attn.v_proj</td>\n",
       "      <td>10.955287</td>\n",
       "      <td>10.961409</td>\n",
       "      <td>-0.006121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>model.layers.21.self_attn.q_proj</td>\n",
       "      <td>10.959927</td>\n",
       "      <td>10.961409</td>\n",
       "      <td>-0.001482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>model.layers.21.self_attn.o_proj</td>\n",
       "      <td>10.968125</td>\n",
       "      <td>10.961409</td>\n",
       "      <td>0.006716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>model.layers.21.self_attn.k_proj</td>\n",
       "      <td>10.974499</td>\n",
       "      <td>10.961409</td>\n",
       "      <td>0.013091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>model.layers.21.mlp.down_proj</td>\n",
       "      <td>11.220519</td>\n",
       "      <td>10.961409</td>\n",
       "      <td>0.259110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              layer  ppl_with_act_fakequant  ppl_baseline  \\\n",
       "0       model.layers.21.mlp.up_proj               10.717280     10.961409   \n",
       "1     model.layers.21.mlp.gate_proj               10.731192     10.961409   \n",
       "2                           lm_head               10.904405     10.961409   \n",
       "3  model.layers.21.self_attn.v_proj               10.955287     10.961409   \n",
       "4  model.layers.21.self_attn.q_proj               10.959927     10.961409   \n",
       "5  model.layers.21.self_attn.o_proj               10.968125     10.961409   \n",
       "6  model.layers.21.self_attn.k_proj               10.974499     10.961409   \n",
       "7     model.layers.21.mlp.down_proj               11.220519     10.961409   \n",
       "\n",
       "   delta_ppl  \n",
       "0  -0.244129  \n",
       "1  -0.230217  \n",
       "2  -0.057004  \n",
       "3  -0.006121  \n",
       "4  -0.001482  \n",
       "5   0.006716  \n",
       "6   0.013091  \n",
       "7   0.259110  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_act_sens = activation_fake_quant_sensitivity_cpu(model, tokenizer, last_k_linear=8)\n",
    "df_act_sens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc487708-4d5c-46c6-a651-75651328c833",
   "metadata": {},
   "source": [
    "# Exercise 2 — CPU Decoding Strategy Benchmark (No Quant, No Training)\n",
    "This exercise:\n",
    "- Benchmarks latency and throughput for different decoding strategies on CPU.\n",
    "- Evaluates strategies such as greedy decoding, top-k sampling, top-p sampling, and beam search.\n",
    "- Measures performance across various prompt lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e747eb1d-733d-4ef0-94a9-ecf21afc4c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpu_decoding_strategy_benchmark(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    base_text=\"This is a short prompt for CPU decoding benchmark. \",\n",
    "    prompt_lengths=(16, 64, 256),\n",
    "    new_tokens=64,\n",
    "    runs=3,\n",
    "):\n",
    "    \"\"\"\n",
    "    Measure CPU decoding latency & throughput across decoding strategies and prompt lengths.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame with columns:\n",
    "      ['prompt_len_tokens(approx)', 'strategy', 'latency_ms', 'throughput_tokens_per_s']\n",
    "\n",
    "    Student TODOs:\n",
    "      - Ensure the model is on CPU and in eval() mode.\n",
    "      - Define a few decoding strategies (greedy, top-k, top-p, beam).\n",
    "      - Build prompts that roughly match target token lengths.\n",
    "      - Warm up each strategy before timing.\n",
    "      - Time multiple runs and average.\n",
    "      - Return a tidy DataFrame.\n",
    "    \"\"\"\n",
    "    # --- Imports (keep local so function is self-contained) ---\n",
    "    import time\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import pandas as pd\n",
    "\n",
    "    # ---------- TODO 1: Put model in eval mode and move to CPU ----------\n",
    "    # Hints:\n",
    "    #   - Use model.eval()\n",
    "    #   - Use model.to(\"cpu\")\n",
    "    #   - Disable grads with torch.set_grad_enabled(False)\n",
    "    # YOUR CODE HERE\n",
    "    # model.eval()\n",
    "    # model.to(\"cpu\")\n",
    "    # torch.set_grad_enabled(False)\n",
    "\n",
    "    # ---------- TODO 2: Define decoding strategies ----------\n",
    "    # Provide a small set with different search behaviors:\n",
    "    #   - \"greedy\": no sampling\n",
    "    #   - \"topk\":   do_sample=True, top_k=50\n",
    "    #   - \"topp\":   do_sample=True, top_p=0.9\n",
    "    #   - \"beam4\":  beam search with num_beams=4 (no sampling)\n",
    "    # Keep temperature at 1.0 unless you want them to explore.\n",
    "    strategies = {\n",
    "    }\n",
    "\n",
    "    # ---------- Helper: approximate a prompt with ~L tokens ----------\n",
    "    # TODO 3: Implement build_prompt(L) so it returns a string whose tokenized\n",
    "    # length is approximately L. You can:\n",
    "    #   - tokenize base_text once to get a token list\n",
    "    #   - repeat it to exceed L\n",
    "    #   - decode back to string and (optionally) trim by tokens or characters\n",
    "    def build_prompt(L: int) -> str:\n",
    "        raise NotImplementedError(\"build_prompt(L) not implemented\")\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    # ---------- Main loop over prompt lengths ----------\n",
    "    for L in prompt_lengths:\n",
    "        # TODO 4: Build prompt and encode once on CPU\n",
    "        #   - Use tokenizer(prompt, return_tensors=\"pt\")\n",
    "        #   - Keep the encoded dict on CPU (no .to('cuda'))\n",
    "        prompt = None  # = build_prompt(L)\n",
    "        enc = None     # = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        # Optional: record the actual input length from enc[\"input_ids\"].size(1)\n",
    "        # input_len = enc[\"input_ids\"].size(1)\n",
    "\n",
    "        # ---------- Loop over decoding strategies ----------\n",
    "        for name, gen_kwargs in strategies.items():\n",
    "            # TODO 5: Warm-up (short generate) to trigger kernels/JIT\n",
    "            #   - e.g., max_new_tokens=8\n",
    "            # with torch.inference_mode():\n",
    "            #     _ = model.generate(**enc, max_new_tokens=8, **gen_kwargs)\n",
    "\n",
    "            # TODO 6: Timed runs (average over `runs`)\n",
    "            times = []\n",
    "\n",
    "            # TODO 7: Compute metrics\n",
    "\n",
    "            # TODO 8: Append a result row\n",
    "\n",
    "    # ---------- TODO 9: Return a tidy DataFrame sorted by prompt length then strategy ----------\n",
    "    # df = pd.DataFrame(rows).sort_values([\"prompt_len_tokens(approx)\", \"strategy\"]).reset_index(drop=True)\n",
    "    # return df\n",
    "    raise NotImplementedError(\"Return the DataFrame once all TODOs are implemented\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a046c1ad",
   "metadata": {},
   "source": [
    "# Run CPU Decoding Strategy Benchmark\n",
    "This cell:\n",
    "- Runs the decoding strategy benchmark for prompt lengths of 16, 64, and 256 tokens.\n",
    "- Outputs a DataFrame showing latency and throughput for each strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce861757-5519-4f58-9a1e-1a63b0a50ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_len_tokens(approx)</th>\n",
       "      <th>strategy</th>\n",
       "      <th>latency_ms</th>\n",
       "      <th>throughput_tokens_per_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>beam4</td>\n",
       "      <td>5062.797904</td>\n",
       "      <td>6.320616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>greedy</td>\n",
       "      <td>2567.549944</td>\n",
       "      <td>12.463243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>topk</td>\n",
       "      <td>2583.953261</td>\n",
       "      <td>12.384125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>topp</td>\n",
       "      <td>2620.415092</td>\n",
       "      <td>12.211806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>beam4</td>\n",
       "      <td>5132.568240</td>\n",
       "      <td>6.234695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>64</td>\n",
       "      <td>greedy</td>\n",
       "      <td>2692.276001</td>\n",
       "      <td>11.885854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>64</td>\n",
       "      <td>topk</td>\n",
       "      <td>2410.951018</td>\n",
       "      <td>13.272771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>64</td>\n",
       "      <td>topp</td>\n",
       "      <td>2791.317344</td>\n",
       "      <td>11.464121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>256</td>\n",
       "      <td>beam4</td>\n",
       "      <td>5807.430387</td>\n",
       "      <td>5.510182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>256</td>\n",
       "      <td>greedy</td>\n",
       "      <td>2800.542116</td>\n",
       "      <td>11.426359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>256</td>\n",
       "      <td>topk</td>\n",
       "      <td>2793.191195</td>\n",
       "      <td>11.456430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>256</td>\n",
       "      <td>topp</td>\n",
       "      <td>2317.993283</td>\n",
       "      <td>13.805044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    prompt_len_tokens(approx) strategy   latency_ms  throughput_tokens_per_s\n",
       "0                          16    beam4  5062.797904                 6.320616\n",
       "1                          16   greedy  2567.549944                12.463243\n",
       "2                          16     topk  2583.953261                12.384125\n",
       "3                          16     topp  2620.415092                12.211806\n",
       "4                          64    beam4  5132.568240                 6.234695\n",
       "5                          64   greedy  2692.276001                11.885854\n",
       "6                          64     topk  2410.951018                13.272771\n",
       "7                          64     topp  2791.317344                11.464121\n",
       "8                         256    beam4  5807.430387                 5.510182\n",
       "9                         256   greedy  2800.542116                11.426359\n",
       "10                        256     topk  2793.191195                11.456430\n",
       "11                        256     topp  2317.993283                13.805044"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cpu_decode = cpu_decoding_strategy_benchmark(model, tokenizer, prompt_lengths=(16, 64, 256), new_tokens=32, runs=2)\n",
    "df_cpu_decode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3d28f6-9ffb-414c-9495-5849de991ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
