{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "953202e5-05ab-41f3-bc8d-66d15b626a78",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16999039",
   "metadata": {},
   "source": [
    "# Model Quantization Demo: TinyLlama Optimization\n",
    "\n",
    "This notebook demonstrates two approaches to model quantization:\n",
    "1. GPU-based weight-only quantization using Optimum-Quanto\n",
    "2. CPU-based dynamic quantization using PyTorch\n",
    "\n",
    "We'll use the TinyLlama-1.1B-Chat model as our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ec4f653-dc32-46e1-b3a2-6e3c3496f30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 19:49:33.114781: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-31 19:49:33.128764: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-08-31 19:49:33.146588: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-08-31 19:49:33.151886: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-08-31 19:49:33.164321: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os, shutil, time, math, gc, tempfile, json, contextlib\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Optimum (Quanto) for post-training quantization (static, weight-only int8)\n",
    "from optimum.quanto import quantize, freeze, qint8\n",
    "\n",
    "MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "DEVICE   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEED     = 42\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26def653",
   "metadata": {},
   "source": [
    "# Setup and Dependencies\n",
    "\n",
    "This cell:\n",
    "- Imports required Python libraries for model handling, quantization, and data processing\n",
    "- Sets up the TinyLlama model configuration\n",
    "- Configures device settings (GPU if available, otherwise CPU)\n",
    "- Sets random seed for reproducibility\n",
    "- Initializes CUDA random seed if GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f7f7c8a-7645-44b8-9146-e25bdc62fd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1) Utilities -------------------------------------------------------------\n",
    "def dir_size_mb(path: str) -> float:\n",
    "    total = 0\n",
    "    for root, _, files in os.walk(path):\n",
    "        for f in files:\n",
    "            total += os.path.getsize(os.path.join(root, f))\n",
    "    return total / (1024**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0f5623",
   "metadata": {},
   "source": [
    "# Utility Function: Directory Size Calculator\n",
    "\n",
    "Defines a helper function `dir_size_mb` that:\n",
    "- Recursively calculates the total size of a directory\n",
    "- Converts the size from bytes to megabytes\n",
    "- Used for measuring model storage requirements before and after quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5f3f6f4-ab44-41af-916c-a4ad5481a270",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextlib.contextmanager\n",
    "def torch_cuda_monitor():\n",
    "    \"\"\"Context manager to measure peak GPU memory in MB.\"\"\"\n",
    "    if DEVICE == \"cuda\":\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.synchronize()\n",
    "        start_alloc = torch.cuda.memory_allocated()\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            torch.cuda.synchronize()\n",
    "            peak = torch.cuda.max_memory_allocated()\n",
    "            torch.cuda.empty_cache()\n",
    "            # return values indirectly by storing on the function object\n",
    "            torch_cuda_monitor.peak_mb = peak / (1024**2)\n",
    "            torch_cuda_monitor.start_mb = start_alloc / (1024**2)\n",
    "    else:\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            torch_cuda_monitor.peak_mb = 0.0\n",
    "            torch_cuda_monitor.start_mb = 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25424a5",
   "metadata": {},
   "source": [
    "# GPU Memory Monitor Context Manager\n",
    "\n",
    "Implements a context manager that:\n",
    "- Tracks GPU memory usage during model operations\n",
    "- Records peak memory allocation\n",
    "- Measures memory changes from start to peak\n",
    "- Handles both GPU and CPU scenarios\n",
    "- Automatically cleans up memory after monitoring\n",
    "- Returns results in megabytes for easy comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09cf124e-9e5d-4f87-8aa3-3a3254f3e3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GenMetrics:\n",
    "    latency_s: float\n",
    "    tokens_per_sec: float\n",
    "    peak_gpu_mem_mb: float"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f6c7d0",
   "metadata": {},
   "source": [
    "# Generation Metrics Data Class\n",
    "\n",
    "Defines a dataclass `GenMetrics` to store key performance metrics:\n",
    "- Latency in seconds\n",
    "- Generation throughput (tokens per second)\n",
    "- Peak GPU memory usage in megabytes\n",
    "This structured format enables consistent reporting across different model configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc104801-a488-487d-8f5d-d2d1229c320a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_generate(model, tokenizer, prompt: str, max_new_tokens=64, runs=3) -> GenMetrics:\n",
    "    \"\"\"Measure latency, throughput, and peak GPU memory for text generation.\"\"\"\n",
    "    model.eval()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    # Warmup\n",
    "    with torch.inference_mode():\n",
    "        _ = model.generate(**inputs, max_new_tokens=8, do_sample=False, use_cache=True)\n",
    "\n",
    "    latencies, tps = [], []\n",
    "    with torch_cuda_monitor():\n",
    "        for _ in range(runs):\n",
    "            if DEVICE == \"cuda\":\n",
    "                torch.cuda.synchronize()\n",
    "            t0 = time.perf_counter()\n",
    "            with torch.inference_mode():\n",
    "                out = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False, use_cache=True)\n",
    "            if DEVICE == \"cuda\":\n",
    "                torch.cuda.synchronize()\n",
    "            t1 = time.perf_counter()\n",
    "\n",
    "            gen_len = out.shape[1] - input_len\n",
    "            lat = t1 - t0\n",
    "            latencies.append(lat)\n",
    "            tps.append(gen_len / lat if lat > 0 else float(\"nan\"))\n",
    "\n",
    "    return GenMetrics(\n",
    "        latency_s=sum(latencies)/len(latencies),\n",
    "        tokens_per_sec=sum(tps)/len(tps),\n",
    "        peak_gpu_mem_mb=getattr(torch_cuda_monitor, \"peak_mb\", 0.0),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288e711e",
   "metadata": {},
   "source": [
    "# Text Generation Performance Measurement\n",
    "\n",
    "Implements a comprehensive function to measure model generation performance:\n",
    "- Takes a model, tokenizer, and prompt as input\n",
    "- Performs multiple generation runs for reliable measurements\n",
    "- Includes warmup run to eliminate cold-start effects\n",
    "- Measures:\n",
    "  - Generation latency\n",
    "  - Token throughput\n",
    "  - Peak GPU memory usage\n",
    "- Handles both CPU and GPU scenarios\n",
    "- Uses inference mode for optimal performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8100b7cb-8fc1-4608-abc5-c04aa4c63a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_perplexity(model, tokenizer, seq_len=128) -> float:\n",
    "    \"\"\"\n",
    "    Self-contained perplexity estimate using a small built-in eval text.\n",
    "    Keeps evaluation light but still allows FP32 vs INT8 comparison.\n",
    "    \"\"\"\n",
    "    eval_text = (\n",
    "        \"Quantization reduces the precision of neural network weights and activations. \"\n",
    "        \"This process shrinks model size, lowers memory use, and can speed up inference. \"\n",
    "        \"The tradeoff is a small drop in accuracy. \"\n",
    "        \"Perplexity measures how well a language model predicts text: \"\n",
    "        \"a lower perplexity means the model is more confident in its predictions. \"\n",
    "        \"Large language models like LLaMA or TinyLlama are evaluated on benchmarks such as WikiText, \"\n",
    "        \"where perplexity is calculated over thousands of tokens. \"\n",
    "        \"In practice, we only need a small text sample to compare relative changes. \"\n",
    "        \"By quantizing a model to 8-bit, we can observe whether perplexity increases significantly. \"\n",
    "        \"If the rise is modest while speed and memory improve, quantization is usually a good trade-off. \"\n",
    "        \"This evaluation text is deliberately extended to ensure enough tokens for testing.\"\n",
    "    )\n",
    "\n",
    "    enc = tokenizer(eval_text, return_tensors=\"pt\")\n",
    "    input_ids = enc[\"input_ids\"][0]\n",
    "\n",
    "    usable = (len(input_ids) // seq_len) * seq_len\n",
    "    input_ids = input_ids[:usable + 1]\n",
    "    if len(input_ids) <= seq_len:\n",
    "        raise ValueError(\"Not enough tokens for perplexity calculation. Try reducing seq_len.\")\n",
    "\n",
    "    nll_sum, tok_count = 0.0, 0\n",
    "    model.eval()\n",
    "\n",
    "    for start in range(0, len(input_ids) - 1 - seq_len, seq_len):\n",
    "        chunk = input_ids[start:start+seq_len+1]\n",
    "        inp = chunk[:-1].unsqueeze(0).to(DEVICE)\n",
    "        labels = chunk[1:].unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        out = model(input_ids=inp, labels=labels)\n",
    "        nll_sum += float(out.loss) * labels.numel()\n",
    "        tok_count += labels.numel()\n",
    "\n",
    "    return math.exp(nll_sum / max(1, tok_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d92382",
   "metadata": {},
   "source": [
    "# Perplexity Calculation Function\n",
    "\n",
    "Implements a self-contained perplexity evaluation:\n",
    "- Uses a predefined evaluation text about quantization\n",
    "- Processes text in fixed-length sequences\n",
    "- Calculates token-level negative log likelihood\n",
    "- Computes perplexity as exp(average NLL)\n",
    "- Enables comparison between FP32 and INT8 models\n",
    "- Provides quick quality assessment without external datasets\n",
    "- Uses sliding window approach for efficient computation\n",
    "- Runs in inference mode for optimal performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed8600e2-772c-4fd3-a35e-10b51d247f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_size(model, tokenizer, out_dir: str) -> float:\n",
    "    if os.path.exists(out_dir):\n",
    "        shutil.rmtree(out_dir)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    tokenizer.save_pretrained(out_dir)\n",
    "    model.save_pretrained(out_dir, safe_serialization=True)\n",
    "    return dir_size_mb(out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf003929",
   "metadata": {},
   "source": [
    "# Model and Tokenizer Save Utility\n",
    "\n",
    "Implements a function that:\n",
    "- Safely saves model and tokenizer to disk\n",
    "- Cleans up existing directory if present\n",
    "- Creates new directory for saved files\n",
    "- Uses safe serialization for model weights\n",
    "- Returns total size of saved files in MB\n",
    "This helps track model size changes after quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea589669-30ec-4f20-809a-67981d4e01b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_row(title, size_mb, lat_s, tps, gpu_mb, ppl):\n",
    "    print(\n",
    "        f\"{title:18s} | Size: {size_mb:8.1f} MB | Latency: {lat_s:7.3f} s | \"\n",
    "        f\"Throughput: {tps:7.2f} tok/s | Peak VRAM: {gpu_mb:7.1f} MB | PPL: {ppl:7.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5979f72",
   "metadata": {},
   "source": [
    "# Metric Reporting Function\n",
    "\n",
    "Defines a formatted print function that displays:\n",
    "- Model variant title\n",
    "- Model size in MB\n",
    "- Generation latency in seconds\n",
    "- Generation throughput in tokens/second\n",
    "- Peak VRAM usage in MB\n",
    "- Perplexity score\n",
    "Enables consistent comparison between different model configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb783181-a538-4435-b5d9-bd4a7020a092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2) Load tokenizer --------------------------------------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "# Some chat models have no pad token; make generation/perplexity robust:\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b414446f",
   "metadata": {},
   "source": [
    "# Tokenizer Initialization\n",
    "\n",
    "Sets up the tokenizer for text processing:\n",
    "- Loads the TinyLlama tokenizer with fast tokenization\n",
    "- Ensures pad token is properly configured\n",
    "- Falls back to EOS token as pad token if needed\n",
    "This configuration is shared between baseline and quantized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca49638d-435b-4537-a677-86261d780471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Baseline FP32 ==\n",
      "FP32 (baseline)    | Size:   4200.3 MB | Latency:   2.644 s | Throughput:   48.42 tok/s | Peak VRAM:  4212.9 MB | PPL:    1.00\n"
     ]
    }
   ],
   "source": [
    "# --- 3) Baseline: FP32 --------------------------------------------------------\n",
    "print(\"\\n== Baseline FP32 ==\")\n",
    "baseline_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float32,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "baseline_model.to(DEVICE)\n",
    "\n",
    "baseline_size_mb = save_and_size(baseline_model, tokenizer, out_dir=\"tinyllama_fp32\")\n",
    "baseline_gen = measure_generate(\n",
    "    baseline_model,\n",
    "    tokenizer,\n",
    "    prompt=\"Explain quantization in one paragraph for ML engineers.\",\n",
    "    max_new_tokens=128,\n",
    "    runs=3,\n",
    ")\n",
    "baseline_ppl = compute_perplexity(baseline_model, tokenizer, seq_len=128)\n",
    "\n",
    "print_row(\"FP32 (baseline)\", baseline_size_mb, baseline_gen.latency_s, baseline_gen.tokens_per_sec,\n",
    "          baseline_gen.peak_gpu_mem_mb, baseline_ppl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f09563",
   "metadata": {},
   "source": [
    "# Baseline FP32 Model Evaluation\n",
    "\n",
    "Loads and evaluates the full-precision (FP32) model:\n",
    "- Initializes model with 32-bit floating point precision\n",
    "- Moves model to specified device (GPU/CPU)\n",
    "- Measures:\n",
    "  - Model size on disk\n",
    "  - Generation performance (latency & throughput)\n",
    "  - Memory usage\n",
    "  - Perplexity score\n",
    "This establishes the baseline for comparing quantized versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41bd6114-154f-4ea8-896c-b6f28d278ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== PTQ INT8 (Optimum-Quanto, weight-only) ==\n",
      "INT8 (Quanto)      | Size:   1242.5 MB | Latency:   6.658 s | Throughput:   19.23 tok/s | Peak VRAM:  5765.4 MB | PPL:    1.00\n"
     ]
    }
   ],
   "source": [
    "# --- 4) Post-Training Quantization: 8-bit (static, weight-only) --------------\n",
    "# Fresh load in FP32, then quantize weights to int8 via Optimum-Quanto.\n",
    "print(\"\\n== PTQ INT8 (Optimum-Quanto, weight-only) ==\")\n",
    "q_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float32,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "# Static PTQ (weight-only): no calibration set required.\n",
    "# This converts Linear weights to int8-packed format and wires quant/dequant where needed.\n",
    "quantize(q_model, weights=qint8)\n",
    "freeze(q_model)               # finalize quantization graphs / params\n",
    "q_model.to(DEVICE)\n",
    "\n",
    "q_size_mb = save_and_size(q_model, tokenizer, out_dir=\"tinyllama_int8_quanto\")\n",
    "\n",
    "q_gen = measure_generate(\n",
    "    q_model,\n",
    "    tokenizer,\n",
    "    prompt=\"Explain quantization in one paragraph for ML engineers.\",\n",
    "    max_new_tokens=128,\n",
    "    runs=3,\n",
    ")\n",
    "q_ppl = compute_perplexity(q_model, tokenizer, seq_len=128)\n",
    "\n",
    "print_row(\"INT8 (Quanto)\", q_size_mb, q_gen.latency_s, q_gen.tokens_per_sec, q_gen.peak_gpu_mem_mb, q_ppl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9017813",
   "metadata": {},
   "source": [
    "# Weight-Only INT8 Quantization\n",
    "\n",
    "Implements static weight-only quantization using Optimum-Quanto:\n",
    "- Loads fresh FP32 model\n",
    "- Applies INT8 quantization to weights only\n",
    "- Uses static quantization (no calibration needed)\n",
    "- Freezes quantization parameters\n",
    "- Evaluates quantized model performance:\n",
    "  - Size reduction\n",
    "  - Generation speed\n",
    "  - Memory usage\n",
    "  - Impact on perplexity\n",
    "Demonstrates the benefits of weight-only quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcd0af01-42ac-42f6-a745-69f5669f918b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved summary -> ptq_tinyllama_summary.json\n"
     ]
    }
   ],
   "source": [
    "# --- 5) Summary JSON (optional) ----------------------------------------------\n",
    "summary = {\n",
    "    \"device\": DEVICE,\n",
    "    \"model\": MODEL_ID,\n",
    "    \"seed\": SEED,\n",
    "    \"baseline_fp32\": {\n",
    "        \"size_mb\": baseline_size_mb,\n",
    "        \"latency_s\": baseline_gen.latency_s,\n",
    "        \"tokens_per_sec\": baseline_gen.tokens_per_sec,\n",
    "        \"peak_gpu_mem_mb\": baseline_gen.peak_gpu_mem_mb,\n",
    "        \"perplexity\": baseline_ppl,\n",
    "    },\n",
    "    \"int8_quanto\": {\n",
    "        \"size_mb\": q_size_mb,\n",
    "        \"latency_s\": q_gen.latency_s,\n",
    "        \"tokens_per_sec\": q_gen.tokens_per_sec,\n",
    "        \"peak_gpu_mem_mb\": q_gen.peak_gpu_mem_mb,\n",
    "        \"perplexity\": q_ppl,\n",
    "    },\n",
    "}\n",
    "with open(\"ptq_tinyllama_summary.json\", \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"\\nSaved summary -> ptq_tinyllama_summary.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e1543b",
   "metadata": {},
   "source": [
    "# Results Summary and Export\n",
    "\n",
    "Creates and saves a comprehensive JSON summary containing:\n",
    "- Test environment details (device, model, seed)\n",
    "- Baseline FP32 metrics:\n",
    "  - Model size\n",
    "  - Latency\n",
    "  - Throughput\n",
    "  - Memory usage\n",
    "  - Perplexity\n",
    "- INT8 quantized metrics\n",
    "  - All corresponding measurements\n",
    "Enables easy analysis and comparison of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c945bd5-a1be-44c3-a869-73bd852985c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a739eff-c363-43d0-8b42-9dc81b1caf25",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88873421",
   "metadata": {},
   "source": [
    "# CPU-Based Dynamic Quantization Demo\n",
    "\n",
    "This section demonstrates PyTorch's dynamic quantization approach:\n",
    "- Focuses on CPU-only execution\n",
    "- Uses dynamic quantization (weights static, activations dynamic)\n",
    "- Compares performance with FP32 baseline\n",
    "- Showcases CPU-specific optimization techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72e5aca4-569f-4749-bac1-ee30703eae2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.ao.quantization import quantize_dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2496fdf4",
   "metadata": {},
   "source": [
    "# Import Dependencies for CPU Quantization\n",
    "\n",
    "Sets up required libraries for dynamic quantization:\n",
    "- PyTorch core modules\n",
    "- Transformers for model handling\n",
    "- Quantization utilities from torch.ao.quantization\n",
    "Focuses on CPU-specific implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20f98e74-4215-4169-ae85-c362bf4d0794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Config (CPU-only demo)\n",
    "# -----------------------------\n",
    "MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # swap to \"sshleifer/tiny-gpt2\" if CPU is tight\n",
    "DEVICE = \"cpu\"\n",
    "MAX_NEW_TOKENS = 128\n",
    "RUNS = 3\n",
    "PROMPT = \"Quantization test: explain why int8 dynamic quantization can be faster on CPU.\"\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "torch.set_num_threads(max(1, os.cpu_count() or 1))  # let PyTorch use available cores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0994d28",
   "metadata": {},
   "source": [
    "# Configuration Setup\n",
    "\n",
    "Establishes test parameters and environment:\n",
    "- Selects model (TinyLlama or fallback option)\n",
    "- Forces CPU device usage\n",
    "- Sets generation parameters\n",
    "- Configures test runs count\n",
    "- Defines evaluation prompt\n",
    "- Disables gradient computation\n",
    "- Optimizes PyTorch thread count for CPU\n",
    "These settings ensure consistent and optimized CPU testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9cca988-f1cf-42a2-82dc-d748b8d68732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_generate(model, tokenizer, prompt=PROMPT, max_new_tokens=MAX_NEW_TOKENS, runs=RUNS):\n",
    "    model.eval()\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    enc = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    # Warmup\n",
    "    with torch.inference_mode():\n",
    "        _ = model.generate(**enc, max_new_tokens=8, use_cache=True)\n",
    "\n",
    "    latencies, throughputs = [], []\n",
    "    for _ in range(runs):\n",
    "        t0 = time.perf_counter()\n",
    "        with torch.inference_mode():\n",
    "            out = model.generate(**enc, max_new_tokens=max_new_tokens, use_cache=True)\n",
    "        t1 = time.perf_counter()\n",
    "        gen_len = out.shape[1] - enc[\"input_ids\"].shape[1]\n",
    "        lat = t1 - t0\n",
    "        latencies.append(lat)\n",
    "        throughputs.append(gen_len / lat)\n",
    "\n",
    "    return sum(latencies)/len(latencies), sum(throughputs)/len(throughputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bdd159",
   "metadata": {},
   "source": [
    "# Performance Measurement Function\n",
    "\n",
    "Implements a streamlined function for CPU performance testing:\n",
    "- Takes model, tokenizer, and generation parameters\n",
    "- Handles padding for consistent input processing\n",
    "- Includes warmup run to prime the system\n",
    "- Measures multiple runs for reliable statistics\n",
    "- Calculates:\n",
    "  - Average generation latency\n",
    "  - Token generation throughput\n",
    "Perfect for comparing FP32 vs INT8 performance on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95fafbda-0c09-4064-b6d7-2728690d787c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Tokenizer (shared)\n",
    "# -----------------------------\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "if tok.pad_token_id is None:\n",
    "    tok.pad_token_id = tok.eos_token_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c96b8f",
   "metadata": {},
   "source": [
    "# Tokenizer Setup for CPU Testing\n",
    "\n",
    "Initializes the tokenizer with CPU-specific configurations:\n",
    "- Loads the model's tokenizer\n",
    "- Enables fast tokenization\n",
    "- Ensures proper padding token setup\n",
    "- Uses EOS token as fallback pad token if needed\n",
    "Prepares for consistent text processing across model variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "924f702c-4cbe-4a64-b890-a3175af2f75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== CPU FP32 Baseline ==\n",
      "FP32 (CPU) | Latency 0.611s | Throughput 1.66 tok/s\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Baseline: FP32 on CPU\n",
    "# -----------------------------\n",
    "print(\"== CPU FP32 Baseline ==\")\n",
    "model_fp32 = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=torch.float32).to(DEVICE).eval()\n",
    "model_fp32.config.use_cache = True\n",
    "\n",
    "lat_fp32, tps_fp32 = measure_generate(model_fp32, tok)\n",
    "print(f\"FP32 (CPU) | Latency {lat_fp32:.3f}s | Throughput {tps_fp32:.2f} tok/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16812e3",
   "metadata": {},
   "source": [
    "# FP32 Baseline Model on CPU\n",
    "\n",
    "Loads and evaluates the baseline full-precision model:\n",
    "- Initializes model in FP32 precision\n",
    "- Moves model to CPU and sets evaluation mode\n",
    "- Enables attention caching for faster generation\n",
    "- Measures and prints:\n",
    "  - Generation latency\n",
    "  - Token throughput\n",
    "Establishes baseline CPU performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b3c98f8-c8e7-43be-bb4f-e6149602ffe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Quantized: INT8 dynamic (CPU)\n",
    "# -----------------------------\n",
    "# Quantize only Linear layers to int8 (native PyTorch). This is weight-only int8 + dynamic activation quant.\n",
    "model_int8 = quantize_dynamic(\n",
    "    model_fp32.cpu(),\n",
    "    {torch.nn.Linear},\n",
    "    dtype=torch.qint8\n",
    ").eval()\n",
    "# (Optional) free original object to reduce memory\n",
    "del model_fp32\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85684a3",
   "metadata": {},
   "source": [
    "# Dynamic INT8 Quantization for CPU\n",
    "\n",
    "Implements PyTorch's dynamic quantization:\n",
    "- Targets Linear layers for quantization\n",
    "- Converts weights to INT8 format\n",
    "- Uses dynamic quantization for activations\n",
    "- Keeps other layers in FP32\n",
    "- Cleans up original model to free memory\n",
    "This approach is specifically optimized for CPU inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c0ced22-a4a4-4678-a05a-9894682ea773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Summary ==\n",
      "CPU FP32 latency: 0.611s | CPU INT8 latency: 0.220s | Speedup: 2.78x\n",
      "CPU FP32 tput:   1.66 tok/s | CPU INT8 tput:   4.55 tok/s\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Summary\n",
    "# -----------------------------\n",
    "lat_int8, tps_int8 = measure_generate(model_int8, tok)\n",
    "speedup = lat_fp32 / lat_int8 if lat_int8 > 0 else float(\"inf\")\n",
    "\n",
    "\n",
    "print(\"\\n== Summary ==\")\n",
    "print(f\"CPU FP32 latency: {lat_fp32:.3f}s | CPU INT8 latency: {lat_int8:.3f}s | Speedup: {speedup:.2f}x\")\n",
    "print(f\"CPU FP32 tput:   {tps_fp32:.2f} tok/s | CPU INT8 tput:   {tps_int8:.2f} tok/s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3234fe",
   "metadata": {},
   "source": [
    "# Performance Comparison and Results\n",
    "\n",
    "Evaluates and compares model variants:\n",
    "- Measures INT8 model performance\n",
    "- Calculates speedup factor over FP32\n",
    "- Compares:\n",
    "  - Generation latency\n",
    "  - Token throughput\n",
    "  - Relative performance gains\n",
    "Provides clear metrics for quantization benefits on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8df066-22a2-4194-9ecf-895cc60a112d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280de51d-067e-4755-8c2f-4640c1adf542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5895b706-8118-4307-9fa6-bfc17f5492a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
