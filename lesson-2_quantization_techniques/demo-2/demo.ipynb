{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb2b1242-8e58-4d60-82d1-3972b6186d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 00:00:41.315292: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-05 00:00:41.330496: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-08-05 00:00:41.351020: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-08-05 00:00:41.356649: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-08-05 00:00:41.370077: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-05 00:00:49 [WARNING][auto_accelerator.py:454] Auto detect accelerator: CUDA_Accelerator.\n",
      "2025-08-05 00:00:49 [WARNING][modeling_auto.py:807] please install transformers>=4.46 for quantizing Qwen2VLForConditionalGeneration.\n",
      "2025-08-05 00:00:49 [WARNING][modeling_auto.py:814] please install transformers>=4.46 for quantizing MllamaForConditionalGeneration.\n",
      "2025-08-05 00:00:49 [WARNING][modeling_auto.py:821] please install transformers>=4.46 for quantizing LlavaForConditionalGeneration.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import shutil\n",
    "import torch\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from datasets import load_dataset, DownloadConfig\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    EvalPrediction,\n",
    ")\n",
    "from optimum.intel import INCTrainer, INCModelForCausalLM\n",
    "from neural_compressor import QuantizationAwareTrainingConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd3cb91",
   "metadata": {},
   "source": [
    "# Quantization-Aware Training with Intel Neural Compressor\n",
    "\n",
    "This notebook demonstrates how to implement Quantization-Aware Training (QAT) using Intel's Neural Compressor framework.\n",
    "\n",
    "## Required Dependencies\n",
    "Setting up essential libraries for:\n",
    "- Core Python utilities: os, time, math, shutil\n",
    "- Deep Learning: PyTorch\n",
    "- Numerical operations: NumPy\n",
    "- Model & Data handling: \n",
    "  - Hugging Face Transformers\n",
    "  - Datasets library\n",
    "  - Evaluation metrics\n",
    "- Intel optimization tools:\n",
    "  - Neural Compressor\n",
    "  - Optimum Intel integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afdb9262-9057-474c-a135-fcf214fd5ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── SETTINGS ────────────────────────────────────────────────────────────────\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "PROMPT = (\n",
    "    \"Over the next decade, sustainable energy solutions will revolutionize \"\n",
    "    \"global power grids, reducing carbon footprints and fostering resilient \"\n",
    "    \"communities through innovative storage and distribution technologies.\"\n",
    ")\n",
    "PERP_TEXT = (\n",
    "    \"Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast \"\n",
    "    \"to the natural intelligence displayed by humans and animals. Leading AI textbooks \"\n",
    "    \"define the field as the study of intelligent agents: any system that perceives \"\n",
    "    \"its environment and takes actions that maximize its chance of achieving its goals.\"\n",
    ")\n",
    "MAX_NEW_TOKENS = 50\n",
    "OUTPUT_DIR = \"qat_tinyllama\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6484b07d",
   "metadata": {},
   "source": [
    "# Global Configuration Settings\n",
    "\n",
    "Defines essential parameters for the QAT experiment:\n",
    "\n",
    "1. Model Configuration:\n",
    "   - Uses TinyLlama-1.1B-Chat model\n",
    "   - Chosen for balance of size and capability\n",
    "\n",
    "2. Test Data:\n",
    "   - PROMPT: Complex text about sustainable energy\n",
    "     * Tests model's ability to handle technical content\n",
    "     * Evaluates coherence in generation\n",
    "   \n",
    "   - PERP_TEXT: Definition of AI\n",
    "     * Used for perplexity calculation\n",
    "     * Tests model's language understanding\n",
    "\n",
    "3. Generation Parameters:\n",
    "   - MAX_NEW_TOKENS: Controls generation length\n",
    "   - OUTPUT_DIR: Directory for saving quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c3d52ef-d54d-416f-9ac9-68088d41f182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_id: str):\n",
    "    # 1. Load the model\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "    # (Optional) save memory during training\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    # 2. Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "    # 3. Make sure we have a pad token\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf0be25",
   "metadata": {},
   "source": [
    "# Model and Tokenizer Initialization\n",
    "\n",
    "Defines a function that sets up the model and tokenizer with proper configurations:\n",
    "\n",
    "1. Model Loading:\n",
    "   - Initializes CausalLM architecture\n",
    "   - Enables gradient checkpointing for memory efficiency\n",
    "   - Optimizes for training large models\n",
    "\n",
    "2. Tokenizer Setup:\n",
    "   - Uses fast tokenizer implementation\n",
    "   - Configures padding strategy:\n",
    "     * Sets EOS token as padding token\n",
    "     * Updates model config to recognize padding\n",
    "   \n",
    "3. Memory Management:\n",
    "   - Implements memory-efficient loading\n",
    "   - Ensures proper token handling\n",
    "   - Maintains model-tokenizer alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9c7f232-bc3c-45a3-a97e-ebe0eaffa447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(tokenizer, block_size=8, train_size=1000, eval_size=500):\n",
    "    # 1) Load the raw Wikitext-2 dataset\n",
    "    raw = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "    # 2) Tokenization + label prep\n",
    "    def _tokenize(examples):\n",
    "        out = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=block_size,\n",
    "        )\n",
    "        out[\"labels\"] = out[\"input_ids\"].copy()\n",
    "        return out\n",
    "\n",
    "    tokenized = raw.map(\n",
    "        _tokenize,\n",
    "        batched=True,\n",
    "        remove_columns=[\"text\"],\n",
    "    )\n",
    "\n",
    "    # 3) Select the subsets\n",
    "    train_ds = tokenized[\"train\"].select(range(train_size))\n",
    "    eval_ds  = tokenized[\"validation\"].select(range(eval_size))\n",
    "\n",
    "    return train_ds, eval_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b93a4d",
   "metadata": {},
   "source": [
    "# Dataset Preparation for QAT\n",
    "\n",
    "Implements dataset processing pipeline for training and evaluation:\n",
    "\n",
    "1. Data Loading:\n",
    "   - Sources Wikitext-2 dataset\n",
    "   - Handles raw text format\n",
    "   - Manages download and caching\n",
    "\n",
    "2. Text Processing:\n",
    "   - Implements efficient tokenization:\n",
    "     * Applies truncation for consistent length\n",
    "     * Uses max-length padding\n",
    "     * Creates aligned input/label pairs\n",
    "   - Processes in batches for efficiency\n",
    "\n",
    "3. Dataset Management:\n",
    "   - Creates training subset (default: 1000 examples)\n",
    "   - Prepares evaluation set (default: 500 examples)\n",
    "   - Maintains data consistency\n",
    "\n",
    "4. Performance Considerations:\n",
    "   - Uses batched processing for speed\n",
    "   - Removes unnecessary columns\n",
    "   - Optimizes memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080245c8-386e-43c1-b949-fa2c6879d276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inc_trainer(model, tokenizer, train_ds, quant_config, output_dir):\n",
    "    # 1) Define the Hugging Face TrainingArguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        fp16=True,\n",
    "        optim=\"adamw_bnb_8bit\",\n",
    "        evaluation_strategy=\"no\",\n",
    "        save_strategy=\"no\", \n",
    "        logging_steps=50,\n",
    "        save_total_limit=1,\n",
    "    )\n",
    "\n",
    "    # 2) Instantiate the INCTrainer\n",
    "    trainer = INCTrainer(\n",
    "        model=model,\n",
    "        quantization_config=quant_config,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=default_data_collator,\n",
    "    )\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b4a409",
   "metadata": {},
   "source": [
    "# Intel Neural Compressor Trainer Setup\n",
    "\n",
    "Creates a specialized trainer for Quantization-Aware Training:\n",
    "\n",
    "1. Training Configuration:\n",
    "   - Sets up HuggingFace TrainingArguments:\n",
    "     * Single epoch training\n",
    "     * Small batch size (1) with gradient accumulation (4 steps)\n",
    "     * Mixed precision (FP16) for efficiency\n",
    "     * 8-bit Adam optimizer for memory savings\n",
    "     * Periodic logging (every 50 steps)\n",
    "     * Checkpoint management (keeps latest only)\n",
    "\n",
    "2. QAT Trainer Initialization:\n",
    "   - Integrates quantization configuration\n",
    "   - Sets up training dataset\n",
    "   - Configures data collation\n",
    "   - Maintains tokenizer alignment\n",
    "\n",
    "3. Optimization Features:\n",
    "   - Uses Intel's optimized training\n",
    "   - Implements efficient memory usage\n",
    "   - Enables proper quantization tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75af9f3b-c3e2-406c-856d-a743aa5e2eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_compute_ppl_fn(pad_token_id: int):\n",
    "    \"\"\"\n",
    "    Returns a compute_metrics function that knows the pad_token_id.\n",
    "    \"\"\"\n",
    "    def compute_ppl(pred: EvalPrediction):\n",
    "        # 1) Unpack\n",
    "        logits = pred.predictions         # np array (batch, seq_len, vocab_size)\n",
    "        labels = pred.label_ids           # np array (batch, seq_len)\n",
    "\n",
    "        # 2) Shift so each token predicts the next one\n",
    "        shift_logits = logits[..., :-1, :]\n",
    "        shift_labels = labels[..., 1:]\n",
    "\n",
    "        # 3) Flatten\n",
    "        flat_logits = shift_logits.reshape(-1, shift_logits.shape[-1])\n",
    "        flat_labels = shift_labels.reshape(-1)\n",
    "\n",
    "        # 4) To torch\n",
    "        logits_t = torch.from_numpy(flat_logits)\n",
    "        labels_t = torch.from_numpy(flat_labels)\n",
    "\n",
    "        # 5) CE loss ignoring pad_token_id\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "        loss = loss_fct(logits_t, labels_t)\n",
    "\n",
    "        # 6) Return perplexity\n",
    "        return {\"eval_perplexity\": torch.exp(loss).item()}\n",
    "    return compute_ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6498606",
   "metadata": {},
   "source": [
    "# Perplexity Calculation Function Generator\n",
    "\n",
    "Creates a closure-based metrics computation function:\n",
    "\n",
    "1. Prediction Processing:\n",
    "   - Handles complex tensor operations:\n",
    "     * Unpacks logits and labels\n",
    "     * Applies sequence shifting for next-token prediction\n",
    "     * Manages tensor reshaping and flattening\n",
    "   \n",
    "2. Perplexity Computation:\n",
    "   - Implements sophisticated calculation:\n",
    "     * Converts arrays to PyTorch tensors\n",
    "     * Uses CrossEntropyLoss with padding handling\n",
    "     * Computes exponential of loss\n",
    "   \n",
    "3. Technical Features:\n",
    "   - Handles padding tokens properly\n",
    "   - Maintains numerical stability\n",
    "   - Returns HuggingFace-compatible metrics\n",
    "\n",
    "4. Memory Efficiency:\n",
    "   - Uses efficient tensor operations\n",
    "   - Minimizes memory overhead\n",
    "   - Properly manages GPU/CPU transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1896d4b-50bd-4197-a100-2a424ab3a94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_qat_and_evaluate(trainer, eval_ds, tokenizer):\n",
    "    # 1) Train with QAT\n",
    "    trainer.train()\n",
    "\n",
    "    # 2) Small slice for eval\n",
    "    small_eval = eval_ds.select(range(min(len(eval_ds), 100)))\n",
    "\n",
    "    # 3) Build a ppl_fn that closes over pad_token_id\n",
    "    ppl_fn = make_compute_ppl_fn(tokenizer.pad_token_id)\n",
    "\n",
    "    # 4) Set up evaluation INCTrainer\n",
    "    eval_args = TrainingArguments(\n",
    "        output_dir=trainer.args.output_dir,\n",
    "        per_device_eval_batch_size=1,\n",
    "        fp16=True,\n",
    "        eval_strategy=\"no\",\n",
    "        save_strategy=\"no\",\n",
    "        logging_steps=50,\n",
    "    )\n",
    "    eval_trainer = INCTrainer(\n",
    "        model=trainer.model,\n",
    "        args=eval_args,\n",
    "        eval_dataset=small_eval,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=default_data_collator,\n",
    "        compute_metrics=ppl_fn,  # << use the closure here\n",
    "    )\n",
    "\n",
    "    # 5) Evaluate and return metrics\n",
    "    return eval_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65058de0",
   "metadata": {},
   "source": [
    "# QAT Training and Evaluation Pipeline\n",
    "\n",
    "Implements comprehensive training and evaluation workflow:\n",
    "\n",
    "1. Training Phase:\n",
    "   - Executes QAT training process\n",
    "   - Manages model updates and quantization\n",
    "\n",
    "2. Evaluation Preparation:\n",
    "   - Creates manageable evaluation subset\n",
    "   - Initializes perplexity calculator\n",
    "   - Sets up evaluation environment\n",
    "\n",
    "3. Evaluation Configuration:\n",
    "   - Configures evaluation parameters:\n",
    "     * Small batch size for accuracy\n",
    "     * FP16 precision for efficiency\n",
    "     * Proper logging intervals\n",
    "     * Metric computation setup\n",
    "\n",
    "4. Execution:\n",
    "   - Runs evaluation pipeline\n",
    "   - Collects performance metrics\n",
    "   - Returns comprehensive results\n",
    "\n",
    "5. Memory Management:\n",
    "   - Efficient dataset handling\n",
    "   - Proper resource cleanup\n",
    "   - Optimized evaluation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c43f6335-f9fd-48c8-870d-df9a6da99969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_load_qat_model(trainer, output_dir):\n",
    "    # 1) Save to the specified directory\n",
    "    trainer.save_model(output_dir)\n",
    "\n",
    "    # 2) Load it back as an INCModelForCausalLM\n",
    "    loaded_model = INCModelForCausalLM.from_pretrained(output_dir)\n",
    "\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f09855",
   "metadata": {},
   "source": [
    "# Model Persistence Functions\n",
    "\n",
    "Handles saving and loading of quantized models:\n",
    "\n",
    "1. Model Saving:\n",
    "   - Uses specialized trainer save method\n",
    "   - Preserves quantization parameters\n",
    "   - Maintains architecture integrity\n",
    "   - Ensures proper weight storage\n",
    "\n",
    "2. Model Loading:\n",
    "   - Loads as INCModelForCausalLM\n",
    "   - Restores quantization settings\n",
    "   - Maintains Intel optimizations\n",
    "   - Verifies model integrity\n",
    "\n",
    "3. Features:\n",
    "   - Proper format handling\n",
    "   - Quantization preservation\n",
    "   - Efficient storage/loading\n",
    "   - Version compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67f7f1f8-3f95-47c1-9299-826e281c6db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_latency_and_throughput(model, tokenizer, prompt: str, device, max_new_tokens=50):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_len = inputs[\"input_ids\"].size(1)\n",
    "\n",
    "    # warm-up\n",
    "    _ = model.generate(**inputs, max_new_tokens=5)\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    # timed generation\n",
    "    start = time.time()\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "\n",
    "    latency = end - start\n",
    "    gen_tokens = outputs.size(1) - input_len\n",
    "    return latency, gen_tokens / latency\n",
    "\n",
    "def measure_peak_mem_and_perplexity(model, tokenizer, text: str, device):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    torch.cuda.synchronize()\n",
    "    peak_mem_mib = torch.cuda.max_memory_allocated(device) / 1024**2\n",
    "    perplexity = math.exp(out.loss.item())\n",
    "    return peak_mem_mib, perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebd1b36",
   "metadata": {},
   "source": [
    "# Performance Measurement Functions\n",
    "\n",
    "Implements comprehensive performance metrics collection:\n",
    "\n",
    "1. Latency and Throughput Measurement:\n",
    "   - Handles input preparation\n",
    "   - Includes warm-up generation\n",
    "   - Measures generation time\n",
    "   - Calculates tokens per second\n",
    "   - Manages GPU synchronization\n",
    "   \n",
    "2. Memory and Perplexity Analysis:\n",
    "   - Tracks peak GPU memory usage\n",
    "   - Calculates model perplexity\n",
    "   - Handles device-specific operations\n",
    "   - Provides memory usage in MB\n",
    "   \n",
    "3. Features:\n",
    "   - Accurate timing mechanisms\n",
    "   - Proper GPU synchronization\n",
    "   - Memory usage tracking\n",
    "   - Comprehensive metrics collection\n",
    "   - Cross-device compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26545b47-4068-45c7-8b15-1c9b568d3725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start():\n",
    "    # 1) Load model & tokenizer\n",
    "    model, tokenizer = load_model_and_tokenizer(MODEL_NAME)\n",
    "\n",
    "    # 2) Prepare datasets\n",
    "    train_ds, eval_ds = prepare_datasets(tokenizer)\n",
    "\n",
    "    # 3) QAT configuration\n",
    "    quant_config = QuantizationAwareTrainingConfig()\n",
    "\n",
    "    # 4) Create and run QAT trainer\n",
    "    qat_trainer = create_inc_trainer(model, tokenizer, train_ds, quant_config, OUTPUT_DIR)\n",
    "    metrics = run_qat_and_evaluate(qat_trainer, eval_ds, tokenizer)\n",
    "    print(f\"Final perplexity: {metrics['eval_perplexity']:.2f}\")\n",
    "\n",
    "    # 5) Save & load quantized model\n",
    "    qat_model = save_and_load_qat_model(qat_trainer, OUTPUT_DIR)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    qat_model.to(device)\n",
    "\n",
    "    # 6) Benchmarks\n",
    "    latency, throughput = measure_latency_and_throughput(qat_model, tokenizer, PROMPT, device)\n",
    "    print(f\"Latency     : {latency:.3f} s\")\n",
    "    print(f\"Throughput  : {throughput:.1f} tokens/s\")\n",
    "\n",
    "    peak_mem, ppl = measure_peak_mem_and_perplexity(qat_model, tokenizer, PERP_TEXT, device)\n",
    "    print(f\"Peak GPU memory     : {peak_mem:.1f} MiB\")\n",
    "    print(f\"Next-token perplexity: {ppl:.3f}\")\n",
    "    size_mb = sum(os.path.getsize(os.path.join(OUTPUT_DIR, f)) for f in os.listdir(OUTPUT_DIR)) / 1024**2\n",
    "\n",
    "    print(f\"Quantized model size : {size_mb:.2f} MB\")\n",
    "    print(f\"Average latency      : {latency*1000:.1f} ms\")\n",
    "    print(f\"Throughput           : {throughput:.1f} tokens/sec\")\n",
    "    if peak_mem is not None:\n",
    "        print(f\"Peak GPU memory      : {peak_mem:.1f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f340caa",
   "metadata": {},
   "source": [
    "# Main Execution Function\n",
    "\n",
    "Orchestrates the complete QAT workflow:\n",
    "\n",
    "1. Initialization (Steps 1-2):\n",
    "   - Loads model and tokenizer\n",
    "   - Prepares training/evaluation datasets\n",
    "   \n",
    "2. QAT Setup and Training (Steps 3-4):\n",
    "   - Configures quantization parameters\n",
    "   - Creates and runs QAT trainer\n",
    "   - Evaluates model performance\n",
    "   - Reports perplexity metrics\n",
    "\n",
    "3. Model Management (Step 5):\n",
    "   - Saves quantized model\n",
    "   - Loads for inference\n",
    "   - Moves to appropriate device\n",
    "\n",
    "4. Performance Analysis (Step 6):\n",
    "   - Measures generation metrics:\n",
    "     * Latency\n",
    "     * Throughput\n",
    "     * Memory usage\n",
    "     * Model size\n",
    "   - Calculates perplexity\n",
    "   - Reports comprehensive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ca15c9c-e473-4d91-8f70-f12362ef6174",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 04:20, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.787100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.348500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.415500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.125900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.139800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final perplexity: 49.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency     : 1.084 s\n",
      "Throughput  : 46.1 tokens/s\n",
      "Peak GPU memory     : 10592.2 MiB\n",
      "Next-token perplexity: 10.600\n",
      "Quantized model size : 4200.37 MB\n",
      "Average latency      : 1084.0 ms\n",
      "Throughput           : 46.1 tokens/sec\n",
      "Peak GPU memory      : 10592.2 MB\n"
     ]
    }
   ],
   "source": [
    "start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158f1867",
   "metadata": {},
   "source": [
    "# Execute QAT Pipeline\n",
    "\n",
    "Initiates the complete Quantization-Aware Training process:\n",
    "- Runs all steps from model loading to evaluation\n",
    "- Executes training and quantization\n",
    "- Generates performance metrics\n",
    "- Displays comprehensive results\n",
    "\n",
    "This cell starts the actual execution of our QAT experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c575f545-7858-4131-b727-3e4c8550a751",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
