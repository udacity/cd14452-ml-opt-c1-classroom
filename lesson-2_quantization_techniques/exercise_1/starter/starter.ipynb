{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009e9ac1-df12-439f-a19c-108b6629eaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, time, math, gc, tempfile, json, contextlib\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Optimum (Quanto) for post-training quantization (static, weight-only int8)\n",
    "from optimum.quanto import quantize, freeze, qint8\n",
    "\n",
    "MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "DEVICE   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEED     = 42\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffbc05f-e232-48cc-888d-0aafdae30dac",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19312df6-4362-46f7-bd40-4f83f3d19cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dir_size_mb(path: str) -> float:\n",
    "    total = 0\n",
    "    for root, _, files in os.walk(path):\n",
    "        for f in files:\n",
    "            total += os.path.getsize(os.path.join(root, f))\n",
    "    return total / (1024**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc2ecd9-c2e2-4e7f-8b30-d2b98bef2b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextlib.contextmanager\n",
    "def torch_cuda_monitor():\n",
    "    \"\"\"Context manager to measure peak GPU memory in MB.\"\"\"\n",
    "    if DEVICE == \"cuda\":\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.synchronize()\n",
    "        start_alloc = torch.cuda.memory_allocated()\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            torch.cuda.synchronize()\n",
    "            peak = torch.cuda.max_memory_allocated()\n",
    "            torch.cuda.empty_cache()\n",
    "            # return values indirectly by storing on the function object\n",
    "            torch_cuda_monitor.peak_mb = peak / (1024**2)\n",
    "            torch_cuda_monitor.start_mb = start_alloc / (1024**2)\n",
    "    else:\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            torch_cuda_monitor.peak_mb = 0.0\n",
    "            torch_cuda_monitor.start_mb = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dde08e-9dd8-4d28-abed-3c79c7270735",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GenMetrics:\n",
    "    latency_s: float\n",
    "    tokens_per_sec: float\n",
    "    peak_gpu_mem_mb: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1c4a50-a21c-431e-8e82-41ce16df3aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_generate(model, tokenizer, prompt: str, max_new_tokens=64, runs=3) -> GenMetrics:\n",
    "    \"\"\"Measure latency, throughput, and peak GPU memory for text generation.\"\"\"\n",
    "    model.eval()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    # Warmup\n",
    "    with torch.inference_mode():\n",
    "        _ = model.generate(**inputs, max_new_tokens=8, do_sample=False, use_cache=True)\n",
    "\n",
    "    latencies, tps = [], []\n",
    "    with torch_cuda_monitor():\n",
    "        for _ in range(runs):\n",
    "            if DEVICE == \"cuda\":\n",
    "                torch.cuda.synchronize()\n",
    "            t0 = time.perf_counter()\n",
    "            with torch.inference_mode():\n",
    "                out = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False, use_cache=True)\n",
    "            if DEVICE == \"cuda\":\n",
    "                torch.cuda.synchronize()\n",
    "            t1 = time.perf_counter()\n",
    "\n",
    "            gen_len = out.shape[1] - input_len\n",
    "            lat = t1 - t0\n",
    "            latencies.append(lat)\n",
    "            tps.append(gen_len / lat if lat > 0 else float(\"nan\"))\n",
    "\n",
    "    return GenMetrics(\n",
    "        latency_s=sum(latencies)/len(latencies),\n",
    "        tokens_per_sec=sum(tps)/len(tps),\n",
    "        peak_gpu_mem_mb=getattr(torch_cuda_monitor, \"peak_mb\", 0.0),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5021512a-31a3-4838-a7bd-49b17bda4886",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_perplexity(model, tokenizer, seq_len=128) -> float:\n",
    "    \"\"\"\n",
    "    Self-contained perplexity estimate using a small built-in eval text.\n",
    "    Keeps evaluation light but still allows FP32 vs INT8 comparison.\n",
    "    \"\"\"\n",
    "    eval_text = (\n",
    "        \"Quantization reduces the precision of neural network weights and activations. \"\n",
    "        \"This process shrinks model size, lowers memory use, and can speed up inference. \"\n",
    "        \"The tradeoff is a small drop in accuracy. \"\n",
    "        \"Perplexity measures how well a language model predicts text: \"\n",
    "        \"a lower perplexity means the model is more confident in its predictions. \"\n",
    "        \"Large language models like LLaMA or TinyLlama are evaluated on benchmarks such as WikiText, \"\n",
    "        \"where perplexity is calculated over thousands of tokens. \"\n",
    "        \"In practice, we only need a small text sample to compare relative changes. \"\n",
    "        \"By quantizing a model to 8-bit, we can observe whether perplexity increases significantly. \"\n",
    "        \"If the rise is modest while speed and memory improve, quantization is usually a good trade-off. \"\n",
    "        \"This evaluation text is deliberately extended to ensure enough tokens for testing.\"\n",
    "    )\n",
    "\n",
    "    enc = tokenizer(eval_text, return_tensors=\"pt\")\n",
    "    input_ids = enc[\"input_ids\"][0]\n",
    "\n",
    "    usable = (len(input_ids) // seq_len) * seq_len\n",
    "    input_ids = input_ids[:usable + 1]\n",
    "    if len(input_ids) <= seq_len:\n",
    "        raise ValueError(\"Not enough tokens for perplexity calculation. Try reducing seq_len.\")\n",
    "\n",
    "    nll_sum, tok_count = 0.0, 0\n",
    "    model.eval()\n",
    "\n",
    "    for start in range(0, len(input_ids) - 1 - seq_len, seq_len):\n",
    "        chunk = input_ids[start:start+seq_len+1]\n",
    "        inp = chunk[:-1].unsqueeze(0).to(DEVICE)\n",
    "        labels = chunk[1:].unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        out = model(input_ids=inp, labels=labels)\n",
    "        nll_sum += float(out.loss) * labels.numel()\n",
    "        tok_count += labels.numel()\n",
    "\n",
    "    return math.exp(nll_sum / max(1, tok_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843a4a98-1347-4eee-8320-6565e3fbf56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_size(model, tokenizer, out_dir: str) -> float:\n",
    "    if os.path.exists(out_dir):\n",
    "        shutil.rmtree(out_dir)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    tokenizer.save_pretrained(out_dir)\n",
    "    model.save_pretrained(out_dir, safe_serialization=True)\n",
    "    return dir_size_mb(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1bbdbe-a2a6-4376-b275-1785547eab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_row(title, size_mb, lat_s, tps, gpu_mb, ppl):\n",
    "    print(\n",
    "        f\"{title:18s} | Size: {size_mb:8.1f} MB | Latency: {lat_s:7.3f} s | \"\n",
    "        f\"Throughput: {tps:7.2f} tok/s | Peak VRAM: {gpu_mb:7.1f} MB | PPL: {ppl:7.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7748cbea-a298-4251-90e0-2de38d68dde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "# Some chat models have no pad token; make generation/perplexity robust:\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadcb532-6185-4a5a-a52d-9cf0aaf8e73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n== Baseline FP32 ==\")\n",
    "baseline_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float32,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "baseline_model.to(DEVICE)\n",
    "\n",
    "baseline_size_mb = save_and_size(baseline_model, tokenizer, out_dir=\"tinyllama_fp32\")\n",
    "baseline_gen = measure_generate(\n",
    "    baseline_model,\n",
    "    tokenizer,\n",
    "    prompt=\"Explain quantization in one paragraph for ML engineers.\",\n",
    "    max_new_tokens=128,\n",
    "    runs=3,\n",
    ")\n",
    "baseline_ppl = compute_perplexity(baseline_model, tokenizer, seq_len=128)\n",
    "\n",
    "print_row(\"FP32 (baseline)\", baseline_size_mb, baseline_gen.latency_s, baseline_gen.tokens_per_sec,\n",
    "          baseline_gen.peak_gpu_mem_mb, baseline_ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cab1fb-62a2-41bd-a3f2-9dd89ee245fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n== PTQ INT8 (Optimum-Quanto, weight-only) ==\")\n",
    "q_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float32,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "# Static PTQ (weight-only): no calibration set required.\n",
    "# This converts Linear weights to int8-packed format and wires quant/dequant where needed.\n",
    "quantize(q_model, weights=qint8)\n",
    "freeze(q_model)               # finalize quantization graphs / params\n",
    "q_model.to(DEVICE)\n",
    "\n",
    "q_size_mb = save_and_size(q_model, tokenizer, out_dir=\"tinyllama_int8_quanto\")\n",
    "\n",
    "q_gen = measure_generate(\n",
    "    q_model,\n",
    "    tokenizer,\n",
    "    prompt=\"Explain quantization in one paragraph for ML engineers.\",\n",
    "    max_new_tokens=128,\n",
    "    runs=3,\n",
    ")\n",
    "q_ppl = compute_perplexity(q_model, tokenizer, seq_len=128)\n",
    "\n",
    "print_row(\"INT8 (Quanto)\", q_size_mb, q_gen.latency_s, q_gen.tokens_per_sec, q_gen.peak_gpu_mem_mb, q_ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcd0af01-42ac-42f6-a745-69f5669f918b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-26 22:42:09.317398: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-26 22:42:09.331712: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-08-26 22:42:09.349317: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-08-26 22:42:09.354787: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-08-26 22:42:09.367749: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Baseline FP32 ==\n",
      "FP32 (baseline)    | Size:   4200.3 MB | Latency:   2.654 s | Throughput:   48.24 tok/s | Peak VRAM:  4212.9 MB | PPL:    1.00\n",
      "\n",
      "== PTQ INT8 (Optimum-Quanto, weight-only) ==\n",
      "INT8 (Quanto)      | Size:   1242.5 MB | Latency:   6.688 s | Throughput:   19.14 tok/s | Peak VRAM:  5765.4 MB | PPL:    1.00\n",
      "\n",
      "Saved summary -> ptq_tinyllama_summary.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 5) Summary JSON (optional) ----------------------------------------------\n",
    "summary = {\n",
    "    \"device\": DEVICE,\n",
    "    \"model\": MODEL_ID,\n",
    "    \"seed\": SEED,\n",
    "    \"baseline_fp32\": {\n",
    "        \"size_mb\": baseline_size_mb,\n",
    "        \"latency_s\": baseline_gen.latency_s,\n",
    "        \"tokens_per_sec\": baseline_gen.tokens_per_sec,\n",
    "        \"peak_gpu_mem_mb\": baseline_gen.peak_gpu_mem_mb,\n",
    "        \"perplexity\": baseline_ppl,\n",
    "    },\n",
    "    \"int8_quanto\": {\n",
    "        \"size_mb\": q_size_mb,\n",
    "        \"latency_s\": q_gen.latency_s,\n",
    "        \"tokens_per_sec\": q_gen.tokens_per_sec,\n",
    "        \"peak_gpu_mem_mb\": q_gen.peak_gpu_mem_mb,\n",
    "        \"perplexity\": q_ppl,\n",
    "    },\n",
    "}\n",
    "with open(\"ptq_tinyllama_summary.json\", \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"\\nSaved summary -> ptq_tinyllama_summary.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4493789-c6de-4184-a1a0-d71950f6d97b",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c0ced22-a4a4-4678-a05a-9894682ea773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== CPU FP32 Baseline ==\n",
      "FP32 (CPU) | Latency 0.710s | Throughput 1.60 tok/s\n",
      "\n",
      "== CPU INT8 (dynamic) ==\n",
      "INT8 dyn (CPU) | Latency 0.220s | Throughput 4.57 tok/s\n",
      "\n",
      "== Summary ==\n",
      "CPU FP32 latency: 0.710s | CPU INT8 latency: 0.220s | Speedup: 3.22x\n",
      "CPU FP32 tput:   1.60 tok/s | CPU INT8 tput:   4.57 tok/s\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.ao.quantization import quantize_dynamic\n",
    "\n",
    "# -----------------------------\n",
    "# Config (CPU-only demo)\n",
    "# -----------------------------\n",
    "MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # swap to \"sshleifer/tiny-gpt2\" if CPU is tight\n",
    "DEVICE = \"cpu\"\n",
    "MAX_NEW_TOKENS = 128\n",
    "RUNS = 3\n",
    "PROMPT = \"Quantization test: explain why int8 dynamic quantization can be faster on CPU.\"\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "torch.set_num_threads(max(1, os.cpu_count() or 1))  # let PyTorch use available cores\n",
    "\n",
    "def measure_generate(model, tokenizer, prompt=PROMPT, max_new_tokens=MAX_NEW_TOKENS, runs=RUNS):\n",
    "    model.eval()\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    enc = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    # Warmup\n",
    "    with torch.inference_mode():\n",
    "        _ = model.generate(**enc, max_new_tokens=8, use_cache=True)\n",
    "\n",
    "    latencies, throughputs = [], []\n",
    "    for _ in range(runs):\n",
    "        t0 = time.perf_counter()\n",
    "        with torch.inference_mode():\n",
    "            out = model.generate(**enc, max_new_tokens=max_new_tokens, use_cache=True)\n",
    "        t1 = time.perf_counter()\n",
    "        gen_len = out.shape[1] - enc[\"input_ids\"].shape[1]\n",
    "        lat = t1 - t0\n",
    "        latencies.append(lat)\n",
    "        throughputs.append(gen_len / lat)\n",
    "\n",
    "    return sum(latencies)/len(latencies), sum(throughputs)/len(throughputs)\n",
    "\n",
    "# -----------------------------\n",
    "# Tokenizer (shared)\n",
    "# -----------------------------\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "if tok.pad_token_id is None:\n",
    "    tok.pad_token_id = tok.eos_token_id\n",
    "\n",
    "# -----------------------------\n",
    "# Baseline: FP32 on CPU\n",
    "# -----------------------------\n",
    "print(\"== CPU FP32 Baseline ==\")\n",
    "model_fp32 = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=torch.float32).to(DEVICE).eval()\n",
    "model_fp32.config.use_cache = True\n",
    "\n",
    "lat_fp32, tps_fp32 = measure_generate(model_fp32, tok)\n",
    "print(f\"FP32 (CPU) | Latency {lat_fp32:.3f}s | Throughput {tps_fp32:.2f} tok/s\")\n",
    "\n",
    "# -----------------------------\n",
    "# Quantized: INT8 dynamic (CPU)\n",
    "# -----------------------------\n",
    "print(\"\\n== CPU INT8 (dynamic) ==\")\n",
    "# Quantize only Linear layers to int8 (native PyTorch). This is weight-only int8 + dynamic activation quant.\n",
    "model_int8 = quantize_dynamic(\n",
    "    model_fp32.cpu(),\n",
    "    {torch.nn.Linear},\n",
    "    dtype=torch.qint8\n",
    ").eval()\n",
    "# (Optional) free original object to reduce memory\n",
    "del model_fp32\n",
    "\n",
    "lat_int8, tps_int8 = measure_generate(model_int8, tok)\n",
    "print(f\"INT8 dyn (CPU) | Latency {lat_int8:.3f}s | Throughput {tps_int8:.2f} tok/s\")\n",
    "\n",
    "# -----------------------------\n",
    "# Summary\n",
    "# -----------------------------\n",
    "speedup = lat_fp32 / lat_int8 if lat_int8 > 0 else float(\"inf\")\n",
    "print(\"\\n== Summary ==\")\n",
    "print(f\"CPU FP32 latency: {lat_fp32:.3f}s | CPU INT8 latency: {lat_int8:.3f}s | Speedup: {speedup:.2f}x\")\n",
    "print(f\"CPU FP32 tput:   {tps_fp32:.2f} tok/s | CPU INT8 tput:   {tps_int8:.2f} tok/s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366feef7-53fa-42c5-bf87-dbd46d4d0117",
   "metadata": {},
   "source": [
    "# [EX-1] Calibration set + Static PTQ (Optimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23b8f53c-c717-45fe-b8a7-01e52c871551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_calibration_texts():\n",
    "    \"\"\"\n",
    "    TODO: Return ~15–30 short, varied sentences for calibration.\n",
    "    Hints:\n",
    "      - Mix styles: facts, questions, lists, numbers.\n",
    "      - Keep each under ~200 tokens.\n",
    "    \"\"\"\n",
    "    CALIB_TEXTS = [\n",
    "        # \"Add sentences here...\",\n",
    "    ]\n",
    "    return CALIB_TEXTS\n",
    "\n",
    "\n",
    "def make_calib_dataloader(tokenizer, texts, batch_size=4, max_length=256):\n",
    "    \"\"\"\n",
    "    TODO: Build a torch DataLoader that yields dicts with 'input_ids' and 'attention_mask'.\n",
    "    Hints:\n",
    "      - Tokenize each text with truncation + max_length.\n",
    "      - Use a simple pad-sequence collator (pad_token_id=tokenizer.pad_token_id).\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "    class _CalibDS(Dataset):\n",
    "        def __init__(self, texts):\n",
    "            self.texts = texts\n",
    "        def __len__(self): return len(self.texts)\n",
    "        def __getitem__(self, idx):\n",
    "            enc = tokenizer(\n",
    "                texts[idx],\n",
    "                truncation=True, max_length=max_length, return_tensors=\"pt\"\n",
    "            )\n",
    "            return {k: v.squeeze(0) for k, v in enc.items()}\n",
    "\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "\n",
    "    def _collate(batch):\n",
    "        # TODO: pad input_ids and attention_mask to same length\n",
    "        # Hints:\n",
    "        #   input_ids = pad_sequence([...], batch_first=True, padding_value=pad_id)\n",
    "        #   attention = pad_sequence([...], batch_first=True, padding_value=0)\n",
    "        input_ids = ...\n",
    "        attention = ...\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention}\n",
    "\n",
    "    ds = _CalibDS(texts)\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=_collate)\n",
    "\n",
    "\n",
    "def run_static_ptq_optimum(model_fp32, calib_loader, out_dir=\"tinyllama-int8-static\"):\n",
    "    \"\"\"\n",
    "    TODO: Perform static PTQ with Hugging Face Optimum (INC).\n",
    "    Steps:\n",
    "      - from optimum.intel.neural_compressor import INCQuantizer, PostTrainingQuantConfig\n",
    "      - qconfig = PostTrainingQuantConfig(approach=\"static\", precision=\"int8\")\n",
    "      - quantizer = INCQuantizer.from_pretrained(model_fp32, task=\"text-generation\")\n",
    "      - quantizer.quantize(qconfig, calib_dataloader=calib_loader)\n",
    "      - quantizer.save_pretrained(out_dir)\n",
    "    Note:\n",
    "      - This requires `pip install optimum neural-compressor`.\n",
    "    \"\"\"\n",
    "    # from optimum.intel.neural_compressor import INCQuantizer, PostTrainingQuantConfig\n",
    "    # qconfig = ...\n",
    "    # quantizer = ...\n",
    "    # quantizer.quantize(...)\n",
    "    # quantizer.save_pretrained(out_dir)\n",
    "    raise NotImplementedError(\"Student TODO: implement static PTQ with Optimum.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da140ae8-b046-48cf-b3cc-6ebc0c421901",
   "metadata": {},
   "source": [
    "# [EX-2] Prompt-length sensitivity sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a879281-fdc6-4c88-a83d-acc04160d2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep_prompt_lengths(model_dict, tokenizer, prompt, lengths=(16, 64, 256), runs=3):\n",
    "    \"\"\"\n",
    "    For each model, measure latency & throughput across max_new_tokens.\n",
    "    Returns: {model_key: [{'L': L, 'lat': ..., 'tps': ...}, ...], ...}\n",
    "\n",
    "    TODOs:\n",
    "      1) Convert `lengths` to a sorted list of ints to ensure consistent plots.\n",
    "      2) Set each model to eval mode and (if available) enable use_cache=True.\n",
    "      3) Loop over lengths:\n",
    "         - Call `measure_generate(model, tokenizer, prompt=prompt, max_new_tokens=L, runs=runs)`.\n",
    "         - Accept either a (lat, tps) tuple OR a metrics object with .latency_s / .tokens_per_sec.\n",
    "      4) Append a dict per length: {\"L\": L, \"lat\": <float>, \"tps\": <float>}.\n",
    "      5) Return the results dict.\n",
    "      6) (Optional) If your `measure_generate` exposes std/variance, include \"lat_std\"/\"tps_std\".\n",
    "    \"\"\"\n",
    "    # 1) TODO: sanitize and sort lengths\n",
    "    # lengths = ...\n",
    "\n",
    "    # 2) Prepare results structure\n",
    "    results = {name: [] for name in model_dict.keys()}\n",
    "\n",
    "    for name, model in model_dict.items():\n",
    "        # 2) TODO: model.eval() and enable caching if present\n",
    "        # try:\n",
    "        #     ...\n",
    "        # except Exception:\n",
    "        #     pass\n",
    "\n",
    "        for L in lengths:\n",
    "            # 3) TODO: run benchmark at this L\n",
    "            # out = measure_generate(...)\n",
    "\n",
    "            # 3) TODO: support both return types\n",
    "            # if isinstance(out, tuple):\n",
    "            #     lat, tps = out\n",
    "            # else:\n",
    "            #     lat = getattr(out, \"latency_s\", getattr(out, \"lat\", None))\n",
    "            #     tps = getattr(out, \"tokens_per_sec\", getattr(out, \"tps\", None))\n",
    "            #     # (Optional std)\n",
    "            #     lat_std = getattr(out, \"latency_std\", None)\n",
    "            #     tps_std = getattr(out, \"tps_std\", None)\n",
    "\n",
    "            # 4) TODO: build row dict and append\n",
    "            # row = {\"L\": L, \"lat\": lat, \"tps\": tps}\n",
    "            # if lat_std is not None: row[\"lat_std\"] = lat_std\n",
    "            # if tps_std is not None: row[\"tps_std\"] = tps_std\n",
    "            # results[name].append(row)\n",
    "\n",
    "    # 5) TODO: return results\n",
    "    return results\n",
    "\n",
    "def plot_sweep(results, save_prefix=None):\n",
    "    \"\"\"\n",
    "    Plot (L vs latency) and (L vs throughput) for each model.\n",
    "    Uses matplotlib (no custom styles/colors).\n",
    "\n",
    "    TODOs:\n",
    "      1) Create a figure for latency.\n",
    "         - For each model, extract L (x) and lat (y).\n",
    "         - If \"lat_std\" present, use error bars; else use line plot.\n",
    "         - Label axes, set title, add legend; show the figure.\n",
    "         - If save_prefix is set, save as f\"{save_prefix}_latency.png\".\n",
    "      2) Create a separate figure for throughput.\n",
    "         - For each model, extract L (x) and tps (y).\n",
    "         - If \"tps_std\" present, use error bars; else use line plot.\n",
    "         - Label axes, set title, add legend; show the figure.\n",
    "         - If save_prefix is set, save as f\"{save_prefix}_throughput.png\".\n",
    "      3) Return the two figure objects for downstream use.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # 1) TODO: Latency figure\n",
    "    # fig1 = plt.figure()\n",
    "    # ax1 = plt.gca()\n",
    "    # for name, rows in results.items():\n",
    "    #     Ls = [r[\"L\"] for r in rows]\n",
    "    #     lats = [r[\"lat\"] for r in rows]\n",
    "    #     if \"lat_std\" in rows[0]:\n",
    "    #         errs = [r.get(\"lat_std\", 0.0) for r in rows]\n",
    "    #         ax1.errorbar(Ls, lats, yerr=errs, marker=\"o\", label=name)\n",
    "    #     else:\n",
    "    #         ax1.plot(Ls, lats, marker=\"o\", label=name)\n",
    "    # ax1.set_xlabel(\"max_new_tokens\")\n",
    "    # ax1.set_ylabel(\"Latency (s)\")\n",
    "    # ax1.set_title(\"Latency vs Generation Length\")\n",
    "    # ax1.legend()\n",
    "    # plt.show()\n",
    "    # if save_prefix:\n",
    "    #     fig1.savefig(f\"{save_prefix}_latency.png\", bbox_inches=\"tight\", dpi=150)\n",
    "\n",
    "    # 2) TODO: Throughput figure\n",
    "    # fig2 = plt.figure()\n",
    "    # ax2 = plt.gca()\n",
    "    # for name, rows in results.items():\n",
    "    #     Ls = [r[\"L\"] for r in rows]\n",
    "    #     tps = [r[\"tps\"] for r in rows]\n",
    "    #     if \"tps_std\" in rows[0]:\n",
    "    #         errs = [r.get(\"tps_std\", 0.0) for r in rows]\n",
    "    #         ax2.errorbar(Ls, tps, yerr=errs, marker=\"o\", label=name)\n",
    "    #     else:\n",
    "    #         ax2.plot(Ls, tps, marker=\"o\", label=name)\n",
    "    # ax2.set_xlabel(\"max_new_tokens\")\n",
    "    # ax2.set_ylabel(\"Throughput (tok/s)\")\n",
    "    # ax2.set_title(\"Throughput vs Generation Length\")\n",
    "    # ax2.legend()\n",
    "    # plt.show()\n",
    "    # if save_prefix:\n",
    "    #     fig2.savefig(f\"{save_prefix}_throughput.png\", bbox_inches=\"tight\", dpi=150)\n",
    "\n",
    "    # 3) TODO: return both figs\n",
    "    # return fig1, fig2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6624c6-9c4e-4b63-9fe4-5150ddce8e72",
   "metadata": {},
   "source": [
    "# [EX-3] KV-cache & padding-side ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3c538ce-b624-469d-bd98-d6e3c2da01c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ablate_cache_and_padding(model, tokenizer, prompt, max_new_tokens=128, runs=3):\n",
    "    \"\"\"\n",
    "    Measure under four scenarios:\n",
    "      A) cache on,  padding left\n",
    "      B) cache off, padding left\n",
    "      C) cache on,  padding right\n",
    "      D) cache off, padding right\n",
    "    Returns dict with lat/tps for each scenario.\n",
    "    \"\"\"\n",
    "    # TODO: save original cache setting and tokenizer padding_side\n",
    "\n",
    "    scenarios = {\n",
    "        \"A_cache_on_left\":  (True,  \"left\"),\n",
    "        \"B_cache_off_left\": (False, \"left\"),\n",
    "        \"C_cache_on_right\": (True,  \"right\"),\n",
    "        \"D_cache_off_right\":(False, \"right\"),\n",
    "    }\n",
    "    out = {}\n",
    "    for key, (use_cache, side) in scenarios.items():\n",
    "        # TODO: set model.config.use_cache and tokenizer.padding_side\n",
    "        # TODO: call measure_generate(...)\n",
    "        # TODO: store results in out[key] = {\"lat\": ..., \"tps\": ...}\n",
    "        pass\n",
    "\n",
    "    # TODO: restore original cache + padding settings\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8faab46-8ac7-43da-94c3-ff55d6183782",
   "metadata": {},
   "source": [
    "# [EX-4] Batch size sensitivity (micro-batching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30db549d-ab71-462e-845a-1f38524b9ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_batched(model, tokenizer, prompt, batch_sizes=(1, 2, 4), max_new_tokens=128, runs=3):\n",
    "    \"\"\"\n",
    "    For B in batch_sizes, build a batch by repeating prompt B times and timing a single generate().\n",
    "    Returns {B: {'lat': ..., 'tps_per_sample': ...}, ...}\n",
    "\n",
    "    TODOs:\n",
    "      1) Loop over each batch size B.\n",
    "      2) Tokenize the same prompt repeated B times (padding=True) and move tensors to DEVICE.\n",
    "      3) Run a short warmup generate() to stabilize performance.\n",
    "      4) For each run:\n",
    "         - Record start time.\n",
    "         - Run model.generate() with max_new_tokens and use_cache=True.\n",
    "         - Record end time.\n",
    "         - Compute latency and per-sample throughput (tokens generated ÷ latency).\n",
    "      5) Average latency and throughput across runs.\n",
    "      6) Store results in dict: results[B] = {\"lat\": avg_latency, \"tps_per_sample\": avg_throughput}.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for B in batch_sizes:\n",
    "        # TODO: tokenize [prompt] * B with padding=True, send to DEVICE\n",
    "        # enc = ...\n",
    "\n",
    "        # TODO: warmup with small generate\n",
    "        # with torch.inference_mode():\n",
    "        #     _ = model.generate(...)\n",
    "\n",
    "        lats = []\n",
    "        tputs = []\n",
    "        for _ in range(runs):\n",
    "            # TODO: record start time\n",
    "            # t0 = ...\n",
    "\n",
    "            # TODO: run generate inside torch.inference_mode()\n",
    "            # out = ...\n",
    "\n",
    "            # TODO: record end time\n",
    "            # t1 = ...\n",
    "\n",
    "            # TODO: compute generated length per sample\n",
    "            # gen_len = ...\n",
    "\n",
    "            # TODO: compute latency and per-sample throughput\n",
    "            # lat = ...\n",
    "            # tps = ...\n",
    "\n",
    "            # lats.append(lat)\n",
    "            # tputs.append(tps)\n",
    "\n",
    "        # TODO: average latency and throughput across runs\n",
    "        # results[B] = {\"lat\": ..., \"tps_per_sample\": ...}\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3601e2-f8e4-4860-9aea-d07d3be3458d",
   "metadata": {},
   "source": [
    "# Measuring Disk size + peak memory accounting (Nothing to Implement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b009d187-6dda-4431-8a15-89826da73dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_measure_size(model, tokenizer, out_dir):\n",
    "    \"\"\"\n",
    "    Save model + tokenizer to `out_dir` and return total size in bytes.\n",
    "    \"\"\"\n",
    "    p = Path(out_dir)\n",
    "    if p.exists():\n",
    "        shutil.rmtree(p)\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    model.save_pretrained(p)\n",
    "    tokenizer.save_pretrained(p)\n",
    "    total_bytes = sum(f.stat().st_size for f in p.rglob(\"*\") if f.is_file())\n",
    "    return total_bytes\n",
    "\n",
    "\n",
    "class PeakMemory:\n",
    "    \"\"\"\n",
    "    Context manager to record peak CPU (and GPU if available) memory during a block.\n",
    "    CPU via psutil RSS; GPU via torch.cuda.max_memory_allocated().\n",
    "    \"\"\"\n",
    "    def __enter__(self):\n",
    "        import psutil\n",
    "        self._psutil = psutil\n",
    "        self._proc = psutil.Process(os.getpid())\n",
    "        self.cpu_peak_bytes = self._proc.memory_info().rss\n",
    "        self.gpu_peak_bytes = 0\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc, tb):\n",
    "        # CPU: we sample once at the end (simple approach).\n",
    "        self.cpu_peak_bytes = max(self.cpu_peak_bytes, self._proc.memory_info().rss)\n",
    "        if torch.cuda.is_available():\n",
    "            self.gpu_peak_bytes = torch.cuda.max_memory_allocated()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afc2eb3-a382-4cd3-843b-852291abbe9f",
   "metadata": {},
   "source": [
    "# [EX-6] Quality proxy: pseudo-perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53645c63-3689-49a5-a5e2-97ffa7180ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def pseudo_perplexity(model, tokenizer, text, max_len=128):\n",
    "    \"\"\"\n",
    "    Leave-one-out style pseudo-perplexity.\n",
    "    For each token position i, predict token i given all tokens < i.\n",
    "    Return exp(average loss).\n",
    "    \n",
    "    TODOs:\n",
    "      1) Tokenize the text (truncate to max_len), move tensors to DEVICE.\n",
    "      2) Loop from token 1 to end:\n",
    "         - Feed prefix [:i] into the model.\n",
    "         - Get logits for the last position.\n",
    "         - Compute cross-entropy loss against the true token at position i.\n",
    "         - Accumulate loss.\n",
    "      3) Average the losses and return math.exp(avg_loss).\n",
    "    \"\"\"\n",
    "    # enc = tokenizer(...)\n",
    "    # input_ids, attn = ...\n",
    "    # if n < 2: return float(\"nan\")\n",
    "\n",
    "    loss_sum = 0.0\n",
    "    steps = 0\n",
    "    for i in range(1, n):\n",
    "        # TODO: forward pass on prefix\n",
    "        # logits = ...\n",
    "        # target = ...\n",
    "        # loss = torch.nn.functional.cross_entropy(...)\n",
    "        # loss_sum += ...\n",
    "        # steps += 1\n",
    "\n",
    "    # TODO: compute avg_loss and return exp(avg_loss)\n",
    "    return ...\n",
    "\n",
    "\n",
    "def compare_ppplx(models, tokenizer, texts):\n",
    "    \"\"\"\n",
    "    Compute pseudo-perplexity for each model over a list of texts.\n",
    "    Return: {model_key: [scores...], ...}\n",
    "\n",
    "    TODOs:\n",
    "      1) Loop over models in dict.\n",
    "      2) For each model, compute pseudo_perplexity on all texts.\n",
    "      3) Collect results in a dict mapping model_name -> list of scores.\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "    for name, model in models.items():\n",
    "        # TODO: run pseudo_perplexity on each text\n",
    "        # scores[name] = [...]\n",
    "        pass\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4d4e09-d4b1-4e5e-93a4-dee1b2ce7227",
   "metadata": {},
   "source": [
    "# [EX-7] Results table + short reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63a1a5d1-0212-43b7-af46-33f83d396b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_results_table(metrics_rows):\n",
    "    \"\"\"\n",
    "    Build a pandas DataFrame from a list of dicts.\n",
    "    Expected keys per row:\n",
    "      precision, size_bytes, lat_64, tps_64, lat_256, tps_256, ppplx_avg\n",
    "\n",
    "    TODOs:\n",
    "      1) Convert metrics_rows into a pandas DataFrame.\n",
    "      2) If \"size_bytes\" is present, add a new column \"size_MB\" by dividing by 1024**2\n",
    "         and rounding to 1 decimal place.\n",
    "      3) Return the DataFrame with the following column order (only if present):\n",
    "         [\"precision\",\"size_bytes\",\"size_MB\",\"lat_64\",\"tps_64\",\"lat_256\",\"tps_256\",\"ppplx_avg\"]\n",
    "    \"\"\"\n",
    "    # TODO: create DataFrame\n",
    "    # df = pd.DataFrame(metrics_rows)\n",
    "\n",
    "    # TODO: add size_MB if \"size_bytes\" exists\n",
    "    # if \"size_bytes\" in df.columns:\n",
    "    #     df[\"size_MB\"] = ...\n",
    "\n",
    "    # TODO: filter/reorder columns\n",
    "    # cols = [...]\n",
    "    # return df[[c for c in cols if c in df.columns]]\n",
    "    return ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf3cd0d-83d9-4537-9abd-aa103436beff",
   "metadata": {},
   "source": [
    "# [EX-8 - Stretch] Edge-case prompts check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e282aaa1-fdcc-43cb-82e2-f5f2f526c7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probe_edge_prompts(models, tokenizer, prompts, max_new_tokens=64):\n",
    "    \"\"\"\n",
    "    Generate for a small set of edge prompts.\n",
    "    Returns: {model_key: [decoded_outputs...], ...}\n",
    "\n",
    "    TODOs:\n",
    "      1) Create an outputs dict with model names as keys and empty lists as values.\n",
    "      2) For each model and each prompt:\n",
    "         - Tokenize the prompt and move tensors to DEVICE.\n",
    "         - Run model.generate() with max_new_tokens and use_cache=True inside torch.inference_mode().\n",
    "         - Decode the generated ids into text (skip special tokens).\n",
    "         - Append decoded text to the model’s outputs list.\n",
    "      3) Return the outputs dict.\n",
    "    \"\"\"\n",
    "    outputs = {k: [] for k in models.keys()}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        for p in prompts:\n",
    "            # TODO: tokenize\n",
    "            # enc = ...\n",
    "\n",
    "            # TODO: run generate under inference_mode\n",
    "            # out_ids = ...\n",
    "\n",
    "            # TODO: decode output ids\n",
    "            # decoded = ...\n",
    "\n",
    "            # TODO: append to outputs[name]\n",
    "            # outputs[name].append(decoded)\n",
    "            pass\n",
    "\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1b4f84-09b6-4e5c-bfd2-0405e6146213",
   "metadata": {},
   "source": [
    "# Master controller for the quantization exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc24a702-73bc-4e0c-96fc-8b8f2d655ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_quantization_exercise():\n",
    "    \"\"\"\n",
    "    Controller to run all parts of the exercise.\n",
    "    Students are expected to fill in the TODOs inside each helper function.\n",
    "    This function orchestrates the flow and prints / returns results.\n",
    "    \"\"\"\n",
    "    # -------------------------------\n",
    "    # 1. Build calibration set + run static PTQ\n",
    "    # -------------------------------\n",
    "    print(\"\\n[1] Calibration + Static PTQ\")\n",
    "    calib_texts = build_calibration_texts()\n",
    "    calib_loader = make_calib_dataloader(tok, calib_texts)\n",
    "    # Run static PTQ and save quantized model\n",
    "    run_static_ptq_optimum(model_fp32, calib_loader, out_dir=\"tinyllama-int8-static\")\n",
    "    \n",
    "    # Reload quantized model (student will implement saving inside run_static_ptq_optimum)\n",
    "    model_static = AutoModelForCausalLM.from_pretrained(\"tinyllama-int8-static\").to(DEVICE).eval()\n",
    "    model_static.config.use_cache = True\n",
    "\n",
    "    models = {\n",
    "        \"fp32\": model_fp32,\n",
    "        \"int8-dyn\": model_int8,\n",
    "        \"int8-static\": model_static\n",
    "    }\n",
    "\n",
    "    # -------------------------------\n",
    "    # 2. Prompt-length sensitivity sweep\n",
    "    # -------------------------------\n",
    "    print(\"\\n[2] Prompt-length sweep\")\n",
    "    sweep_results = sweep_prompt_lengths(models, tok, PROMPT, lengths=(16, 64, 256))\n",
    "    plot_sweep(sweep_results)\n",
    "\n",
    "    # -------------------------------\n",
    "    # 3. KV-cache & padding ablation\n",
    "    # -------------------------------\n",
    "    print(\"\\n[3] Cache & padding ablation\")\n",
    "    ablation = ablate_cache_and_padding(model_static, tok, PROMPT)\n",
    "    print(\"Ablation results:\", ablation)\n",
    "\n",
    "    # -------------------------------\n",
    "    # 4. Batch-size sensitivity\n",
    "    # -------------------------------\n",
    "    print(\"\\n[4] Batch-size sensitivity\")\n",
    "    batch_res = measure_batched(model_static, tok, PROMPT)\n",
    "    print(\"Batch results:\", batch_res)\n",
    "\n",
    "    # -------------------------------\n",
    "    # 5. Model size + memory tracking\n",
    "    # -------------------------------\n",
    "    print(\"\\n[5] Model size + memory\")\n",
    "    size_fp32 = save_and_measure_size(model_fp32, tok, \"fp32-save\")\n",
    "    size_int8_dyn = save_and_measure_size(model_int8, tok, \"int8dyn-save\")\n",
    "    size_int8_static = save_and_measure_size(model_static, tok, \"int8static-save\")\n",
    "\n",
    "    with PeakMemory() as pm:\n",
    "        measure_generate(model_static, tok, PROMPT, max_new_tokens=64)\n",
    "    print(\"Peak memory (CPU/GPU):\", pm.cpu_peak_bytes, pm.gpu_peak_bytes)\n",
    "\n",
    "    # -------------------------------\n",
    "    # 6. Quality proxy: pseudo-perplexity\n",
    "    # -------------------------------\n",
    "    print(\"\\n[6] Quality proxy (pseudo-perplexity)\")\n",
    "    texts = [\"Quantization reduces model size.\", \"Speed matters for deployment.\"]\n",
    "    ppplx_scores = compare_ppplx(models, tok, texts)\n",
    "    print(\"PPPLX:\", ppplx_scores)\n",
    "\n",
    "    # -------------------------------\n",
    "    # 7. Build results table\n",
    "    # -------------------------------\n",
    "    print(\"\\n[7] Summary table\")\n",
    "    metrics = [\n",
    "        {\n",
    "            \"precision\": \"FP32\",\n",
    "            \"size_bytes\": size_fp32,\n",
    "            \"lat_64\": None,  # TODO: fill from sweep_results\n",
    "            \"tps_64\": None,\n",
    "            \"lat_256\": None,\n",
    "            \"tps_256\": None,\n",
    "            \"ppplx_avg\": None,\n",
    "        },\n",
    "        # TODO: add rows for int8-dyn and int8-static\n",
    "    ]\n",
    "    df = build_results_table(metrics)\n",
    "    print(df)\n",
    "\n",
    "    # -------------------------------\n",
    "    # 8. (Stretch) Edge prompts\n",
    "    # -------------------------------\n",
    "    print(\"\\n[8] Edge-case prompts\")\n",
    "    edge_prompts = [\n",
    "        \"Supercalifragilisticexpialidocious\",\n",
    "        \"12345678901234567890\",\n",
    "        \"A very very very very very long name...\",\n",
    "    ]\n",
    "    outputs = probe_edge_prompts(models, tok, edge_prompts)\n",
    "    for m, outs in outputs.items():\n",
    "        print(f\"--- {m} ---\")\n",
    "        for o in outs:\n",
    "            print(o)\n",
    "            print(\"=\"*40)\n",
    "\n",
    "    return {\n",
    "        \"sweep\": sweep_results,\n",
    "        \"ablation\": ablation,\n",
    "        \"batch\": batch_res,\n",
    "        \"sizes\": (size_fp32, size_int8_dyn, size_int8_static),\n",
    "        \"ppplx\": ppplx_scores,\n",
    "        \"table\": df,\n",
    "        \"edge_outputs\": outputs\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5a70128-3f1e-41d2-8c40-d10aaf7bf47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] Calibration + Static PTQ\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_fp32' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_quantization_exercise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m, in \u001b[0;36mrun_quantization_exercise\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m calib_loader \u001b[38;5;241m=\u001b[39m make_calib_dataloader(tok, calib_texts)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Run static PTQ and save quantized model\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m run_static_ptq_optimum(\u001b[43mmodel_fp32\u001b[49m, calib_loader, out_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtinyllama-int8-static\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Reload quantized model (student will implement saving inside run_static_ptq_optimum)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m model_static \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtinyllama-int8-static\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(DEVICE)\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_fp32' is not defined"
     ]
    }
   ],
   "source": [
    "run_quantization_exercise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c4fc3f-9820-46d7-9102-a50fd9d53773",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
