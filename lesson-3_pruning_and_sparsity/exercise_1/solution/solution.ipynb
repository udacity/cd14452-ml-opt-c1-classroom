{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "553eb726",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "This cell:\n",
    "- Imports standard libraries for file handling, timing, and mathematical operations.\n",
    "- Imports PyTorch for deep learning operations and pruning utilities.\n",
    "- Imports Hugging Face Transformers for model and tokenizer handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "851cb58b-1175-45d8-9113-9869ca6cfed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af7b2d8",
   "metadata": {},
   "source": [
    "# Define Model and Evaluation Settings\n",
    "This cell:\n",
    "- Specifies the model name to be used for pruning and evaluation.\n",
    "- Defines the fraction of neurons to prune in the MLP layers (`MLP_PRUNE_FRAC`).\n",
    "- Sets the maximum number of tokens to generate during inference.\n",
    "- Provides sample texts for benchmarking latency, throughput, and perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d3b226d-9e0d-4ae9-984d-4e00e8006d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME     = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "MLP_PRUNE_FRAC = 0.5       # fraction of inner neurons to prune\n",
    "MAX_NEW_TOKENS = 50\n",
    "PROMPT = (\n",
    "    \"Over the next decade, sustainable energy solutions will revolutionize \"\n",
    "    \"global power grids, reducing carbon footprints and fostering resilient \"\n",
    "    \"communities through innovative storage and distribution technologies.\"\n",
    ")\n",
    "PERP_TEXT = (\n",
    "    \"Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast \"\n",
    "    \"to the natural intelligence displayed by humans and animals. Leading AI textbooks \"\n",
    "    \"define the field as the study of intelligent agents: any system that perceives \"\n",
    "    \"its environment and takes actions that maximize its chance of achieving its goals.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb75c67",
   "metadata": {},
   "source": [
    "# Load Model and Tokenizer\n",
    "This function:\n",
    "- Loads the tokenizer and model using Hugging Face Transformers.\n",
    "- Configures the model to use FP16 precision for faster inference.\n",
    "- Moves the model to the specified device (CPU or GPU).\n",
    "- Sets the model to evaluation mode to disable gradient computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d36546f-6bbe-4d65-b952-2520b94ab986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name: str, device: torch.device):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Load AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "      - Load AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "      - Move model to `device` and set to .eval()\n",
    "      - Return tokenizer, model\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    # load model in FP16 for faster inference\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    # move to device and set to eval\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e3bb63",
   "metadata": {},
   "source": [
    "# Measure Baseline Performance\n",
    "This function:\n",
    "- Measures the baseline performance of the model before pruning.\n",
    "- Evaluates:\n",
    "  - **Latency**: Time taken to generate tokens for a given prompt.\n",
    "  - **Throughput**: Tokens generated per second.\n",
    "  - **Peak GPU Memory Usage**: Maximum memory used during inference.\n",
    "  - **Perplexity**: A measure of how well the model predicts the given text.\n",
    "- Prints the baseline metrics for comparison with the pruned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7273e561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_latency_and_throughput(model, tokenizer, prompt: str, device: torch.device, max_new_tokens=50, runs=3):\n",
    "    \"\"\"\n",
    "    Measure latency and throughput for text generation.\n",
    "\n",
    "    Args:\n",
    "        model: The language model to evaluate.\n",
    "        tokenizer: The tokenizer associated with the model.\n",
    "        prompt (str): Input prompt for text generation.\n",
    "        device (torch.device): Device to run the model on.\n",
    "        max_new_tokens (int): Maximum number of new tokens to generate.\n",
    "        runs (int): Number of runs for averaging metrics.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Average latency (seconds) and throughput (tokens per second).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "    # Warmup\n",
    "    with torch.inference_mode():\n",
    "        _ = model.generate(**inputs, max_new_tokens=8)\n",
    "\n",
    "    latencies = []\n",
    "    throughputs = []\n",
    "    for _ in range(runs):\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        t0 = time.perf_counter()\n",
    "        with torch.inference_mode():\n",
    "            outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        t1 = time.perf_counter()\n",
    "\n",
    "        gen_len = outputs.shape[1] - inputs[\"input_ids\"].shape[1]\n",
    "        latency = t1 - t0\n",
    "        throughput = gen_len / latency if latency > 0 else float(\"nan\")\n",
    "\n",
    "        latencies.append(latency)\n",
    "        throughputs.append(throughput)\n",
    "\n",
    "    avg_latency = sum(latencies) / len(latencies)\n",
    "    avg_throughput = sum(throughputs) / len(throughputs)\n",
    "    return avg_latency, avg_throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39e97ed4-3031-452c-b3d9-7d79afa84e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_baseline(model: nn.Module, tokenizer, prompt: str, perp_text: str, device: torch.device):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Warm up & measure generation latency & throughput on `prompt`\n",
    "      - Measure peak GPU memory & perplexity on `perp_text`\n",
    "      - Print or return these baseline metrics\n",
    "    \"\"\"\n",
    "    # 1) Measure latency & throughput\n",
    "    latency, throughput = measure_latency_and_throughput(model, tokenizer, prompt, device)\n",
    "    # 2) Measure peak GPU memory & perplexity\n",
    "    peak_mem, perplexity = measure_peak_mem_and_perplexity(model, tokenizer, perp_text, device)\n",
    "\n",
    "    # 3) Print baseline metrics\n",
    "    print(f\"[Baseline] latency   = {latency:.3f}s\")\n",
    "    print(f\"[Baseline] throughput= {throughput:.1f} tok/s\")\n",
    "    print(f\"[Baseline] peak GPU  = {peak_mem:.1f} MiB\")\n",
    "    print(f\"[Baseline] perplexity= {perplexity:.3f}\")\n",
    "\n",
    "    # Return in case caller wants to use them programmatically\n",
    "    return {\n",
    "        \"latency\": latency,\n",
    "        \"throughput\": throughput,\n",
    "        \"peak_gpu_mem\": peak_mem,\n",
    "        \"perplexity\": perplexity\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330f5a15",
   "metadata": {},
   "source": [
    "# Prune MLP Rows and Columns\n",
    "This function:\n",
    "- Prunes the MLP layers in the model by:\n",
    "  - Zeroing out a fraction of rows in the `gate_proj` and `up_proj` layers.\n",
    "  - Zeroing out the corresponding columns in the `down_proj` layer.\n",
    "- Uses structured pruning to remove entire rows or columns.\n",
    "- Ensures pruning is performed on the CPU to avoid GPU memory issues.\n",
    "- Removes the pruning reparameterizations after applying the masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d94de27-8bb4-4110-84f5-b9cac02dc045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_mlp_rows_and_cols(model: nn.Module, prune_frac: float):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Move model to CPU\n",
    "      - For each layer in model.model.layers:\n",
    "          • Zero out `prune_frac` of rows in gate_proj and up_proj\n",
    "          • Zero out corresponding `prune_frac` of columns in down_proj\n",
    "      - Remove pruning reparameterizations\n",
    "    \"\"\"\n",
    "    # 1) Ensure we prune on CPU to avoid GPU OOM\n",
    "    model.cpu()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # 2) Iterate through each decoder layer’s MLP\n",
    "    for layer in model.model.layers:\n",
    "        gate = layer.mlp.gate_proj   # [inner, hidden]\n",
    "        up   = layer.mlp.up_proj     # [inner, hidden]\n",
    "        down = layer.mlp.down_proj   # [hidden, inner]\n",
    "\n",
    "        # 2a) Zero out rows in gate_proj and up_proj\n",
    "        for proj in (gate, up):\n",
    "            prune.ln_structured(\n",
    "                proj,\n",
    "                name=\"weight\",\n",
    "                amount=prune_frac,\n",
    "                n=1,\n",
    "                dim=0,           # prune entire rows\n",
    "            )\n",
    "            prune.remove(proj, \"weight\")\n",
    "\n",
    "        # 2b) Zero out corresponding columns in down_proj\n",
    "        prune.ln_structured(\n",
    "            down,\n",
    "            name=\"weight\",\n",
    "            amount=prune_frac,\n",
    "            n=1,\n",
    "            dim=1,               # prune entire columns\n",
    "        )\n",
    "        prune.remove(down, \"weight\")\n",
    "\n",
    "    # 3) Return the model (now with zeros in place)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1ba705",
   "metadata": {},
   "source": [
    "# Rebuild MLP Blocks\n",
    "This function:\n",
    "- Reconstructs the pruned MLP layers with reduced dimensions.\n",
    "- Identifies the neurons that were not pruned in the `gate_proj` layer.\n",
    "- Creates new `nn.Linear` modules for `gate_proj`, `up_proj`, and `down_proj` with updated dimensions.\n",
    "- Copies the weights and biases from the original layers to the new layers.\n",
    "- Replaces the old modules with the new ones in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d85619ae-1a0f-44f2-99f1-cb8cb3d7bf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_mlp_blocks(model: nn.Module):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - For each layer in model.model.layers:\n",
    "          1) Identify kept neuron indices in gate_proj\n",
    "          2) Construct new nn.Linear modules for gate_proj, up_proj, down_proj\n",
    "             with reduced dimensions\n",
    "          3) Copy over weights and biases\n",
    "          4) Replace the old modules on the model\n",
    "    \"\"\"\n",
    "    for layer in model.model.layers:\n",
    "        # original modules (still on CPU, dtype=original)\n",
    "        old_gate = layer.mlp.gate_proj\n",
    "        old_up   = layer.mlp.up_proj\n",
    "        old_down = layer.mlp.down_proj\n",
    "\n",
    "        # discover surviving rows in gate_proj\n",
    "        Wg = old_gate.weight.data     # [inner_orig, hidden], dtype say torch.half\n",
    "        keep_idx = (Wg.abs().sum(dim=1) != 0).nonzero(as_tuple=False).view(-1)\n",
    "        inner_new = keep_idx.numel()\n",
    "        hidden    = Wg.size(1)\n",
    "        dtype     = Wg.dtype\n",
    "        device    = Wg.device\n",
    "\n",
    "        # helper to build a new Linear with the same dtype/device\n",
    "        def make_linear(in_f, out_f, bias, old_weight, old_bias=None):\n",
    "            nl = nn.Linear(in_f, out_f, bias=bias)\n",
    "            # init in correct dtype & device\n",
    "            nl.weight.data = old_weight.clone().to(device=device, dtype=dtype)\n",
    "            if bias and old_bias is not None:\n",
    "                nl.bias.data = old_bias.clone().to(device=device, dtype=dtype)\n",
    "            return nl\n",
    "\n",
    "        # rebuild gate_proj: hidden -> inner_new\n",
    "        new_gate = make_linear(\n",
    "            hidden, inner_new, \n",
    "            bias=(old_gate.bias is not None),\n",
    "            old_weight=old_gate.weight.data[keep_idx],\n",
    "            old_bias=old_gate.bias.data[keep_idx] if old_gate.bias is not None else None\n",
    "        )\n",
    "\n",
    "        # rebuild up_proj: hidden -> inner_new\n",
    "        new_up = make_linear(\n",
    "            hidden, inner_new,\n",
    "            bias=(old_up.bias is not None),\n",
    "            old_weight=old_up.weight.data[keep_idx],\n",
    "            old_bias=old_up.bias.data[keep_idx] if old_up.bias is not None else None\n",
    "        )\n",
    "\n",
    "        # rebuild down_proj: inner_new -> hidden\n",
    "        new_down = make_linear(\n",
    "            inner_new, hidden,\n",
    "            bias=(old_down.bias is not None),\n",
    "            old_weight=old_down.weight.data[:, keep_idx],\n",
    "            old_bias=old_down.bias.data if old_down.bias is not None else None\n",
    "        )\n",
    "\n",
    "        # swap in-place\n",
    "        layer.mlp.gate_proj = new_gate\n",
    "        layer.mlp.up_proj   = new_up\n",
    "        layer.mlp.down_proj = new_down\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb3b715",
   "metadata": {},
   "source": [
    "# Measure Performance After Rebuilding\n",
    "This function:\n",
    "- Evaluates the performance of the rebuilt model after pruning and reconstruction.\n",
    "- Measures:\n",
    "  - **Latency**: Time taken to generate tokens for a given prompt.\n",
    "  - **Throughput**: Tokens generated per second.\n",
    "  - **Peak GPU Memory Usage**: Maximum memory used during inference.\n",
    "  - **Perplexity**: A measure of how well the model predicts the given text.\n",
    "- Prints the metrics for comparison with the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79f5c2f5-9541-42e1-8389-dcd1b216add7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_rebuilt(model: nn.Module, tokenizer, prompt: str, perp_text: str, device: torch.device):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Move rebuilt model to `device` & .eval()\n",
    "      - Re-measure latency, throughput, peak memory, perplexity\n",
    "      - Print or return these metrics\n",
    "    \"\"\"\n",
    "    # 1) Move to device and set to eval\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # 2) Measure latency & throughput\n",
    "    latency, throughput = measure_latency_and_throughput(\n",
    "        model, tokenizer, prompt, device\n",
    "    )\n",
    "\n",
    "    # 3) Measure peak GPU memory & perplexity\n",
    "    peak_mem, perplexity = measure_peak_mem_and_perplexity(\n",
    "        model, tokenizer, perp_text, device\n",
    "    )\n",
    "\n",
    "    # 4) Print results\n",
    "    print(f\"[Rebuilt] latency   = {latency:.3f}s\")\n",
    "    print(f\"[Rebuilt] throughput= {throughput:.1f} tok/s\")\n",
    "    print(f\"[Rebuilt] peak GPU  = {peak_mem:.1f} MiB\")\n",
    "    print(f\"[Rebuilt] perplexity= {perplexity:.3f}\")\n",
    "\n",
    "    # 5) Return for further use if needed\n",
    "    return {\n",
    "        \"latency\": latency,\n",
    "        \"throughput\": throughput,\n",
    "        \"peak_gpu_mem\": peak_mem,\n",
    "        \"perplexity\": perplexity\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d350c8c",
   "metadata": {},
   "source": [
    "# Save Model and Report Size\n",
    "This function:\n",
    "- Saves the pruned and rebuilt model to the specified output directory.\n",
    "- Calculates the total size of the saved model files on disk.\n",
    "- Prints the on-disk size of the model for comparison with the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b5cde41-77ee-4339-a06c-6fe825e6ad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_report_size(model: nn.Module, output_dir: str):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - model.save_pretrained(output_dir)\n",
    "      - Walk `output_dir` to sum file sizes (in MiB)\n",
    "      - Print the on-disk size\n",
    "    \"\"\"\n",
    "    # 1) Save\n",
    "    model.save_pretrained(output_dir)\n",
    "\n",
    "    # 2) Sum file sizes\n",
    "    total_bytes = 0\n",
    "    for root, _, files in os.walk(output_dir):\n",
    "        for fname in files:\n",
    "            total_bytes += os.path.getsize(os.path.join(root, fname))\n",
    "\n",
    "    # 3) Convert to MiB and print\n",
    "    size_mb = total_bytes / 1024**2\n",
    "    print(f\"[Rebuilt] on-disk size = {size_mb:.1f} MiB\")\n",
    "\n",
    "    return size_mb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab775d2",
   "metadata": {},
   "source": [
    "# Main Execution Flow\n",
    "This function:\n",
    "- Loads the model and tokenizer.\n",
    "- Measures the baseline performance of the model.\n",
    "- Applies structured pruning to the MLP layers.\n",
    "- Rebuilds the pruned MLP layers with reduced dimensions.\n",
    "- Measures the performance of the rebuilt model.\n",
    "- Saves the pruned and rebuilt model to disk and reports its size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77c3803f-9b33-42b3-a1cf-0891f0ea130b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start():\n",
    "    device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    tokenizer, model = load_model_and_tokenizer(MODEL_NAME, device)\n",
    "\n",
    "    # Baseline\n",
    "    measure_baseline(model, tokenizer, PROMPT, PERP_TEXT, device)\n",
    "\n",
    "    # Prune on CPU\n",
    "    prune_mlp_rows_and_cols(model, MLP_PRUNE_FRAC)\n",
    "\n",
    "    # Rebuild smaller MLPs\n",
    "    rebuild_mlp_blocks(model)\n",
    "\n",
    "    # Re-benchmark rebuilt model\n",
    "    measure_rebuilt(model, tokenizer, PROMPT, PERP_TEXT, device)\n",
    "\n",
    "    # Save & report on-disk size\n",
    "    save_and_report_size(model, \"llama_pruned_rebuilt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0be5e3",
   "metadata": {},
   "source": [
    "# Start the Pruning and Evaluation Process\n",
    "This cell:\n",
    "- Calls the `start` function to execute the entire pruning and evaluation pipeline.\n",
    "- Outputs the baseline and post-pruning metrics, as well as the on-disk size of the pruned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2824316e-ff26-4dfa-b1f4-f7d3e4c3e2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 00:54:22.776464: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-31 00:54:22.791036: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-08-31 00:54:22.809382: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-08-31 00:54:22.815104: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-08-31 00:54:22.828245: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Baseline] latency   = 1.164s\n",
      "[Baseline] throughput= 43.0 tok/s\n",
      "[Baseline] peak GPU  = 2129.6 MiB\n",
      "[Baseline] perplexity= 4.557\n",
      "[Rebuilt] latency   = 1.141s\n",
      "[Rebuilt] throughput= 43.8 tok/s\n",
      "[Rebuilt] peak GPU  = 1469.6 MiB\n",
      "[Rebuilt] perplexity= 428383.216\n",
      "[Rebuilt] on-disk size = 1372.2 MiB\n"
     ]
    }
   ],
   "source": [
    "start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5858e1e0-40f2-4d81-9fae-5fe74b1e5a09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
