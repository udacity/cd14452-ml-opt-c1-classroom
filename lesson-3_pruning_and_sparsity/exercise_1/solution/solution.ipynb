{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "851cb58b-1175-45d8-9113-9869ca6cfed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d3b226d-9e0d-4ae9-984d-4e00e8006d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME     = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "MLP_PRUNE_FRAC = 0.5       # fraction of inner neurons to prune\n",
    "MAX_NEW_TOKENS = 50\n",
    "PROMPT = (\n",
    "    \"Over the next decade, sustainable energy solutions will revolutionize \"\n",
    "    \"global power grids, reducing carbon footprints and fostering resilient \"\n",
    "    \"communities through innovative storage and distribution technologies.\"\n",
    ")\n",
    "PERP_TEXT = (\n",
    "    \"Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast \"\n",
    "    \"to the natural intelligence displayed by humans and animals. Leading AI textbooks \"\n",
    "    \"define the field as the study of intelligent agents: any system that perceives \"\n",
    "    \"its environment and takes actions that maximize its chance of achieving its goals.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d36546f-6bbe-4d65-b952-2520b94ab986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name: str, device: torch.device):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Load AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "      - Load AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "      - Move model to `device` and set to .eval()\n",
    "      - Return tokenizer, model\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    # load model in FP16 for faster inference\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    # move to device and set to eval\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39e97ed4-3031-452c-b3d9-7d79afa84e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_baseline(model: nn.Module, tokenizer, prompt: str, perp_text: str, device: torch.device):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Warm up & measure generation latency & throughput on `prompt`\n",
    "      - Measure peak GPU memory & perplexity on `perp_text`\n",
    "      - Print or return these baseline metrics\n",
    "    \"\"\"\n",
    "    # 1) Measure latency & throughput\n",
    "    latency, throughput = measure_latency_and_throughput(model, tokenizer, prompt, device)\n",
    "    # 2) Measure peak GPU memory & perplexity\n",
    "    peak_mem, perplexity = measure_peak_mem_and_perplexity(model, tokenizer, perp_text, device)\n",
    "\n",
    "    # 3) Print baseline metrics\n",
    "    print(f\"[Baseline] latency   = {latency:.3f}s\")\n",
    "    print(f\"[Baseline] throughput= {throughput:.1f} tok/s\")\n",
    "    print(f\"[Baseline] peak GPU  = {peak_mem:.1f} MiB\")\n",
    "    print(f\"[Baseline] perplexity= {perplexity:.3f}\")\n",
    "\n",
    "    # Return in case caller wants to use them programmatically\n",
    "    return {\n",
    "        \"latency\": latency,\n",
    "        \"throughput\": throughput,\n",
    "        \"peak_gpu_mem\": peak_mem,\n",
    "        \"perplexity\": perplexity\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d94de27-8bb4-4110-84f5-b9cac02dc045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_mlp_rows_and_cols(model: nn.Module, prune_frac: float):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Move model to CPU\n",
    "      - For each layer in model.model.layers:\n",
    "          • Zero out `prune_frac` of rows in gate_proj and up_proj\n",
    "          • Zero out corresponding `prune_frac` of columns in down_proj\n",
    "      - Remove pruning reparameterizations\n",
    "    \"\"\"\n",
    "    # 1) Ensure we prune on CPU to avoid GPU OOM\n",
    "    model.cpu()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # 2) Iterate through each decoder layer’s MLP\n",
    "    for layer in model.model.layers:\n",
    "        gate = layer.mlp.gate_proj   # [inner, hidden]\n",
    "        up   = layer.mlp.up_proj     # [inner, hidden]\n",
    "        down = layer.mlp.down_proj   # [hidden, inner]\n",
    "\n",
    "        # 2a) Zero out rows in gate_proj and up_proj\n",
    "        for proj in (gate, up):\n",
    "            prune.ln_structured(\n",
    "                proj,\n",
    "                name=\"weight\",\n",
    "                amount=prune_frac,\n",
    "                n=1,\n",
    "                dim=0,           # prune entire rows\n",
    "            )\n",
    "            prune.remove(proj, \"weight\")\n",
    "\n",
    "        # 2b) Zero out corresponding columns in down_proj\n",
    "        prune.ln_structured(\n",
    "            down,\n",
    "            name=\"weight\",\n",
    "            amount=prune_frac,\n",
    "            n=1,\n",
    "            dim=1,               # prune entire columns\n",
    "        )\n",
    "        prune.remove(down, \"weight\")\n",
    "\n",
    "    # 3) Return the model (now with zeros in place)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d85619ae-1a0f-44f2-99f1-cb8cb3d7bf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_mlp_blocks(model: nn.Module):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - For each layer in model.model.layers:\n",
    "          1) Identify kept neuron indices in gate_proj\n",
    "          2) Construct new nn.Linear modules for gate_proj, up_proj, down_proj\n",
    "             with reduced dimensions\n",
    "          3) Copy over weights and biases\n",
    "          4) Replace the old modules on the model\n",
    "    \"\"\"\n",
    "    for layer in model.model.layers:\n",
    "        # original modules (still on CPU, dtype=original)\n",
    "        old_gate = layer.mlp.gate_proj\n",
    "        old_up   = layer.mlp.up_proj\n",
    "        old_down = layer.mlp.down_proj\n",
    "\n",
    "        # discover surviving rows in gate_proj\n",
    "        Wg = old_gate.weight.data     # [inner_orig, hidden], dtype say torch.half\n",
    "        keep_idx = (Wg.abs().sum(dim=1) != 0).nonzero(as_tuple=False).view(-1)\n",
    "        inner_new = keep_idx.numel()\n",
    "        hidden    = Wg.size(1)\n",
    "        dtype     = Wg.dtype\n",
    "        device    = Wg.device\n",
    "\n",
    "        # helper to build a new Linear with the same dtype/device\n",
    "        def make_linear(in_f, out_f, bias, old_weight, old_bias=None):\n",
    "            nl = nn.Linear(in_f, out_f, bias=bias)\n",
    "            # init in correct dtype & device\n",
    "            nl.weight.data = old_weight.clone().to(device=device, dtype=dtype)\n",
    "            if bias and old_bias is not None:\n",
    "                nl.bias.data = old_bias.clone().to(device=device, dtype=dtype)\n",
    "            return nl\n",
    "\n",
    "        # rebuild gate_proj: hidden -> inner_new\n",
    "        new_gate = make_linear(\n",
    "            hidden, inner_new, \n",
    "            bias=(old_gate.bias is not None),\n",
    "            old_weight=old_gate.weight.data[keep_idx],\n",
    "            old_bias=old_gate.bias.data[keep_idx] if old_gate.bias is not None else None\n",
    "        )\n",
    "\n",
    "        # rebuild up_proj: hidden -> inner_new\n",
    "        new_up = make_linear(\n",
    "            hidden, inner_new,\n",
    "            bias=(old_up.bias is not None),\n",
    "            old_weight=old_up.weight.data[keep_idx],\n",
    "            old_bias=old_up.bias.data[keep_idx] if old_up.bias is not None else None\n",
    "        )\n",
    "\n",
    "        # rebuild down_proj: inner_new -> hidden\n",
    "        new_down = make_linear(\n",
    "            inner_new, hidden,\n",
    "            bias=(old_down.bias is not None),\n",
    "            old_weight=old_down.weight.data[:, keep_idx],\n",
    "            old_bias=old_down.bias.data if old_down.bias is not None else None\n",
    "        )\n",
    "\n",
    "        # swap in-place\n",
    "        layer.mlp.gate_proj = new_gate\n",
    "        layer.mlp.up_proj   = new_up\n",
    "        layer.mlp.down_proj = new_down\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79f5c2f5-9541-42e1-8389-dcd1b216add7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_rebuilt(model: nn.Module, tokenizer, prompt: str, perp_text: str, device: torch.device):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Move rebuilt model to `device` & .eval()\n",
    "      - Re-measure latency, throughput, peak memory, perplexity\n",
    "      - Print or return these metrics\n",
    "    \"\"\"\n",
    "    # 1) Move to device and set to eval\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # 2) Measure latency & throughput\n",
    "    latency, throughput = measure_latency_and_throughput(\n",
    "        model, tokenizer, prompt, device\n",
    "    )\n",
    "\n",
    "    # 3) Measure peak GPU memory & perplexity\n",
    "    peak_mem, perplexity = measure_peak_mem_and_perplexity(\n",
    "        model, tokenizer, perp_text, device\n",
    "    )\n",
    "\n",
    "    # 4) Print results\n",
    "    print(f\"[Rebuilt] latency   = {latency:.3f}s\")\n",
    "    print(f\"[Rebuilt] throughput= {throughput:.1f} tok/s\")\n",
    "    print(f\"[Rebuilt] peak GPU  = {peak_mem:.1f} MiB\")\n",
    "    print(f\"[Rebuilt] perplexity= {perplexity:.3f}\")\n",
    "\n",
    "    # 5) Return for further use if needed\n",
    "    return {\n",
    "        \"latency\": latency,\n",
    "        \"throughput\": throughput,\n",
    "        \"peak_gpu_mem\": peak_mem,\n",
    "        \"perplexity\": perplexity\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b5cde41-77ee-4339-a06c-6fe825e6ad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_report_size(model: nn.Module, output_dir: str):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - model.save_pretrained(output_dir)\n",
    "      - Walk `output_dir` to sum file sizes (in MiB)\n",
    "      - Print the on-disk size\n",
    "    \"\"\"\n",
    "    # 1) Save\n",
    "    model.save_pretrained(output_dir)\n",
    "\n",
    "    # 2) Sum file sizes\n",
    "    total_bytes = 0\n",
    "    for root, _, files in os.walk(output_dir):\n",
    "        for fname in files:\n",
    "            total_bytes += os.path.getsize(os.path.join(root, fname))\n",
    "\n",
    "    # 3) Convert to MiB and print\n",
    "    size_mb = total_bytes / 1024**2\n",
    "    print(f\"[Rebuilt] on-disk size = {size_mb:.1f} MiB\")\n",
    "\n",
    "    return size_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77c3803f-9b33-42b3-a1cf-0891f0ea130b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start():\n",
    "    device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    tokenizer, model = load_model_and_tokenizer(MODEL_NAME, device)\n",
    "\n",
    "    # Baseline\n",
    "    measure_baseline(model, tokenizer, PROMPT, PERP_TEXT, device)\n",
    "\n",
    "    # Prune on CPU\n",
    "    prune_mlp_rows_and_cols(model, MLP_PRUNE_FRAC)\n",
    "\n",
    "    # Rebuild smaller MLPs\n",
    "    rebuild_mlp_blocks(model)\n",
    "\n",
    "    # Re-benchmark rebuilt model\n",
    "    measure_rebuilt(model, tokenizer, PROMPT, PERP_TEXT, device)\n",
    "\n",
    "    # Save & report on-disk size\n",
    "    save_and_report_size(model, \"llama_pruned_rebuilt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2824316e-ff26-4dfa-b1f4-f7d3e4c3e2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 00:54:22.776464: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-31 00:54:22.791036: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-08-31 00:54:22.809382: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-08-31 00:54:22.815104: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-08-31 00:54:22.828245: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Baseline] latency   = 1.164s\n",
      "[Baseline] throughput= 43.0 tok/s\n",
      "[Baseline] peak GPU  = 2129.6 MiB\n",
      "[Baseline] perplexity= 4.557\n",
      "[Rebuilt] latency   = 1.141s\n",
      "[Rebuilt] throughput= 43.8 tok/s\n",
      "[Rebuilt] peak GPU  = 1469.6 MiB\n",
      "[Rebuilt] perplexity= 428383.216\n",
      "[Rebuilt] on-disk size = 1372.2 MiB\n"
     ]
    }
   ],
   "source": [
    "start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5858e1e0-40f2-4d81-9fae-5fe74b1e5a09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
