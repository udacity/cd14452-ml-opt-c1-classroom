{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95b441f8",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "This cell:\n",
    "- Imports standard libraries for file handling, timing, and mathematical operations.\n",
    "- Imports PyTorch for deep learning operations and pruning utilities.\n",
    "- Imports Hugging Face Transformers for model and tokenizer handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "851cb58b-1175-45d8-9113-9869ca6cfed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd025667",
   "metadata": {},
   "source": [
    "# Define Model and Evaluation Settings\n",
    "This cell:\n",
    "- Specifies the model name to be used for pruning and evaluation.\n",
    "- Defines the fraction of neurons to prune in the MLP layers (`MLP_PRUNE_FRAC`).\n",
    "- Sets the maximum number of tokens to generate during inference.\n",
    "- Provides sample texts for benchmarking latency, throughput, and perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d3b226d-9e0d-4ae9-984d-4e00e8006d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME     = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "MLP_PRUNE_FRAC = 0.5       # fraction of inner neurons to prune\n",
    "MAX_NEW_TOKENS = 50\n",
    "PROMPT = (\n",
    "    \"Over the next decade, sustainable energy solutions will revolutionize \"\n",
    "    \"global power grids, reducing carbon footprints and fostering resilient \"\n",
    "    \"communities through innovative storage and distribution technologies.\"\n",
    ")\n",
    "PERP_TEXT = (\n",
    "    \"Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast \"\n",
    "    \"to the natural intelligence displayed by humans and animals. Leading AI textbooks \"\n",
    "    \"define the field as the study of intelligent agents: any system that perceives \"\n",
    "    \"its environment and takes actions that maximize its chance of achieving its goals.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109bf67b",
   "metadata": {},
   "source": [
    "# Load Model and Tokenizer\n",
    "This function:\n",
    "- Loads the tokenizer and model using Hugging Face Transformers.\n",
    "- Configures the model to use FP16 precision for faster inference.\n",
    "- Moves the model to the specified device (CPU or GPU).\n",
    "- Sets the model to evaluation mode to disable gradient computations.\n",
    "- Returns the loaded tokenizer and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d36546f-6bbe-4d65-b952-2520b94ab986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name: str, device: torch.device):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Load AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "      - Load AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "      - Move model to `device` and set to .eval()\n",
    "      - Return tokenizer, model\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef40413",
   "metadata": {},
   "source": [
    "# Measure Baseline Performance\n",
    "This function:\n",
    "- Measures the baseline performance of the model before pruning.\n",
    "- Evaluates:\n",
    "  - **Latency**: Time taken to generate tokens for a given prompt.\n",
    "  - **Throughput**: Tokens generated per second.\n",
    "  - **Peak GPU Memory Usage**: Maximum memory used during inference.\n",
    "  - **Perplexity**: A measure of how well the model predicts the given text.\n",
    "- Prints the baseline metrics for comparison with the pruned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f49b39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_latency_and_throughput(model, tokenizer, prompt: str, device: torch.device, max_new_tokens=50, runs=3):\n",
    "    \"\"\"\n",
    "    Measure latency and throughput for text generation.\n",
    "\n",
    "    Args:\n",
    "        model: The language model to evaluate.\n",
    "        tokenizer: The tokenizer associated with the model.\n",
    "        prompt (str): Input prompt for text generation.\n",
    "        device (torch.device): Device to run the model on.\n",
    "        max_new_tokens (int): Maximum number of new tokens to generate.\n",
    "        runs (int): Number of runs for averaging metrics.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Average latency (seconds) and throughput (tokens per second).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "    # Warmup\n",
    "    with torch.inference_mode():\n",
    "        _ = model.generate(**inputs, max_new_tokens=8)\n",
    "\n",
    "    latencies = []\n",
    "    throughputs = []\n",
    "    for _ in range(runs):\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        t0 = time.perf_counter()\n",
    "        with torch.inference_mode():\n",
    "            outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        t1 = time.perf_counter()\n",
    "\n",
    "        gen_len = outputs.shape[1] - inputs[\"input_ids\"].shape[1]\n",
    "        latency = t1 - t0\n",
    "        throughput = gen_len / latency if latency > 0 else float(\"nan\")\n",
    "\n",
    "        latencies.append(latency)\n",
    "        throughputs.append(throughput)\n",
    "\n",
    "    avg_latency = sum(latencies) / len(latencies)\n",
    "    avg_throughput = sum(throughputs) / len(throughputs)\n",
    "    return avg_latency, avg_throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39e97ed4-3031-452c-b3d9-7d79afa84e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_baseline(model: nn.Module, tokenizer, prompt: str, perp_text: str, device: torch.device):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Warm up & measure generation latency & throughput on `prompt`\n",
    "      - Measure peak GPU memory & perplexity on `perp_text`\n",
    "      - Print or return these baseline metrics\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c1363b",
   "metadata": {},
   "source": [
    "# Prune MLP Rows and Columns\n",
    "This function:\n",
    "- Prunes the MLP layers in the model by:\n",
    "  - Zeroing out a fraction of rows in the `gate_proj` and `up_proj` layers.\n",
    "  - Zeroing out the corresponding columns in the `down_proj` layer.\n",
    "- Uses structured pruning to remove entire rows or columns.\n",
    "- Ensures pruning is performed on the CPU to avoid GPU memory issues.\n",
    "- Removes the pruning reparameterizations after applying the masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d94de27-8bb4-4110-84f5-b9cac02dc045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_mlp_rows_and_cols(model: nn.Module, prune_frac: float):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Move model to CPU\n",
    "      - For each layer in model.model.layers:\n",
    "          • Zero out `prune_frac` of rows in gate_proj and up_proj\n",
    "          • Zero out corresponding `prune_frac` of columns in down_proj\n",
    "      - Remove pruning reparameterizations\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead0a4b2",
   "metadata": {},
   "source": [
    "# Rebuild MLP Blocks\n",
    "This function:\n",
    "- Reconstructs the pruned MLP layers with reduced dimensions.\n",
    "- Identifies the neurons that were not pruned in the `gate_proj` layer.\n",
    "- Creates new `nn.Linear` modules for `gate_proj`, `up_proj`, and `down_proj` with updated dimensions.\n",
    "- Copies the weights and biases from the original layers to the new layers.\n",
    "- Replaces the old modules with the new ones in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d85619ae-1a0f-44f2-99f1-cb8cb3d7bf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_mlp_blocks(model: nn.Module):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - For each layer in model.model.layers:\n",
    "          1) Identify kept neuron indices in gate_proj\n",
    "          2) Construct new nn.Linear modules for gate_proj, up_proj, down_proj\n",
    "             with reduced dimensions\n",
    "          3) Copy over weights and biases\n",
    "          4) Replace the old modules on the model\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534e2189",
   "metadata": {},
   "source": [
    "# Measure Performance After Rebuilding\n",
    "This function:\n",
    "- Evaluates the performance of the rebuilt model after pruning and reconstruction.\n",
    "- Measures:\n",
    "  - **Latency**: Time taken to generate tokens for a given prompt.\n",
    "  - **Throughput**: Tokens generated per second.\n",
    "  - **Peak GPU Memory Usage**: Maximum memory used during inference.\n",
    "  - **Perplexity**: A measure of how well the model predicts the given text.\n",
    "- Prints the metrics for comparison with the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79f5c2f5-9541-42e1-8389-dcd1b216add7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_rebuilt(model: nn.Module, tokenizer, prompt: str, perp_text: str, device: torch.device):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Move rebuilt model to `device` & .eval()\n",
    "      - Re-measure latency, throughput, peak memory, perplexity\n",
    "      - Print or return these metrics\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186d3b28",
   "metadata": {},
   "source": [
    "# Save Model and Report Size\n",
    "This function:\n",
    "- Saves the pruned and rebuilt model to the specified output directory.\n",
    "- Calculates the total size of the saved model files on disk.\n",
    "- Prints the on-disk size of the model for comparison with the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b5cde41-77ee-4339-a06c-6fe825e6ad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_report_size(model: nn.Module, output_dir: str):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - model.save_pretrained(output_dir)\n",
    "      - Walk `output_dir` to sum file sizes (in MiB)\n",
    "      - Print the on-disk size\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7e4a90",
   "metadata": {},
   "source": [
    "# Main Execution Flow\n",
    "This function:\n",
    "- Loads the model and tokenizer.\n",
    "- Measures the baseline performance of the model.\n",
    "- Applies structured pruning to the MLP layers.\n",
    "- Rebuilds the pruned MLP layers with reduced dimensions.\n",
    "- Measures the performance of the rebuilt model.\n",
    "- Saves the pruned and rebuilt model to disk and reports its size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c3803f-9b33-42b3-a1cf-0891f0ea130b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start():\n",
    "    device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    tokenizer, model = load_model_and_tokenizer(MODEL_NAME, device)\n",
    "\n",
    "    # Baseline\n",
    "    measure_baseline(model, tokenizer, PROMPT, PERP_TEXT, device)\n",
    "\n",
    "    # Prune on CPU\n",
    "    prune_mlp_rows_and_cols(model, MLP_PRUNE_FRAC)\n",
    "\n",
    "    # Rebuild smaller MLPs\n",
    "    rebuild_mlp_blocks(model)\n",
    "\n",
    "    # Re-benchmark rebuilt model\n",
    "    measure_rebuilt(model, tokenizer, PROMPT, PERP_TEXT, device)\n",
    "\n",
    "    # Save & report on-disk size\n",
    "    save_and_report_size(model, \"llama_pruned_rebuilt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b7704e",
   "metadata": {},
   "source": [
    "# Start the Pruning and Evaluation Process\n",
    "This cell:\n",
    "- Calls the `start` function to execute the entire pruning and evaluation pipeline.\n",
    "- Outputs the baseline and post-pruning metrics, as well as the on-disk size of the pruned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2824316e-ff26-4dfa-b1f4-f7d3e4c3e2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
