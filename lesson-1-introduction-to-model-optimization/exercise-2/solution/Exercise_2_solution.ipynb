{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55aa6817-5cb7-40fb-ae31-522ac073db1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1190abf7-932f-4bb4-9620-1a0081689deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── SETTINGS ────────────────────────────────────────────────────────────────\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-1B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8d52c11-a328-496c-9bfc-d90ec45323c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name: str):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Load tokenizer & model from `model_name`\n",
    "      - Move model to GPU if available, choose float16 for CUDA else float32\n",
    "      - Set model to eval mode\n",
    "      - Return (tokenizer, model, device)\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    dtype  = torch.float16 if device.type == \"cuda\" else torch.float32\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=dtype\n",
    "    )\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return tokenizer, model, device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89e5c389-d355-45f4-a42f-ba3549c07529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_text():\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Pick or paste a ~100-token passage (e.g. a Wiki snippet)\n",
    "      - Return (text_str)\n",
    "    \"\"\"\n",
    "    return (\n",
    "        \"Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast to the natural \"\n",
    "        \"intelligence displayed by humans and animals. Leading AI textbooks define the field as the study \"\n",
    "        \"of intelligent agents: any system that perceives its environment and takes actions that maximize \"\n",
    "        \"its chance of achieving its goals. Colloquially, the term \\\"artificial intelligence\\\" is often \"\n",
    "        \"used to describe machines that mimic cognitive functions that humans associate with the human mind, \"\n",
    "        \"such as learning and problem-solving.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b052ce0-7926-49ca-bd28-0fd626883169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_labels(tokenizer, text: str):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Tokenize `text` with return_tensors=\"pt\"\n",
    "      - Prepare inputs and set labels = input_ids\n",
    "      - Return (inputs_dict, input_len)\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    inputs[\"labels\"] = inputs[\"input_ids\"].clone()\n",
    "    input_len = inputs[\"input_ids\"].size(1)\n",
    "    return inputs, input_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bbd671f-4844-4e4c-9c7d-e2c3cc31ebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_peak_memory_and_loss(model, inputs, device):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Reset peak memory stats if CUDA\n",
    "      - Run model(**inputs) under torch.no_grad()\n",
    "      - Sync CUDA if needed\n",
    "      - Retrieve peak memory via torch.cuda.max_memory_allocated (in MiB)\n",
    "      - Return (peak_mem_mib, loss_value)\n",
    "    \"\"\"\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.reset_peak_memory_stats(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=inputs[\"input_ids\"].to(device),\n",
    "            attention_mask=inputs.get(\"attention_mask\", None).to(device) if inputs.get(\"attention_mask\") is not None else None,\n",
    "            labels=inputs[\"labels\"].to(device)\n",
    "        )\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "        peak_bytes = torch.cuda.max_memory_allocated(device)\n",
    "        peak_mib = peak_bytes / 1024**2\n",
    "    else:\n",
    "        peak_mib = float(\"nan\")\n",
    "\n",
    "    return peak_mib, outputs.loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d1968fb-257c-43fe-8ec8-dad0f99389b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(loss: float):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Compute and return math.exp(loss)\n",
    "    \"\"\"\n",
    "    return math.exp(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "232ad78b-2ef9-450e-9dde-314ae0fcecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start():\n",
    "    # 1. Load\n",
    "    tokenizer, model, device = load_model_and_tokenizer(MODEL_NAME)\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # 2. Select & tokenize\n",
    "    text = select_text()\n",
    "    inputs, input_len = tokenize_with_labels(tokenizer, text)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    print(f\"Tokenized length: {input_len}\")\n",
    "\n",
    "    # 3. Measure peak memory & loss\n",
    "    peak_mem, loss = compute_peak_memory_and_loss(model, inputs, device)\n",
    "    print(f\"Peak GPU memory: {peak_mem:.1f} MiB\")\n",
    "\n",
    "    # 4. Compute perplexity\n",
    "    ppl = compute_perplexity(loss)\n",
    "    print(f\"Next-token perplexity: {ppl:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23a34155-3fb2-431f-ae24-5326387b874b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-03 01:08:25.013294: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-03 01:08:25.215918: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-08-03 01:08:25.249061: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-08-03 01:08:25.258579: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-08-03 01:08:25.492654: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Tokenized length: 94\n",
      "Peak GPU memory: 2484.6 MiB\n",
      "Next-token perplexity: 3.425\n"
     ]
    }
   ],
   "source": [
    "start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba59cae-817f-43e5-91d3-bcfabb52ab4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
