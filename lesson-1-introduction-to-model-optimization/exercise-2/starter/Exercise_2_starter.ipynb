{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8c4b7ff-8007-42d3-b546-161ea0311854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "505c6a6b-781d-479c-9da4-0bc638cbb1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── SETTINGS ────────────────────────────────────────────────────────────────\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-1B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7594d665-4e84-450e-b99e-4f314b3e2843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name: str):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Load tokenizer & model from `model_name`\n",
    "      - Move model to GPU if available, choose float16 for CUDA else float32\n",
    "      - Set model to eval mode\n",
    "      - Return (tokenizer, model, device)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a86d4106-cefb-4fbc-98ce-a706826987f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_text():\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Pick or paste a ~100-token passage (e.g. a Wiki snippet)\n",
    "      - Return (text_str)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd910644-75ec-4fe7-a543-5ffc1b859572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_labels(tokenizer, text: str):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Tokenize `text` with return_tensors=\"pt\"\n",
    "      - Prepare inputs and set labels = input_ids\n",
    "      - Return (inputs_dict, input_len)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a79bc77-fcab-4626-82a4-1f6478754e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_peak_memory_and_loss(model, inputs, device):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Reset peak memory stats if CUDA\n",
    "      - Run model(**inputs) under torch.no_grad()\n",
    "      - Sync CUDA if needed\n",
    "      - Retrieve peak memory via torch.cuda.max_memory_allocated (in MiB)\n",
    "      - Return (peak_mem_mib, loss_value)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "678ae06b-a587-4b21-8c88-75144b24dd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(loss: float):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Compute and return math.exp(loss)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473365bf-4f75-467b-a7cf-d6fe1e67868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start():\n",
    "    # 1. Load\n",
    "    tokenizer, model, device = load_model_and_tokenizer(MODEL_NAME)\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # 2. Select & tokenize\n",
    "    text = select_text()\n",
    "    inputs, input_len = tokenize_with_labels(tokenizer, text)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    print(f\"Tokenized length: {input_len}\")\n",
    "\n",
    "    # 3. Measure peak memory & loss\n",
    "    peak_mem, loss = compute_peak_memory_and_loss(model, inputs, device)\n",
    "    print(f\"Peak GPU memory: {peak_mem:.1f} MiB\")\n",
    "\n",
    "    # 4. Compute perplexity\n",
    "    ppl = compute_perplexity(loss)\n",
    "    print(f\"Next-token perplexity: {ppl:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d528dcca-537b-43d1-a05a-088cf6d16708",
   "metadata": {},
   "outputs": [],
   "source": [
    "start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
