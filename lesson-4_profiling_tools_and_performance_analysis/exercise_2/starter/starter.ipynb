{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e67f2abd-0fa2-4927-a7f4-a5e7e4e41a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.quantization as tq\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dbd0e98-eece-4a97-a9f5-ec03519ca90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME      = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "PRUNE_AMOUNT    = 0.30      # 30% magnitude pruning\n",
    "BATCH_SIZE      = 5\n",
    "MAX_NEW_TOKENS  = 50\n",
    "PROMPT          = (\n",
    "    \"In a world increasingly driven by artificial intelligence, the ability to interpret \"\n",
    "    \"large language models efficiently is crucial for both research and deployment.\"\n",
    ")\n",
    "LOGDIR_BASELINE = \"./profiler_logs/baseline\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d99d9f3-4584-4652-9abc-df351833ea1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(device: torch.device):\n",
    "    \"\"\"\n",
    "      - Load the tokenizer: AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "      - Load the model: AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float32)\n",
    "      - Move model to `device` and call .eval()\n",
    "      - Return (tokenizer, model)\n",
    "    \"\"\"\n",
    "    # 1) Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "    # 2) Load FP32 model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float32\n",
    "    )\n",
    "\n",
    "    # 3) Move to device and set to eval mode\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # 4) Return both\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcaecfae-66b7-46bf-9d62-213ecfbfe153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(tokenizer, prompt: str, device: torch.device):\n",
    "    \"\"\"\n",
    "      - Duplicate `prompt` BATCH_SIZE times into a list of strings\n",
    "      - Tokenize with padding and truncation: tokenizer(..., return_tensors=\"pt\")\n",
    "      - Move inputs to `device`\n",
    "      - Return the tokenized inputs\n",
    "    \"\"\"\n",
    "    # 1) Replicate prompt\n",
    "    texts = [prompt] * BATCH_SIZE\n",
    "\n",
    "    # 2) Tokenize with padding & truncation\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    # 3) Move to device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d149f08c-bbec-4027-b990-1f68e5dfe613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_inference(model, inputs, logdir: str, label: str):\n",
    "    \"\"\"\n",
    "      - Create `logdir` if it doesn't exist\n",
    "      - Use `torch.profiler.profile` (CPU & CUDA, record_shapes, profile_memory, with_stack)\n",
    "      - Inside the profiler, wrap the call to `model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS)` in `record_function(label)`\n",
    "      - After profiling, print:\n",
    "          1) Top-3 ops by \"self_cpu_time_total\"\n",
    "          2) Top-3 ops by \"self_cuda_time_total\"\n",
    "          3) Total CPU vs CUDA self-time in milliseconds\n",
    "      - Note: use `prof.key_averages().table(...)` and sum over `evt.self_cpu_time_total`, `evt.self_cuda_time_total`\n",
    "      - Traces should be saved automatically by `tensorboard_trace_handler`\n",
    "    \"\"\"\n",
    "    # 1) Ensure log directory exists\n",
    "    os.makedirs(logdir, exist_ok=True)\n",
    "\n",
    "    # 2) Run profiler\n",
    "    with profile(\n",
    "        activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True,\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler(logdir)\n",
    "    ) as prof:\n",
    "        with record_function(label):\n",
    "            _ = model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS)\n",
    "\n",
    "    # 3) Print top‐3 operators by CPU self time\n",
    "    print(f\"\\n=== {label} Top-3 ops by CPU self time ===\")\n",
    "    print(prof.key_averages().table(\n",
    "        sort_by=\"self_cpu_time_total\", row_limit=3\n",
    "    ))\n",
    "\n",
    "    # 4) Print top‐3 operators by CUDA self time\n",
    "    print(f\"\\n=== {label} Top-3 ops by CUDA self time ===\")\n",
    "    print(prof.key_averages().table(\n",
    "        sort_by=\"self_cuda_time_total\", row_limit=3\n",
    "    ))\n",
    "\n",
    "    # 5) Summarize total CPU vs CUDA self times\n",
    "    events     = prof.key_averages()\n",
    "    total_cpu  = sum(evt.self_cpu_time_total for evt in events)\n",
    "    total_cuda = sum(getattr(evt, \"self_cuda_time_total\", 0) for evt in events)\n",
    "    print(f\"\\n=== {label} Total self-time ===\")\n",
    "    print(f\"CPU  : {total_cpu/1e3:.2f} ms\")\n",
    "    print(f\"CUDA : {total_cuda/1e3:.2f} ms\")\n",
    "\n",
    "    print(f\"\\nTrace files for '{label}' written to: {logdir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8da2b96-d032-4652-9cce-2371abb33ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pruning_and_quant(model: nn.Module):\n",
    "    \"\"\"\n",
    "      - On CPU, apply `prune.l1_unstructured(..., amount=PRUNE_AMOUNT)` to every nn.Linear weight\n",
    "      - Call `prune.remove(...)` to make masks permanent\n",
    "      - Then apply `torch.quantization.quantize_dynamic` on {nn.Linear} with dtype=torch.qint8\n",
    "      - Return the quantized model\n",
    "    \"\"\"\n",
    "    # 1) Prune on CPU\n",
    "    model.cpu()\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # zero out PRUNE_AMOUNT fraction of the smallest‐magnitude weights\n",
    "            prune.l1_unstructured(module, name=\"weight\", amount=PRUNE_AMOUNT)\n",
    "            # make the pruning permanent\n",
    "            prune.remove(module, \"weight\")\n",
    "\n",
    "    # 2) Dynamic 8-bit quantization\n",
    "    quantized = tq.quantize_dynamic(\n",
    "        model,\n",
    "        {nn.Linear},\n",
    "        dtype=torch.qint8\n",
    "    )\n",
    "\n",
    "    return quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7058668-777e-4a34-9cc5-c499064891e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 1) Load & benchmark baseline\n",
    "    tokenizer, model = load_model(device)\n",
    "    batch_inputs     = make_batch(tokenizer, PROMPT, device)\n",
    "    print(\"Profiling inference…\")\n",
    "    profile_inference(model, batch_inputs, LOGDIR_BASELINE, label=\"Baseline\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21dc4a55-3745-4443-8ace-e1a44ec90039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling inference…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2025-09-06 02:57:50 10150:10150 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2025-09-06 02:57:57 10150:10150 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2025-09-06 02:57:57 10150:10150 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Baseline Top-3 ops by CPU self time ===\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                               Baseline        36.95%        1.494s       100.00%        4.043s        4.043s       0.000us         0.00%        1.458s        1.458s           0 b      -3.37 Kb       8.13 Mb      -7.29 Gb             1  \n",
      "                                             cudaMalloc        14.63%     591.609ms        14.63%     591.609ms      24.650ms       5.000us         0.00%       5.000us       0.208us           0 b           0 b           0 b           0 b            24  \n",
      "                                       cudaLaunchKernel        13.53%     546.859ms        13.53%     546.859ms      10.458us     115.198ms         2.42%     115.198ms       2.203us           0 b           0 b           0 b           0 b         52290  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 4.043s\n",
      "Self CUDA time total: 4.769s\n",
      "\n",
      "\n",
      "=== Baseline Top-3 ops by CUDA self time ===\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                               Baseline         0.00%       0.000us         0.00%       0.000us       0.000us        3.435s        72.03%        3.435s        3.435s           0 b           0 b           0 b           0 b             1  \n",
      "                                               aten::mm         5.53%     223.522ms        12.84%     519.233ms      66.998us        1.191s        24.98%        1.223s     157.770us           0 b           0 b     634.13 Mb     634.13 Mb          7750  \n",
      "void gemmSN_TN_kernel<float, 128, 16, 2, 4, 6, 7, fa...         0.00%       0.000us         0.00%       0.000us       0.000us     556.512ms        11.67%     556.512ms     127.611us           0 b           0 b           0 b           0 b          4361  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 4.043s\n",
      "Self CUDA time total: 4.769s\n",
      "\n",
      "\n",
      "=== Baseline Total self-time ===\n",
      "CPU  : 4042.71 ms\n",
      "CUDA : 6226.79 ms\n",
      "\n",
      "Trace files for 'Baseline' written to: ./profiler_logs/baseline\n"
     ]
    }
   ],
   "source": [
    "start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9764bf-4e25-4c1a-b09e-b9c7b340ab05",
   "metadata": {},
   "source": [
    "# Exercise 1: Measure Model Size & Nonzeros\n",
    "\n",
    "## Concept: After pruning and quantization, the student should measure how the model structure has changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e8bb030-e095-46b8-9c53-c2cbb9cf32a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, torch\n",
    "import numpy as np\n",
    "\n",
    "def _safe_state_dict(model):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Get the model state_dict\n",
    "      - Build a new dict that keeps ONLY valid tensor-like entries\n",
    "      - If value is a torch.Tensor -> detach, move to CPU\n",
    "      - If value is a numpy.ndarray -> convert to torch tensor\n",
    "      - Drop any other types (dtype objects, None, etc.)\n",
    "      - Return the filtered dict\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement _safe_state_dict\")\n",
    "\n",
    "\n",
    "def _dir_size_mb(path: str) -> float:\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Walk the directory tree starting at path\n",
    "      - Sum file sizes (os.path.getsize for each file)\n",
    "      - Return total size in megabytes (divide by 1024**2)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement _dir_size_mb\")\n",
    "\n",
    "\n",
    "def measure_model_sparsity_and_size(model, name: str = \"model\"):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Initialize counters total=0, nonzero=0\n",
    "      - Iterate over model.modules()\n",
    "          * If module has a weight tensor, accumulate total params and nonzero count\n",
    "      - Print sparsity stats: nonzero/total and % sparse\n",
    "      - Create/reset a temporary directory for saving\n",
    "      - Try: if model has save_pretrained, call it\n",
    "      - Except: fall back to _safe_state_dict(model) + torch.save(...)\n",
    "      - Compute dir size with _dir_size_mb\n",
    "      - Print final disk size\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement measure_model_sparsity_and_size\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2075985-cde7-4f31-856d-15a7f8f12c1b",
   "metadata": {},
   "source": [
    "# Exercise 2 — Batch size & prompt-length sensitivity (latency/throughput)\n",
    "\n",
    "## Concept: Show how decoding cost scales with batch size and max_new_tokens on your baseline vs optimized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0646ffad-41a1-476f-aa61-e3ed59f0cfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _model_device(model: torch.nn.Module) -> torch.device:\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Inspect model parameters and buffers.\n",
    "      - If any parameter exists, return its device.\n",
    "      - Else if any buffer exists, return its device.\n",
    "      - Else default to torch.device(\"cpu\").\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement _model_device\")\n",
    "\n",
    "\n",
    "def sweep_batch_and_length(tokenizer, model_dict, _unused_device,\n",
    "                           prompt, batch_sizes=(1, 2, 4),\n",
    "                           gen_lengths=(16, 64, 128),\n",
    "                           runs=3):\n",
    "    \"\"\"\n",
    "    TODO: Implement a device-safe sweep to benchmark models.\n",
    "\n",
    "    Steps:\n",
    "      1. Tokenize the prompt ONCE on CPU for a single sample.\n",
    "      2. For each model in model_dict:\n",
    "          a. Detect its device using _model_device.\n",
    "          b. For each batch size:\n",
    "              - Tile the single sample to match batch size.\n",
    "              - Move tensors to the model's device.\n",
    "          c. For each generation length:\n",
    "              - Run multiple trials (runs).\n",
    "              - Use torch.cuda.synchronize() before/after if device is CUDA.\n",
    "              - Time model.generate with max_new_tokens=L, use_cache=True.\n",
    "              - Compute latency and throughput (tokens/sec across the whole batch).\n",
    "      3. Append results as dicts with keys:\n",
    "          [\"model\", \"batch_size\", \"max_new_tokens\", \"latency_s\", \"tokens_per_sec\", \"device\"].\n",
    "      4. Return results as a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement sweep_batch_and_length\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d57909-4f55-42e6-a4b1-7aa959b242a9",
   "metadata": {},
   "source": [
    "# Exercise 3 — Operator hot-spots before vs after optimization\n",
    "\n",
    "## Concept: Use torch.profiler to summarize top operators for baseline vs optimized. Students see where time is spent (e.g., attention matmuls vs layernorm) and how it shifts after pruning/quant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d82aa4-7586-4b01-b7e7-e782ce2ec741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Robust operator hot-spot summary (event-based, device-safe) ---\n",
    "from torch.profiler import profile, ProfilerActivity\n",
    "import torch\n",
    "\n",
    "\n",
    "def _model_device(model: torch.nn.Module) -> torch.device:\n",
    "    \"\"\"\n",
    "    Return the device of the model parameters/buffers.\n",
    "    Students: Implement device extraction logic.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Return the device where the model is located\")\n",
    "\n",
    "\n",
    "def _to_device_batch(batch: dict, device: torch.device) -> dict:\n",
    "    \"\"\"\n",
    "    Move all tensors in batch to the specified device.\n",
    "    Students: Implement tensor device transfer logic.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Move all tensors in the batch to the given device\")\n",
    "\n",
    "\n",
    "def top_ops_summary(model, batch_inputs, device_unused, with_cuda=True, top_k=10, label=\"run\"):\n",
    "    \"\"\"\n",
    "    Profile the model for a single short run and summarize top CPU/CUDA operators.\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"label\": str,\n",
    "            \"device\": str,\n",
    "            \"cpu\": [(op_name, time_ms), ...],\n",
    "            \"cuda\": [(op_name, time_ms), ...],\n",
    "            \"counts\": dict\n",
    "        }\n",
    "    \"\"\"\n",
    "    # TODO: Step 1 – Get model device\n",
    "    mdev = _model_device(model)\n",
    "\n",
    "    # TODO: Step 2 – Move batch to device\n",
    "    enc = _to_device_batch(batch_inputs, mdev)\n",
    "\n",
    "    # TODO: Step 3 – Select profiler activities\n",
    "    raise NotImplementedError(\"Setup profiler activities (CPU always, CUDA if available)\")\n",
    "\n",
    "    # TODO: Step 4 – Run model under profiler\n",
    "    # Hint: use torch.inference_mode(), torch.cuda.synchronize(), and model.generate()\n",
    "    raise NotImplementedError(\"Profile the model execution with torch.profiler\")\n",
    "\n",
    "    # TODO: Step 5 – Extract event statistics\n",
    "    # Hint: prof.key_averages(), self_cpu_time_total, self_cuda_time_total\n",
    "    raise NotImplementedError(\"Summarize operator self-times into top-k CPU/CUDA ops\")\n",
    "\n",
    "    # TODO: Step 6 – Return structured dictionary\n",
    "    raise NotImplementedError(\"Return dict with label, device, cpu, cuda, counts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c645b39f-ac58-4bb6-b72b-2019a9e1f5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ Exercise 1: Sparsity & Size ================\n",
      "[baseline] Nonzero params: 789722330/1100048384 (28.21% sparse)\n",
      "[baseline] Disk size: 4196.37 MB\n",
      "[optimized] Nonzero params: 65628160/65628160 (0.00% sparse)\n",
      "[optimized] Disk size: 250.45 MB\n",
      "\n",
      "================ Exercise 2: Batch Size & Length Sweep ================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>max_new_tokens</th>\n",
       "      <th>latency_s</th>\n",
       "      <th>tokens_per_sec</th>\n",
       "      <th>device</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>1.564530</td>\n",
       "      <td>10.228149</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>baseline</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>5.054521</td>\n",
       "      <td>12.667585</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>baseline</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>1.840176</td>\n",
       "      <td>17.395002</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>baseline</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>6.051661</td>\n",
       "      <td>21.179314</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>optimized</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0.679012</td>\n",
       "      <td>23.564206</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>optimized</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>2.692659</td>\n",
       "      <td>23.768656</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>optimized</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>0.767073</td>\n",
       "      <td>41.722865</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>optimized</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>2.942899</td>\n",
       "      <td>43.494537</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model  batch_size  max_new_tokens  latency_s  tokens_per_sec device\n",
       "0   baseline           1              16   1.564530       10.228149    cpu\n",
       "1   baseline           1              64   5.054521       12.667585    cpu\n",
       "2   baseline           2              16   1.840176       17.395002    cpu\n",
       "3   baseline           2              64   6.051661       21.179314    cpu\n",
       "4  optimized           1              16   0.679012       23.564206    cpu\n",
       "5  optimized           1              64   2.692659       23.768656    cpu\n",
       "6  optimized           2              16   0.767073       41.722865    cpu\n",
       "7  optimized           2              64   2.942899       43.494537    cpu"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ Exercise 3: Operator Hot-Spots ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2025-09-06 03:25:30 10150:10150 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2025-09-06 03:25:35 10150:10150 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2025-09-06 03:25:35 10150:10150 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n",
      "STAGE:2025-09-06 03:25:45 10150:10150 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2025-09-06 03:25:47 10150:10150 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2025-09-06 03:25:47 10150:10150 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline device: cpu | events: {'evt_total': 74, 'cpu_nonzero': 8, 'cuda_nonzero': 0}\n",
      "Optimized device: cpu | events: {'evt_total': 72, 'cpu_nonzero': 8, 'cuda_nonzero': 0}\n",
      "\n",
      "Top CPU ops (baseline): [('aten::mm', 3373.148), ('aten::mul', 69.753), ('aten::cat', 56.771), ('aten::_scaled_dot_product_flash_attention_for_cpu', 52.607), ('aten::copy_', 49.95), ('aten::add', 37.566), ('aten::matmul', 37.352), ('aten::silu', 30.704)]\n",
      "Top CPU ops (optimized): [('quantized::linear_dynamic', 601.199), ('aten::cat', 49.253), ('aten::mul', 45.053), ('aten::_scaled_dot_product_flash_attention_for_cpu', 45.007), ('aten::copy_', 34.219), ('aten::add', 25.002), ('aten::empty', 21.191), ('aten::silu', 20.13)]\n"
     ]
    }
   ],
   "source": [
    "# === Run All Exercises ===\n",
    "def run_all_exercises():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 1) Load baseline model & tokenizer\n",
    "    tokenizer, baseline_model = load_model(device)\n",
    "    batch_inputs = make_batch(tokenizer, PROMPT, device)\n",
    "\n",
    "    # 2) Create optimized model (prune + quantize)\n",
    "    optimized_model = apply_pruning_and_quant(baseline_model)\n",
    "\n",
    "    print(\"\\n================ Exercise 1: Sparsity & Size ================\")\n",
    "    measure_model_sparsity_and_size(baseline_model, \"baseline\")\n",
    "    measure_model_sparsity_and_size(optimized_model, \"optimized\")\n",
    "\n",
    "    print(\"\\n================ Exercise 2: Batch Size & Length Sweep ================\")\n",
    "    df_sweep = sweep_batch_and_length(\n",
    "        tokenizer,\n",
    "        {\"baseline\": baseline_model, \"optimized\": optimized_model},\n",
    "        device,\n",
    "        PROMPT,\n",
    "        batch_sizes=(1, 2),\n",
    "        gen_lengths=(16, 64),\n",
    "        runs=2\n",
    "    )\n",
    "    display(df_sweep)\n",
    "\n",
    "    print(\"\\n================ Exercise 3: Operator Hot-Spots ================\")\n",
    "    base_ops = top_ops_summary(baseline_model, batch_inputs, device, with_cuda=True, top_k=8, label=\"baseline\")\n",
    "    opt_ops  = top_ops_summary(optimized_model, batch_inputs, device, with_cuda=True, top_k=8, label=\"optimized\")\n",
    "    \n",
    "    print(\"Baseline device:\", base_ops[\"device\"], \"| events:\", base_ops[\"counts\"])\n",
    "    print(\"Optimized device:\", opt_ops[\"device\"], \"| events:\", opt_ops[\"counts\"])\n",
    "    print(\"\\nTop CPU ops (baseline):\", base_ops[\"cpu\"])\n",
    "    print(\"Top CPU ops (optimized):\", opt_ops[\"cpu\"])\n",
    "    \n",
    "# Call it\n",
    "run_all_exercises()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035443d5-b3c0-4705-b165-edf95c1920dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
