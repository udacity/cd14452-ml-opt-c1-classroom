{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89bc564b",
   "metadata": {},
   "source": [
    "# üöÄ Import Required Libraries\n",
    "This cell:\n",
    "- Imports essential libraries for PyTorch operations, pruning, and quantization.\n",
    "- Imports Hugging Face Transformers for model and tokenizer handling.\n",
    "- Imports PyTorch Profiler for performance analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d34575b",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "The demo requires a high-performance instance. We recommend running it on your personal workstation or provisioning a suitable cloud instance. On AWS, a g4dn.4xlarge instance is sufficient.\n",
    "\n",
    "#### Recommended Specifications:\n",
    "\n",
    "- vCPUs: 16\n",
    "- GPU: 1 √ó NVIDIA T4 (16 GB VRAM)\n",
    "- Memory (RAM): 64 GB\n",
    "- CUDA / GPU Drivers: CUDA 11.8 or later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e67f2abd-0fa2-4927-a7f4-a5e7e4e41a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.quantization as tq\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c471339",
   "metadata": {},
   "source": [
    "# üîß Define Model and Profiling Settings\n",
    "This cell:\n",
    "- Specifies the model name to be used for pruning and profiling.\n",
    "- Defines the pruning amount (`PRUNE_AMOUNT`) as 30%.\n",
    "- Sets the batch size and maximum number of tokens to generate during inference.\n",
    "- Provides a sample prompt for benchmarking.\n",
    "- Specifies the directory to save profiler logs for the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dbd0e98-eece-4a97-a9f5-ec03519ca90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME      = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "PRUNE_AMOUNT    = 0.30      # 30% magnitude pruning\n",
    "BATCH_SIZE      = 5\n",
    "MAX_NEW_TOKENS  = 50\n",
    "PROMPT          = (\n",
    "    \"In a world increasingly driven by artificial intelligence, the ability to interpret \"\n",
    "    \"large language models efficiently is crucial for both research and deployment.\"\n",
    ")\n",
    "LOGDIR_BASELINE = \"./profiler_logs/baseline\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e728fd4",
   "metadata": {},
   "source": [
    "# üì¶ Load Model and Tokenizer\n",
    "This function:\n",
    "- Loads the tokenizer and model using Hugging Face Transformers.\n",
    "- Configures the model to use FP32 precision for accurate profiling.\n",
    "- Moves the model to the specified device (CPU or GPU).\n",
    "- Sets the model to evaluation mode to disable gradient computations.\n",
    "- Returns the loaded tokenizer and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d99d9f3-4584-4652-9abc-df351833ea1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(device: torch.device):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Load the tokenizer: AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "      - Load the model: AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float32)\n",
    "      - Move model to `device` and call .eval()\n",
    "      - Return (tokenizer, model)\n",
    "    \"\"\"\n",
    "    # 1) Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "    # 2) Load FP32 model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float32\n",
    "    )\n",
    "\n",
    "    # 3) Move to device and set to eval mode\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # 4) Return both\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950435f7",
   "metadata": {},
   "source": [
    "# üìù Create a Batch of Inputs\n",
    "This function:\n",
    "- Duplicates the provided prompt `BATCH_SIZE` times to simulate a batch of inputs.\n",
    "- Tokenizes the batch with padding and truncation to ensure uniform input size.\n",
    "- Moves the tokenized inputs to the specified device (CPU or GPU).\n",
    "- Returns the prepared batch of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcaecfae-66b7-46bf-9d62-213ecfbfe153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(tokenizer, prompt: str, device: torch.device):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Duplicate `prompt` BATCH_SIZE times into a list of strings\n",
    "      - Tokenize with padding and truncation: tokenizer(..., return_tensors=\"pt\")\n",
    "      - Move inputs to `device`\n",
    "      - Return the tokenized inputs\n",
    "    \"\"\"\n",
    "    # 1) Replicate prompt\n",
    "    texts = [prompt] * BATCH_SIZE\n",
    "\n",
    "    # 2) Tokenize with padding & truncation\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    # 3) Move to device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d51daa",
   "metadata": {},
   "source": [
    "# üìä Profile Inference\n",
    "This function:\n",
    "- Profiles the model's inference performance using PyTorch Profiler.\n",
    "- Captures:\n",
    "  - CPU and CUDA activity.\n",
    "  - Memory usage.\n",
    "  - Operator-level performance metrics.\n",
    "- Saves the profiling traces to the specified log directory for visualization in TensorBoard.\n",
    "- Prints:\n",
    "  - Top-3 operators by CPU self-time.\n",
    "  - Top-3 operators by CUDA self-time.\n",
    "  - Total CPU and CUDA self-times in milliseconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d149f08c-bbec-4027-b990-1f68e5dfe613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_inference(model, inputs, logdir: str, label: str):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Create `logdir` if it doesn't exist\n",
    "      - Use `torch.profiler.profile` (CPU & CUDA, record_shapes, profile_memory, with_stack)\n",
    "      - Inside the profiler, wrap the call to `model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS)` in `record_function(label)`\n",
    "      - After profiling, print:\n",
    "          1) Top-3 ops by \"self_cpu_time_total\"\n",
    "          2) Top-3 ops by \"self_cuda_time_total\"\n",
    "          3) Total CPU vs CUDA self-time in milliseconds\n",
    "      - Note: use `prof.key_averages().table(...)` and sum over `evt.self_cpu_time_total`, `evt.self_cuda_time_total`\n",
    "      - Traces should be saved automatically by `tensorboard_trace_handler`\n",
    "    \"\"\"\n",
    "    # 1) Ensure log directory exists\n",
    "    os.makedirs(logdir, exist_ok=True)\n",
    "\n",
    "    # 2) Run profiler\n",
    "    with profile(\n",
    "        activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True,\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler(logdir)\n",
    "    ) as prof:\n",
    "        with record_function(label):\n",
    "            _ = model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS)\n",
    "\n",
    "    # 3) Print top‚Äê3 operators by CPU self time\n",
    "    print(f\"\\n=== {label} Top-3 ops by CPU self time ===\")\n",
    "    print(prof.key_averages().table(\n",
    "        sort_by=\"self_cpu_time_total\", row_limit=3\n",
    "    ))\n",
    "\n",
    "    # 4) Print top‚Äê3 operators by CUDA self time\n",
    "    print(f\"\\n=== {label} Top-3 ops by CUDA self time ===\")\n",
    "    print(prof.key_averages().table(\n",
    "        sort_by=\"self_cuda_time_total\", row_limit=3\n",
    "    ))\n",
    "\n",
    "    # 5) Summarize total CPU vs CUDA self times\n",
    "    events     = prof.key_averages()\n",
    "    total_cpu  = sum(evt.self_cpu_time_total for evt in events)\n",
    "    total_cuda = sum(getattr(evt, \"self_cuda_time_total\", 0) for evt in events)\n",
    "    print(f\"\\n=== {label} Total self-time ===\")\n",
    "    print(f\"CPU  : {total_cpu/1e3:.2f} ms\")\n",
    "    print(f\"CUDA : {total_cuda/1e3:.2f} ms\")\n",
    "\n",
    "    print(f\"\\nTrace files for '{label}' written to: {logdir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efd98de",
   "metadata": {},
   "source": [
    "# ‚úÇÔ∏è Apply Pruning and Quantization\n",
    "This function:\n",
    "- Applies unstructured magnitude-based pruning to all `nn.Linear` layers in the model.\n",
    "  - Prunes 30% of the smallest-magnitude weights.\n",
    "  - Makes the pruning masks permanent.\n",
    "- Dynamically quantizes the pruned model to 8-bit integers (`torch.qint8`) for efficient inference.\n",
    "- Returns the pruned and quantized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8da2b96-d032-4652-9cce-2371abb33ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pruning_and_quant(model: nn.Module):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - On CPU, apply `prune.l1_unstructured(..., amount=PRUNE_AMOUNT)` to every nn.Linear weight\n",
    "      - Call `prune.remove(...)` to make masks permanent\n",
    "      - Then apply `torch.quantization.quantize_dynamic` on {nn.Linear} with dtype=torch.qint8\n",
    "      - Return the quantized model\n",
    "    \"\"\"\n",
    "    # 1) Prune on CPU\n",
    "    model.cpu()\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # zero out PRUNE_AMOUNT fraction of the smallest‚Äêmagnitude weights\n",
    "            prune.l1_unstructured(module, name=\"weight\", amount=PRUNE_AMOUNT)\n",
    "            # make the pruning permanent\n",
    "            prune.remove(module, \"weight\")\n",
    "\n",
    "    # 2) Dynamic 8-bit quantization\n",
    "    quantized = tq.quantize_dynamic(\n",
    "        model,\n",
    "        {nn.Linear},\n",
    "        dtype=torch.qint8\n",
    "    )\n",
    "\n",
    "    return quantized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36030bf",
   "metadata": {},
   "source": [
    "# üö¶ Start the Profiling Process\n",
    "This function:\n",
    "- Loads the model and tokenizer.\n",
    "- Creates a batch of inputs using the provided prompt.\n",
    "- Profiles the baseline model's inference performance.\n",
    "- Saves the profiling results to the specified log directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7058668-777e-4a34-9cc5-c499064891e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 1) Load & benchmark baseline\n",
    "    tokenizer, model = load_model(device)\n",
    "    batch_inputs     = make_batch(tokenizer, PROMPT, device)\n",
    "    print(\"Profiling inference‚Ä¶\")\n",
    "    profile_inference(model, batch_inputs, LOGDIR_BASELINE, label=\"Baseline\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43c0542",
   "metadata": {},
   "source": [
    "# ‚ñ∂Ô∏è Run the Profiling Pipeline\n",
    "This cell:\n",
    "- Calls the `start` function to execute the profiling pipeline.\n",
    "- Outputs the profiling results, including operator-level performance metrics and total CPU/CUDA self-times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21dc4a55-3745-4443-8ace-e1a44ec90039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling inference‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2025-09-04 02:48:46 12034:12034 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2025-09-04 02:48:53 12034:12034 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2025-09-04 02:48:53 12034:12034 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Baseline Top-3 ops by CPU self time ===\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                               Baseline        35.35%        1.413s       100.00%        3.997s        3.997s       0.000us         0.00%        1.459s        1.459s           0 b      -3.52 Kb       8.13 Mb      -7.28 Gb             1  \n",
      "                                             cudaMalloc        17.82%     712.385ms        17.82%     712.385ms      29.683ms       5.000us         0.00%       5.000us       0.208us           0 b           0 b           0 b           0 b            24  \n",
      "                                       cudaLaunchKernel        12.20%     487.536ms        12.20%     487.536ms       9.324us     115.255ms         2.42%     115.255ms       2.204us           0 b           0 b           0 b           0 b         52290  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 3.997s\n",
      "Self CUDA time total: 4.754s\n",
      "\n",
      "\n",
      "=== Baseline Top-3 ops by CUDA self time ===\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                               Baseline         0.00%       0.000us         0.00%       0.000us       0.000us        3.419s        71.92%        3.419s        3.419s           0 b           0 b           0 b           0 b             1  \n",
      "                                               aten::mm         5.79%     231.281ms        15.20%     607.473ms      78.384us        1.192s        25.08%        1.224s     157.893us           0 b           0 b     634.13 Mb     634.13 Mb          7750  \n",
      "void gemmSN_TN_kernel<float, 128, 16, 2, 4, 6, 7, fa...         0.00%       0.000us         0.00%       0.000us       0.000us     556.425ms        11.71%     556.425ms     127.591us           0 b           0 b           0 b           0 b          4361  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 3.997s\n",
      "Self CUDA time total: 4.754s\n",
      "\n",
      "\n",
      "=== Baseline Total self-time ===\n",
      "CPU  : 3996.70 ms\n",
      "CUDA : 6212.76 ms\n",
      "\n",
      "Trace files for 'Baseline' written to: ./profiler_logs/baseline\n"
     ]
    }
   ],
   "source": [
    "start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba68150-f817-4e01-8771-f5cdf5992347",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
