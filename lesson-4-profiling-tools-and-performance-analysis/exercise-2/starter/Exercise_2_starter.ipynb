{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a74e1b4-9032-4e2f-8fb7-9b330e008ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.quantization as tq\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a169e0a0-b283-47a1-946c-feb84c9747cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME      = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "PRUNE_AMOUNT    = 0.30      # 30% magnitude pruning\n",
    "BATCH_SIZE      = 5\n",
    "MAX_NEW_TOKENS  = 50\n",
    "PROMPT          = (\n",
    "    \"In a world increasingly driven by artificial intelligence, the ability to interpret \"\n",
    "    \"large language models efficiently is crucial for both research and deployment.\"\n",
    ")\n",
    "LOGDIR_BASELINE = \"./profiler_logs/baseline\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf91b027-b9c6-4798-8456-db3e8578b231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(device: torch.device):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Load the tokenizer: AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "      - Load the model: AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float32)\n",
    "      - Move model to `device` and call .eval()\n",
    "      - Return (tokenizer, model)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f582dc2-289e-4119-9401-360545bdd71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(tokenizer, prompt: str, device: torch.device):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Duplicate `prompt` BATCH_SIZE times into a list of strings\n",
    "      - Tokenize with padding and truncation: tokenizer(..., return_tensors=\"pt\")\n",
    "      - Move inputs to `device`\n",
    "      - Return the tokenized inputs\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dbc9d3a-d745-44d2-9ea8-1fb8898a9b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_inference(model, inputs, logdir: str, label: str):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Create `logdir` if it doesn't exist\n",
    "      - Use `torch.profiler.profile` (CPU & CUDA, record_shapes, profile_memory, with_stack)\n",
    "      - Inside the profiler, wrap the call to `model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS)` in `record_function(label)`\n",
    "      - After profiling, print:\n",
    "          1) Top-3 ops by \"self_cpu_time_total\"\n",
    "          2) Top-3 ops by \"self_cuda_time_total\"\n",
    "          3) Total CPU vs CUDA self-time in milliseconds\n",
    "      - Note: use `prof.key_averages().table(...)` and sum over `evt.self_cpu_time_total`, `evt.self_cuda_time_total`\n",
    "      - Traces should be saved automatically by `tensorboard_trace_handler`\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d03ed63-c193-4642-929c-44c1513f74a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pruning_and_quant(model: nn.Module):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - On CPU, apply `prune.l1_unstructured(..., amount=PRUNE_AMOUNT)` to every nn.Linear weight\n",
    "      - Call `prune.remove(...)` to make masks permanent\n",
    "      - Then apply `torch.quantization.quantize_dynamic` on {nn.Linear} with dtype=torch.qint8\n",
    "      - Return the quantized model\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfab1f80-7ec2-4242-87d0-ecadecfcc11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 1) Load & benchmark baseline\n",
    "    tokenizer, model = load_model(device)\n",
    "    batch_inputs     = make_batch(tokenizer, PROMPT, device)\n",
    "    print(\"Profiling inferenceâ€¦\")\n",
    "    profile_inference(model, batch_inputs, LOGDIR_BASELINE, label=\"Baseline\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "de79484d-726f-4fad-a01a-595fc53b0b51",
   "metadata": {},
   "source": [
    "start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c054ed-be9e-4699-909e-2fa4b1c0a658",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
