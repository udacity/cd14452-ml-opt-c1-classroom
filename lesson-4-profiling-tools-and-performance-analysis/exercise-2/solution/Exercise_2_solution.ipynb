{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e67f2abd-0fa2-4927-a7f4-a5e7e4e41a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.quantization as tq\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dbd0e98-eece-4a97-a9f5-ec03519ca90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME      = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "PRUNE_AMOUNT    = 0.30      # 30% magnitude pruning\n",
    "BATCH_SIZE      = 5\n",
    "MAX_NEW_TOKENS  = 50\n",
    "PROMPT          = (\n",
    "    \"In a world increasingly driven by artificial intelligence, the ability to interpret \"\n",
    "    \"large language models efficiently is crucial for both research and deployment.\"\n",
    ")\n",
    "LOGDIR_BASELINE = \"./profiler_logs/baseline\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d99d9f3-4584-4652-9abc-df351833ea1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(device: torch.device):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Load the tokenizer: AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "      - Load the model: AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float32)\n",
    "      - Move model to `device` and call .eval()\n",
    "      - Return (tokenizer, model)\n",
    "    \"\"\"\n",
    "    # 1) Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "    # 2) Load FP32 model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float32\n",
    "    )\n",
    "\n",
    "    # 3) Move to device and set to eval mode\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # 4) Return both\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcaecfae-66b7-46bf-9d62-213ecfbfe153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(tokenizer, prompt: str, device: torch.device):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Duplicate `prompt` BATCH_SIZE times into a list of strings\n",
    "      - Tokenize with padding and truncation: tokenizer(..., return_tensors=\"pt\")\n",
    "      - Move inputs to `device`\n",
    "      - Return the tokenized inputs\n",
    "    \"\"\"\n",
    "    # 1) Replicate prompt\n",
    "    texts = [prompt] * BATCH_SIZE\n",
    "\n",
    "    # 2) Tokenize with padding & truncation\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    # 3) Move to device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d149f08c-bbec-4027-b990-1f68e5dfe613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_inference(model, inputs, logdir: str, label: str):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Create `logdir` if it doesn't exist\n",
    "      - Use `torch.profiler.profile` (CPU & CUDA, record_shapes, profile_memory, with_stack)\n",
    "      - Inside the profiler, wrap the call to `model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS)` in `record_function(label)`\n",
    "      - After profiling, print:\n",
    "          1) Top-3 ops by \"self_cpu_time_total\"\n",
    "          2) Top-3 ops by \"self_cuda_time_total\"\n",
    "          3) Total CPU vs CUDA self-time in milliseconds\n",
    "      - Note: use `prof.key_averages().table(...)` and sum over `evt.self_cpu_time_total`, `evt.self_cuda_time_total`\n",
    "      - Traces should be saved automatically by `tensorboard_trace_handler`\n",
    "    \"\"\"\n",
    "    # 1) Ensure log directory exists\n",
    "    os.makedirs(logdir, exist_ok=True)\n",
    "\n",
    "    # 2) Run profiler\n",
    "    with profile(\n",
    "        activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True,\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler(logdir)\n",
    "    ) as prof:\n",
    "        with record_function(label):\n",
    "            _ = model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS)\n",
    "\n",
    "    # 3) Print top‐3 operators by CPU self time\n",
    "    print(f\"\\n=== {label} Top-3 ops by CPU self time ===\")\n",
    "    print(prof.key_averages().table(\n",
    "        sort_by=\"self_cpu_time_total\", row_limit=3\n",
    "    ))\n",
    "\n",
    "    # 4) Print top‐3 operators by CUDA self time\n",
    "    print(f\"\\n=== {label} Top-3 ops by CUDA self time ===\")\n",
    "    print(prof.key_averages().table(\n",
    "        sort_by=\"self_cuda_time_total\", row_limit=3\n",
    "    ))\n",
    "\n",
    "    # 5) Summarize total CPU vs CUDA self times\n",
    "    events     = prof.key_averages()\n",
    "    total_cpu  = sum(evt.self_cpu_time_total for evt in events)\n",
    "    total_cuda = sum(getattr(evt, \"self_cuda_time_total\", 0) for evt in events)\n",
    "    print(f\"\\n=== {label} Total self-time ===\")\n",
    "    print(f\"CPU  : {total_cpu/1e3:.2f} ms\")\n",
    "    print(f\"CUDA : {total_cuda/1e3:.2f} ms\")\n",
    "\n",
    "    print(f\"\\nTrace files for '{label}' written to: {logdir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8da2b96-d032-4652-9cce-2371abb33ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pruning_and_quant(model: nn.Module):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - On CPU, apply `prune.l1_unstructured(..., amount=PRUNE_AMOUNT)` to every nn.Linear weight\n",
    "      - Call `prune.remove(...)` to make masks permanent\n",
    "      - Then apply `torch.quantization.quantize_dynamic` on {nn.Linear} with dtype=torch.qint8\n",
    "      - Return the quantized model\n",
    "    \"\"\"\n",
    "    # 1) Prune on CPU\n",
    "    model.cpu()\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # zero out PRUNE_AMOUNT fraction of the smallest‐magnitude weights\n",
    "            prune.l1_unstructured(module, name=\"weight\", amount=PRUNE_AMOUNT)\n",
    "            # make the pruning permanent\n",
    "            prune.remove(module, \"weight\")\n",
    "\n",
    "    # 2) Dynamic 8-bit quantization\n",
    "    quantized = tq.quantize_dynamic(\n",
    "        model,\n",
    "        {nn.Linear},\n",
    "        dtype=torch.qint8\n",
    "    )\n",
    "\n",
    "    return quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7058668-777e-4a34-9cc5-c499064891e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 1) Load & benchmark baseline\n",
    "    tokenizer, model = load_model(device)\n",
    "    batch_inputs     = make_batch(tokenizer, PROMPT, device)\n",
    "    print(\"Profiling inference…\")\n",
    "    profile_inference(model, batch_inputs, LOGDIR_BASELINE, label=\"Baseline\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21dc4a55-3745-4443-8ace-e1a44ec90039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling baseline inference…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Baseline Top-3 ops by CPU self time ===\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                               Baseline        29.68%     688.039ms       100.00%        2.318s        2.318s       0.000us         0.00%        1.381s        1.381s           0 b        -328 b       8.13 Mb      -6.87 Gb             1  \n",
      "                                       cudaLaunchKernel        13.68%     317.106ms        20.96%     485.789ms      10.293us       0.000us         0.00%       1.169ms       0.025us           0 b           0 b           0 b           0 b         47194  \n",
      "                       Runtime Triggered Module Loading         9.52%     220.771ms         9.52%     220.771ms       3.450ms       4.436ms         0.32%       4.436ms      69.307us           0 b           0 b           0 b           0 b            64  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.318s\n",
      "Self CUDA time total: 1.374s\n",
      "\n",
      "\n",
      "=== Baseline Top-3 ops by CUDA self time ===\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                               Baseline         0.00%       0.000us         0.00%       0.000us       0.000us        2.316s       168.59%        2.316s        2.316s           0 b           0 b           0 b           0 b             1  \n",
      "                                               aten::mm         7.56%     175.166ms        13.73%     318.331ms      41.075us        1.213s        88.28%        1.220s     157.355us           0 b           0 b     610.46 Mb     610.46 Mb          7750  \n",
      "void gemmSN_TN_kernel<float, 128, 16, 2, 4, 6, 7, fa...         0.00%       0.000us         0.00%       0.000us       0.000us     560.082ms        40.77%     560.082ms     128.400us           0 b           0 b           0 b           0 b          4362  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.318s\n",
      "Self CUDA time total: 1.374s\n",
      "\n",
      "\n",
      "=== Baseline Total self-time ===\n",
      "CPU  : 2318.03 ms\n",
      "CUDA : 0.00 ms\n",
      "\n",
      "Trace files for 'Baseline' written to: ./profiler_logs/baseline\n"
     ]
    }
   ],
   "source": [
    "start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba68150-f817-4e01-8771-f5cdf5992347",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
