{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e67f2abd-0fa2-4927-a7f4-a5e7e4e41a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.quantization as tq\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dbd0e98-eece-4a97-a9f5-ec03519ca90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME      = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "PRUNE_AMOUNT    = 0.30      # 30% magnitude pruning\n",
    "BATCH_SIZE      = 5\n",
    "MAX_NEW_TOKENS  = 50\n",
    "PROMPT          = (\n",
    "    \"In a world increasingly driven by artificial intelligence, the ability to interpret \"\n",
    "    \"large language models efficiently is crucial for both research and deployment.\"\n",
    ")\n",
    "LOGDIR_BASELINE = \"./profiler_logs/baseline\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d99d9f3-4584-4652-9abc-df351833ea1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(device: torch.device):\n",
    "    \"\"\"\n",
    "      - Load the tokenizer: AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "      - Load the model: AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float32)\n",
    "      - Move model to `device` and call .eval()\n",
    "      - Return (tokenizer, model)\n",
    "    \"\"\"\n",
    "    # 1) Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "    # 2) Load FP32 model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float32\n",
    "    )\n",
    "\n",
    "    # 3) Move to device and set to eval mode\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # 4) Return both\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcaecfae-66b7-46bf-9d62-213ecfbfe153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(tokenizer, prompt: str, device: torch.device):\n",
    "    \"\"\"\n",
    "      - Duplicate `prompt` BATCH_SIZE times into a list of strings\n",
    "      - Tokenize with padding and truncation: tokenizer(..., return_tensors=\"pt\")\n",
    "      - Move inputs to `device`\n",
    "      - Return the tokenized inputs\n",
    "    \"\"\"\n",
    "    # 1) Replicate prompt\n",
    "    texts = [prompt] * BATCH_SIZE\n",
    "\n",
    "    # 2) Tokenize with padding & truncation\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    # 3) Move to device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d149f08c-bbec-4027-b990-1f68e5dfe613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_inference(model, inputs, logdir: str, label: str):\n",
    "    \"\"\"\n",
    "      - Create `logdir` if it doesn't exist\n",
    "      - Use `torch.profiler.profile` (CPU & CUDA, record_shapes, profile_memory, with_stack)\n",
    "      - Inside the profiler, wrap the call to `model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS)` in `record_function(label)`\n",
    "      - After profiling, print:\n",
    "          1) Top-3 ops by \"self_cpu_time_total\"\n",
    "          2) Top-3 ops by \"self_cuda_time_total\"\n",
    "          3) Total CPU vs CUDA self-time in milliseconds\n",
    "      - Note: use `prof.key_averages().table(...)` and sum over `evt.self_cpu_time_total`, `evt.self_cuda_time_total`\n",
    "      - Traces should be saved automatically by `tensorboard_trace_handler`\n",
    "    \"\"\"\n",
    "    # 1) Ensure log directory exists\n",
    "    os.makedirs(logdir, exist_ok=True)\n",
    "\n",
    "    # 2) Run profiler\n",
    "    with profile(\n",
    "        activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True,\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler(logdir)\n",
    "    ) as prof:\n",
    "        with record_function(label):\n",
    "            _ = model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS)\n",
    "\n",
    "    # 3) Print top‐3 operators by CPU self time\n",
    "    print(f\"\\n=== {label} Top-3 ops by CPU self time ===\")\n",
    "    print(prof.key_averages().table(\n",
    "        sort_by=\"self_cpu_time_total\", row_limit=3\n",
    "    ))\n",
    "\n",
    "    # 4) Print top‐3 operators by CUDA self time\n",
    "    print(f\"\\n=== {label} Top-3 ops by CUDA self time ===\")\n",
    "    print(prof.key_averages().table(\n",
    "        sort_by=\"self_cuda_time_total\", row_limit=3\n",
    "    ))\n",
    "\n",
    "    # 5) Summarize total CPU vs CUDA self times\n",
    "    events     = prof.key_averages()\n",
    "    total_cpu  = sum(evt.self_cpu_time_total for evt in events)\n",
    "    total_cuda = sum(getattr(evt, \"self_cuda_time_total\", 0) for evt in events)\n",
    "    print(f\"\\n=== {label} Total self-time ===\")\n",
    "    print(f\"CPU  : {total_cpu/1e3:.2f} ms\")\n",
    "    print(f\"CUDA : {total_cuda/1e3:.2f} ms\")\n",
    "\n",
    "    print(f\"\\nTrace files for '{label}' written to: {logdir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8da2b96-d032-4652-9cce-2371abb33ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pruning_and_quant(model: nn.Module):\n",
    "    \"\"\"\n",
    "      - On CPU, apply `prune.l1_unstructured(..., amount=PRUNE_AMOUNT)` to every nn.Linear weight\n",
    "      - Call `prune.remove(...)` to make masks permanent\n",
    "      - Then apply `torch.quantization.quantize_dynamic` on {nn.Linear} with dtype=torch.qint8\n",
    "      - Return the quantized model\n",
    "    \"\"\"\n",
    "    # 1) Prune on CPU\n",
    "    model.cpu()\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # zero out PRUNE_AMOUNT fraction of the smallest‐magnitude weights\n",
    "            prune.l1_unstructured(module, name=\"weight\", amount=PRUNE_AMOUNT)\n",
    "            # make the pruning permanent\n",
    "            prune.remove(module, \"weight\")\n",
    "\n",
    "    # 2) Dynamic 8-bit quantization\n",
    "    quantized = tq.quantize_dynamic(\n",
    "        model,\n",
    "        {nn.Linear},\n",
    "        dtype=torch.qint8\n",
    "    )\n",
    "\n",
    "    return quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7058668-777e-4a34-9cc5-c499064891e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 1) Load & benchmark baseline\n",
    "    tokenizer, model = load_model(device)\n",
    "    batch_inputs     = make_batch(tokenizer, PROMPT, device)\n",
    "    print(\"Profiling inference…\")\n",
    "    profile_inference(model, batch_inputs, LOGDIR_BASELINE, label=\"Baseline\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21dc4a55-3745-4443-8ace-e1a44ec90039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling inference…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2025-09-06 02:57:50 10150:10150 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2025-09-06 02:57:57 10150:10150 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2025-09-06 02:57:57 10150:10150 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Baseline Top-3 ops by CPU self time ===\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                               Baseline        36.95%        1.494s       100.00%        4.043s        4.043s       0.000us         0.00%        1.458s        1.458s           0 b      -3.37 Kb       8.13 Mb      -7.29 Gb             1  \n",
      "                                             cudaMalloc        14.63%     591.609ms        14.63%     591.609ms      24.650ms       5.000us         0.00%       5.000us       0.208us           0 b           0 b           0 b           0 b            24  \n",
      "                                       cudaLaunchKernel        13.53%     546.859ms        13.53%     546.859ms      10.458us     115.198ms         2.42%     115.198ms       2.203us           0 b           0 b           0 b           0 b         52290  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 4.043s\n",
      "Self CUDA time total: 4.769s\n",
      "\n",
      "\n",
      "=== Baseline Top-3 ops by CUDA self time ===\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                               Baseline         0.00%       0.000us         0.00%       0.000us       0.000us        3.435s        72.03%        3.435s        3.435s           0 b           0 b           0 b           0 b             1  \n",
      "                                               aten::mm         5.53%     223.522ms        12.84%     519.233ms      66.998us        1.191s        24.98%        1.223s     157.770us           0 b           0 b     634.13 Mb     634.13 Mb          7750  \n",
      "void gemmSN_TN_kernel<float, 128, 16, 2, 4, 6, 7, fa...         0.00%       0.000us         0.00%       0.000us       0.000us     556.512ms        11.67%     556.512ms     127.611us           0 b           0 b           0 b           0 b          4361  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 4.043s\n",
      "Self CUDA time total: 4.769s\n",
      "\n",
      "\n",
      "=== Baseline Total self-time ===\n",
      "CPU  : 4042.71 ms\n",
      "CUDA : 6226.79 ms\n",
      "\n",
      "Trace files for 'Baseline' written to: ./profiler_logs/baseline\n"
     ]
    }
   ],
   "source": [
    "start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9764bf-4e25-4c1a-b09e-b9c7b340ab05",
   "metadata": {},
   "source": [
    "# Exercise 1: Measure Model Size & Nonzeros\n",
    "\n",
    "## Concept: After pruning and quantization, the student should measure how the model structure has changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e8bb030-e095-46b8-9c53-c2cbb9cf32a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, torch\n",
    "import numpy as np\n",
    "\n",
    "def _safe_state_dict(model):\n",
    "    \"\"\"\n",
    "    Return a CPU state_dict with ONLY tensor values.\n",
    "    Filters out dtype/None/custom objects that can break saving.\n",
    "    \"\"\"\n",
    "    sd = model.state_dict()\n",
    "    safe = {}\n",
    "    for k, v in sd.items():\n",
    "        if torch.is_tensor(v):\n",
    "            safe[k] = v.detach().cpu()\n",
    "        # If libraries stash buffers as numpy arrays, you can keep them too:\n",
    "        elif isinstance(v, np.ndarray):\n",
    "            safe[k] = torch.from_numpy(v)\n",
    "        # else: drop non-tensor entries (e.g., dtype objects)\n",
    "    return safe\n",
    "\n",
    "def _dir_size_mb(path: str) -> float:\n",
    "    total = 0\n",
    "    for root, _, files in os.walk(path):\n",
    "        for f in files:\n",
    "            total += os.path.getsize(os.path.join(root, f))\n",
    "    return total / (1024 ** 2)\n",
    "\n",
    "def measure_model_sparsity_and_size(model, name: str = \"model\"):\n",
    "    \"\"\"\n",
    "    Robust version:\n",
    "      - Counts sparsity for ANY submodule that exposes a tensor 'weight'\n",
    "      - Saves either with HF .save_pretrained() (try) or a filtered state_dict (fallback)\n",
    "      - Prints % sparsity + on-disk size (MB)\n",
    "    \"\"\"\n",
    "    total, nonzero = 0, 0\n",
    "\n",
    "    for mod in model.modules():\n",
    "        # count ONLY if there is a real tensor weight\n",
    "        W = getattr(mod, \"weight\", None)\n",
    "        if torch.is_tensor(W):\n",
    "            w = W.detach()\n",
    "            total += w.numel()\n",
    "            nonzero += (w != 0).sum().item()\n",
    "\n",
    "    if total > 0:\n",
    "        sparsity_pct = 100.0 * (1.0 - (nonzero / total))\n",
    "        print(f\"[{name}] Nonzero params: {nonzero}/{total} ({sparsity_pct:.2f}% sparse)\")\n",
    "    else:\n",
    "        print(f\"[{name}] Could not find tensor weights to count (quantized/packed?).\")\n",
    "\n",
    "    # save into a temporary folder\n",
    "    tmp_dir = f\"./tmp_{name}\"\n",
    "    if os.path.exists(tmp_dir):\n",
    "        shutil.rmtree(tmp_dir)\n",
    "    os.makedirs(tmp_dir, exist_ok=True)\n",
    "\n",
    "    size_mb = None\n",
    "    # Try the native HF save first, but guard against bad state entries\n",
    "    try:\n",
    "        if hasattr(model, \"save_pretrained\"):\n",
    "            model.save_pretrained(tmp_dir, safe_serialization=True)\n",
    "        else:\n",
    "            raise AttributeError(\"No save_pretrained; using filtered state_dict fallback.\")\n",
    "    except Exception as e:\n",
    "        # Fallback: filtered, CPU-only tensors\n",
    "        safe_sd = _safe_state_dict(model)\n",
    "        torch.save(safe_sd, os.path.join(tmp_dir, \"model.pt\"))\n",
    "\n",
    "    size_mb = _dir_size_mb(tmp_dir)\n",
    "    print(f\"[{name}] Disk size: {size_mb:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2075985-cde7-4f31-856d-15a7f8f12c1b",
   "metadata": {},
   "source": [
    "# Exercise 2 — Batch size & prompt-length sensitivity (latency/throughput)\n",
    "\n",
    "## Concept: Show how decoding cost scales with batch size and max_new_tokens on your baseline vs optimized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0646ffad-41a1-476f-aa61-e3ed59f0cfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, torch\n",
    "import pandas as pd\n",
    "\n",
    "def _model_device(model: torch.nn.Module) -> torch.device:\n",
    "    for p in model.parameters():\n",
    "        return p.device\n",
    "    for b in model.buffers():\n",
    "        return b.device\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "def sweep_batch_and_length(tokenizer, model_dict, _unused_device,\n",
    "                           prompt, batch_sizes=(1, 2, 4),\n",
    "                           gen_lengths=(16, 64, 128),\n",
    "                           runs=3):\n",
    "    \"\"\"\n",
    "    Device-safe sweep:\n",
    "      - Tokenize once on CPU for a SINGLE sample.\n",
    "      - For each model, move to model's device and tile to the requested batch size.\n",
    "      - Time .generate() for each (B, L), with CUDA sync around timing.\n",
    "      - Return tidy DataFrame.\n",
    "    \"\"\"\n",
    "    # 1) Tokenize ONCE as a single sample\n",
    "    tok_cpu = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids_1 = tok_cpu[\"input_ids\"][:1]            # ensure shape [1, T]\n",
    "    attn_mask_1 = tok_cpu.get(\"attention_mask\", None)\n",
    "    rows = []\n",
    "\n",
    "    for name, mdl in model_dict.items():\n",
    "        mdev = _model_device(mdl)\n",
    "\n",
    "        for B in batch_sizes:\n",
    "            # 2) Tile to B on CPU, then move to model device\n",
    "            input_ids = input_ids_1.expand(B, -1).to(mdev)\n",
    "            if attn_mask_1 is not None:\n",
    "                attn_mask = attn_mask_1.expand(B, -1).to(mdev)\n",
    "                enc = {\"input_ids\": input_ids, \"attention_mask\": attn_mask}\n",
    "            else:\n",
    "                enc = {\"input_ids\": input_ids}\n",
    "\n",
    "            for L in gen_lengths:\n",
    "                latencies, tputs = [], []\n",
    "                for _ in range(runs):\n",
    "                    if mdev.type == \"cuda\":\n",
    "                        torch.cuda.synchronize()\n",
    "                    t0 = time.perf_counter()\n",
    "                    with torch.inference_mode():\n",
    "                        out = mdl.generate(**enc, max_new_tokens=L, use_cache=True)\n",
    "                    if mdev.type == \"cuda\":\n",
    "                        torch.cuda.synchronize()\n",
    "                    t1 = time.perf_counter()\n",
    "\n",
    "                    gen_len = out.shape[1] - enc[\"input_ids\"].shape[1]\n",
    "                    lat = t1 - t0\n",
    "                    latencies.append(lat)\n",
    "                    # tokens/sec across the WHOLE batch\n",
    "                    tputs.append((gen_len * B) / max(lat, 1e-9))\n",
    "\n",
    "                rows.append({\n",
    "                    \"model\": name,\n",
    "                    \"batch_size\": B,\n",
    "                    \"max_new_tokens\": L,\n",
    "                    \"latency_s\": sum(latencies)/len(latencies),\n",
    "                    \"tokens_per_sec\": sum(tputs)/len(tputs),\n",
    "                    \"device\": str(mdev),\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d57909-4f55-42e6-a4b1-7aa959b242a9",
   "metadata": {},
   "source": [
    "# Exercise 3 — Operator hot-spots before vs after optimization\n",
    "\n",
    "## Concept: Use torch.profiler to summarize top operators for baseline vs optimized. Students see where time is spent (e.g., attention matmuls vs layernorm) and how it shifts after pruning/quant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16d82aa4-7586-4b01-b7e7-e782ce2ec741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Robust operator hot-spot summary (event-based, device-safe) ---\n",
    "from torch.profiler import profile, ProfilerActivity\n",
    "import torch\n",
    "\n",
    "def _model_device(model: torch.nn.Module) -> torch.device:\n",
    "    for p in model.parameters():\n",
    "        return p.device\n",
    "    for b in model.buffers():\n",
    "        return b.device\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "def _to_device_batch(batch: dict, device: torch.device) -> dict:\n",
    "    moved = {}\n",
    "    for k, v in batch.items():\n",
    "        moved[k] = v.to(device) if isinstance(v, torch.Tensor) else v\n",
    "    return moved\n",
    "\n",
    "def top_ops_summary(model, batch_inputs, device_unused, with_cuda=True, top_k=10, label=\"run\"):\n",
    "    \"\"\"\n",
    "    Uses profiler event API for reliable CPU/CUDA self times.\n",
    "    Returns: {\"label\", \"device\", \"cpu\": [(op, ms), ...], \"cuda\": [(op, ms), ...], \"counts\": {...}}\n",
    "    \"\"\"\n",
    "    mdev = _model_device(model)\n",
    "    enc  = _to_device_batch(batch_inputs, mdev)\n",
    "\n",
    "    acts = [ProfilerActivity.CPU]\n",
    "    if with_cuda and mdev.type == \"cuda\":\n",
    "        acts.append(ProfilerActivity.CUDA)\n",
    "\n",
    "    # Collect a single short run\n",
    "    with profile(activities=acts, record_shapes=False, profile_memory=False) as prof:\n",
    "        with torch.inference_mode():\n",
    "            if mdev.type == \"cuda\":\n",
    "                torch.cuda.synchronize()\n",
    "            _ = model.generate(**enc, max_new_tokens=24, use_cache=True)  # bump tokens a bit to get more ops\n",
    "            if mdev.type == \"cuda\":\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "    evts = prof.key_averages(group_by_input_shape=False)\n",
    "\n",
    "    # Build CPU top list\n",
    "    cpu_items = []\n",
    "    for e in evts:\n",
    "        cpu_ms = getattr(e, \"self_cpu_time_total\", 0.0) / 1000.0  # us -> ms\n",
    "        if cpu_ms > 0:\n",
    "            cpu_items.append((e.key, cpu_ms))\n",
    "    cpu_items.sort(key=lambda x: x[1], reverse=True)\n",
    "    cpu_items = cpu_items[:top_k]\n",
    "\n",
    "    # Build CUDA top list (if applicable)\n",
    "    cuda_items = []\n",
    "    if mdev.type == \"cuda\":\n",
    "        for e in evts:\n",
    "            cuda_ms = getattr(e, \"self_cuda_time_total\", 0.0) / 1000.0\n",
    "            if cuda_ms > 0:\n",
    "                cuda_items.append((e.key, cuda_ms))\n",
    "        cuda_items.sort(key=lambda x: x[1], reverse=True)\n",
    "        cuda_items = cuda_items[:top_k]\n",
    "\n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"device\": str(mdev),\n",
    "        \"cpu\": cpu_items,\n",
    "        \"cuda\": cuda_items,\n",
    "        \"counts\": {\n",
    "            \"evt_total\": len(evts),\n",
    "            \"cpu_nonzero\": sum(1 for _, ms in cpu_items if ms > 0),\n",
    "            \"cuda_nonzero\": sum(1 for _, ms in cuda_items if ms > 0),\n",
    "        },\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c645b39f-ac58-4bb6-b72b-2019a9e1f5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ Exercise 1: Sparsity & Size ================\n",
      "[baseline] Nonzero params: 789722330/1100048384 (28.21% sparse)\n",
      "[baseline] Disk size: 4196.37 MB\n",
      "[optimized] Nonzero params: 65628160/65628160 (0.00% sparse)\n",
      "[optimized] Disk size: 250.45 MB\n",
      "\n",
      "================ Exercise 2: Batch Size & Length Sweep ================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>max_new_tokens</th>\n",
       "      <th>latency_s</th>\n",
       "      <th>tokens_per_sec</th>\n",
       "      <th>device</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>1.564530</td>\n",
       "      <td>10.228149</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>baseline</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>5.054521</td>\n",
       "      <td>12.667585</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>baseline</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>1.840176</td>\n",
       "      <td>17.395002</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>baseline</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>6.051661</td>\n",
       "      <td>21.179314</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>optimized</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0.679012</td>\n",
       "      <td>23.564206</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>optimized</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>2.692659</td>\n",
       "      <td>23.768656</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>optimized</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>0.767073</td>\n",
       "      <td>41.722865</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>optimized</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>2.942899</td>\n",
       "      <td>43.494537</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model  batch_size  max_new_tokens  latency_s  tokens_per_sec device\n",
       "0   baseline           1              16   1.564530       10.228149    cpu\n",
       "1   baseline           1              64   5.054521       12.667585    cpu\n",
       "2   baseline           2              16   1.840176       17.395002    cpu\n",
       "3   baseline           2              64   6.051661       21.179314    cpu\n",
       "4  optimized           1              16   0.679012       23.564206    cpu\n",
       "5  optimized           1              64   2.692659       23.768656    cpu\n",
       "6  optimized           2              16   0.767073       41.722865    cpu\n",
       "7  optimized           2              64   2.942899       43.494537    cpu"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ Exercise 3: Operator Hot-Spots ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2025-09-06 03:25:30 10150:10150 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2025-09-06 03:25:35 10150:10150 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2025-09-06 03:25:35 10150:10150 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n",
      "STAGE:2025-09-06 03:25:45 10150:10150 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2025-09-06 03:25:47 10150:10150 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2025-09-06 03:25:47 10150:10150 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline device: cpu | events: {'evt_total': 74, 'cpu_nonzero': 8, 'cuda_nonzero': 0}\n",
      "Optimized device: cpu | events: {'evt_total': 72, 'cpu_nonzero': 8, 'cuda_nonzero': 0}\n",
      "\n",
      "Top CPU ops (baseline): [('aten::mm', 3373.148), ('aten::mul', 69.753), ('aten::cat', 56.771), ('aten::_scaled_dot_product_flash_attention_for_cpu', 52.607), ('aten::copy_', 49.95), ('aten::add', 37.566), ('aten::matmul', 37.352), ('aten::silu', 30.704)]\n",
      "Top CPU ops (optimized): [('quantized::linear_dynamic', 601.199), ('aten::cat', 49.253), ('aten::mul', 45.053), ('aten::_scaled_dot_product_flash_attention_for_cpu', 45.007), ('aten::copy_', 34.219), ('aten::add', 25.002), ('aten::empty', 21.191), ('aten::silu', 20.13)]\n"
     ]
    }
   ],
   "source": [
    "# === Run All Exercises ===\n",
    "def run_all_exercises():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 1) Load baseline model & tokenizer\n",
    "    tokenizer, baseline_model = load_model(device)\n",
    "    batch_inputs = make_batch(tokenizer, PROMPT, device)\n",
    "\n",
    "    # 2) Create optimized model (prune + quantize)\n",
    "    optimized_model = apply_pruning_and_quant(baseline_model)\n",
    "\n",
    "    print(\"\\n================ Exercise 1: Sparsity & Size ================\")\n",
    "    measure_model_sparsity_and_size(baseline_model, \"baseline\")\n",
    "    measure_model_sparsity_and_size(optimized_model, \"optimized\")\n",
    "\n",
    "    print(\"\\n================ Exercise 2: Batch Size & Length Sweep ================\")\n",
    "    df_sweep = sweep_batch_and_length(\n",
    "        tokenizer,\n",
    "        {\"baseline\": baseline_model, \"optimized\": optimized_model},\n",
    "        device,\n",
    "        PROMPT,\n",
    "        batch_sizes=(1, 2),\n",
    "        gen_lengths=(16, 64),\n",
    "        runs=2\n",
    "    )\n",
    "    display(df_sweep)\n",
    "\n",
    "    print(\"\\n================ Exercise 3: Operator Hot-Spots ================\")\n",
    "    base_ops = top_ops_summary(baseline_model, batch_inputs, device, with_cuda=True, top_k=8, label=\"baseline\")\n",
    "    opt_ops  = top_ops_summary(optimized_model, batch_inputs, device, with_cuda=True, top_k=8, label=\"optimized\")\n",
    "    \n",
    "    print(\"Baseline device:\", base_ops[\"device\"], \"| events:\", base_ops[\"counts\"])\n",
    "    print(\"Optimized device:\", opt_ops[\"device\"], \"| events:\", opt_ops[\"counts\"])\n",
    "    print(\"\\nTop CPU ops (baseline):\", base_ops[\"cpu\"])\n",
    "    print(\"Top CPU ops (optimized):\", opt_ops[\"cpu\"])\n",
    "    \n",
    "# Call it\n",
    "run_all_exercises()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035443d5-b3c0-4705-b165-edf95c1920dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
