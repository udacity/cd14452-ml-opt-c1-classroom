# âœ… Project Instructions: GPT-2 Model Optimization & Deployment

## ğŸ§  Project Context

In this project, you will act as an ML Engineer responsible for optimizing a GPT-2 language model for efficient inference in a resource-constrained environment. You will apply techniques like attention head pruning, fine-tuning, INT8 post-training quantization, and performance analysis to create a lightweight version of GPT-2 that can run on a CPU with limited memory. This project mimics real-world tasks performed by AI/ML engineers at companies deploying large models on edge devices or cloud services.

---

## ğŸ—‚ï¸ Files & Setup

Work for this project is completed in the provided `project.ipynb` Jupyter notebook. The notebook includes all the code scaffolding, explanations, and clearly marked `TODO` sections where you will implement your logic.

### What you will need:
- GPU: for pruning + fine-tuning (FP16)
- CPU: for quantization + evaluation
- Tokenizer and model: `gpt2`
- Required packages: already pre-installed in your Udacity workspace

---

## ğŸ”§ Development Strategy

We recommend completing the project in four structured phases:

---

### ğŸ“ Part 1: Load and Profile the Base GPT-2 Model (FP16)

- âœ… Load the `gpt2` model using Hugging Face.
- âœ… Convert it to FP16.
- âœ… Profile baseline latency using `torch.profiler`.
- âœ… Measure perplexity and VRAM usage.

ğŸ’¡ This step helps you understand how the original model performs before any optimization.

---

### ğŸ“ Part 2: Prune Attention Heads

- âœ… Implement the `prune_attention_heads()` function.
- âœ… Prune 20% of heads per transformer layer.
- âœ… Recast the model to FP16 after pruning.
- âœ… Measure and compare latency, perplexity, and GPU VRAM again.

ğŸ’¡ This simulates structured model compression by reducing compute-heavy components.

---

### ğŸ“ Part 3: Fine-tune the Pruned Model

- âœ… Use a small custom dataset (or a sample from Hugging Face).
- âœ… Prepare your tokenizer with proper padding settings.
- âœ… Tokenize your dataset using `tokenize_dataset()`.
- âœ… Set up `TrainingArguments` and use Hugging Face's `Trainer`.
- âœ… Fine-tune for 1â€“5 epochs.

ğŸ’¡ Fine-tuning helps the model recover performance lost from pruning.

---

### ğŸ“ Part 4: Post-Training Quantization (INT8 CPU)

- âœ… Use `INCQuantizer` to quantize the fine-tuned model.
- âœ… Filter and tokenize the calibration dataset.
- âœ… Export the model in OpenVINO IR format.
- âœ… Measure performance on CPU:
  - Inference latency
  - RAM usage
  - Perplexity

ğŸ’¡ This step prepares the model for low-latency CPU-only inference using INT8 precision.

---

### âš ï¸ Hardware reality check (T4 GPU users)

If youâ€™re working in an environment with an NVIDIA T4 GPU, donâ€™t expect the INT8-quantized model (CPU, OpenVINO) to be faster than the FP16/FP32 model running on the T4 GPU. INT8 post-training quantization in this project targets CPU-only deployment and efficiency.
For fair comparisons, compare performance within the same device:

- Parts 1â€“3: GPU baselines (FP16/FP32 on T4)

- Part 4: CPU baselines (FP32/FP16 on CPU) vs INT8 on CPU

### ğŸ” Why INT8 on CPU can be slower than FP16/FP32 on a T4 GPU

- Different devices, different strengths. A T4 has thousands of CUDA cores and Tensor Cores optimized for large matrix multiplies and high memory bandwidth. A typical CPU has far fewer vector lanes and much lower memory bandwidth, so even with INT8 it canâ€™t match GPU parallelism.

- Specialized GPU hardware. T4 Tensor Cores accelerate FP16 (and INT8 on GPU). In this project, INT8 runs on CPU via OpenVINOâ€”so youâ€™re not tapping the T4â€™s Tensor Cores at all, while the FP16/FP32 baselines do use the GPU.

- Operator coverage & de/req quant overhead. Not every op/layer is quantized. Frameworks insert dequantize â†’ (float op) â†’ requantize steps where needed, which adds latency and erodes INT8 gains.

- Autoregressive decoding is small-batch & memory-bound. GPT-2 generates token-by-token (batchâ‰ˆ1). That limits CPU vector utilization, and the KV cache reads/writes make the workload memory-boundâ€”bandwidth where GPUs excel.

- CPU ISA matters. Big INT8 gains rely on AVX512-VNNI/AMX. On CPUs without these, performance uplift over FP32 can be modest.

- Graph breaks & dynamic shapes. Varying sequence lengths and dynamic control flow can prevent kernel fusion, increasing framework overhead on CPU.

Bottom line: INT8 quantization here targets CPU efficiency and portability, not beating a T4 GPU. Compare CPU INT8 vs CPU FP32/FP16 to show the real benefit of quantization.

---

## ğŸ“Š Visualizations & Flame Graphs

- âœ… Use `matplotlib` to plot:
  - Latency per optimization stage
  - Perplexity per stage
  - VRAM/RAM usage
- âœ… Export `torch.profiler` traces for flame graph inspection via TensorBoard.

ğŸ’¡ These help you understand the tradeoffs between model size, speed, and quality.

---

## âœ… Submission Checklist

Before submitting, ensure you've completed the following:

- [ ] Implemented all `TODO` sections in the notebook
- [ ] Included charts comparing latency, perplexity, and memory
- [ ] Saved the final notebook with outputs
- [ ] Removed any personal API keys or credentials
- [ ] Saved/exported profiling trace as JSON or viewable TensorBoard logs

---

## ğŸ“¤ Submission Instructions

You can submit the project via:

- The â€œSubmit Projectâ€ button in the Udacity workspace  
- As a zipped notebook (`project.ipynb`)  
- Or as a GitHub repo link with all required files  

---

## ğŸ§© Troubleshooting Tips

- For RAM issues, cast models to FP16 or use `low_cpu_mem_usage=True`
- Use `torch.cuda.empty_cache()` and `del model` between steps if memory is tight
- If using your local environment, install packages from the `requirements.txt` provided