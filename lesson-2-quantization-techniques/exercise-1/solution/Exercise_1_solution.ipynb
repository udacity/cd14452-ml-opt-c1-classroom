{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84c55181-86aa-42ca-9135-57a18189eb3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.11/site-packages (0.46.1)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.11/site-packages (1.9.0)\n",
      "Requirement already satisfied: torch<3,>=2.2 in /opt/conda/lib/python3.11/site-packages (from bitsandbytes) (2.7.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /opt/conda/lib/python3.11/site-packages (from accelerate) (0.34.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.11/site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (2023.6.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.5)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/conda/lib/python3.11/site-packages (from triton==3.3.1->torch<3,>=2.2->bitsandbytes) (78.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10592751-13b4-4df3-9677-7e423dc27468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import shutil\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d897e8f3-978a-444f-a2e5-eabda1c81b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── SETTINGS ────────────────────────────────────────────────────────────────\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "PROMPT = (\n",
    "    \"Over the next decade, sustainable energy solutions will revolutionize \"\n",
    "    \"global power grids, reducing carbon footprints and fostering resilient \"\n",
    "    \"communities through innovative storage and distribution technologies.\"\n",
    ")\n",
    "PERP_TEXT = (\n",
    "    \"Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast \"\n",
    "    \"to the natural intelligence displayed by humans and animals. Leading AI textbooks \"\n",
    "    \"define the field as the study of intelligent agents: any system that perceives \"\n",
    "    \"its environment and takes actions that maximize its chance of achieving its goals.\"\n",
    ")\n",
    "MAX_NEW_TOKENS = 50\n",
    "FP32_DIR = \"./model_fp32\"\n",
    "EIGHTBIT_DIR = \"./model_8bit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dff718c-3d24-4944-bed6-799030c121b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── HELPERS ─────────────────────────────────────────────────────────────────\n",
    "def get_dir_size_mib(path: str) -> float:\n",
    "    \"\"\"\n",
    "    TODO: walk `path` and sum file sizes to return MiB.\n",
    "    \"\"\"\n",
    "    total_bytes = 0\n",
    "    for root, _, files in os.walk(path):\n",
    "        for fname in files:\n",
    "            total_bytes += os.path.getsize(os.path.join(root, fname))\n",
    "    return total_bytes / 1024**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e601dd9-5a44-4446-b30f-f059c01944e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── LOADING & SAVING ─────────────────────────────────────────────────────────\n",
    "def load_and_save_fp32(model_name: str, save_dir: str):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Load tokenizer & FP32 model (torch_dtype=torch.float32)\n",
    "      - Force model onto CPU only (device_map={\"\": \"cpu\"})\n",
    "      - Set eval mode\n",
    "      - Save to `save_dir`\n",
    "      - Return (tokenizer, model)\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    model_fp32 = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float32,\n",
    "        device_map={\"\": \"cpu\"}         # force all weights to CPU\n",
    "    )\n",
    "    model_fp32.eval()\n",
    "\n",
    "    shutil.rmtree(save_dir, ignore_errors=True)\n",
    "    model_fp32.save_pretrained(save_dir)\n",
    "    return tokenizer, model_fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a88a58b-0f72-4af1-badf-c2c2ac779cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_save_8bit(model_name: str, save_dir: str):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Create a BitsAndBytesConfig with load_in_8bit=True\n",
    "      - Load tokenizer & 8-bit model via quantization_config\n",
    "      - Use device_map=\"auto\" so 8-bit layers go to GPU\n",
    "      - Set eval mode\n",
    "      - Save to `save_dir`\n",
    "      - Return model\n",
    "    \"\"\"\n",
    "    quant_config = BitsAndBytesConfig(load_in_8bit=True, llm_int8_threshold=6.0)\n",
    "    # Note: tokenizer can be reused from FP32 load if desired\n",
    "    model_8bit = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quant_config,\n",
    "        device_map=\"auto\"             # will put 8-bit weights on GPU\n",
    "    )\n",
    "    model_8bit.eval()\n",
    "\n",
    "    shutil.rmtree(save_dir, ignore_errors=True)\n",
    "    model_8bit.save_pretrained(save_dir)\n",
    "    return model_8bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "121f5d2f-2484-421d-a8b9-a3521143360a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_latency_and_throughput(model, tokenizer, prompt: str, device):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Tokenize `prompt` to tensors on `device`\n",
    "      - Warm up with a short generate\n",
    "      - Time a full generate(max_new_tokens=MAX_NEW_TOKENS)\n",
    "      - Return (latency_s, tokens_per_second)\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_len = inputs[\"input_ids\"].size(1)\n",
    "\n",
    "    # warm-up\n",
    "    _ = model.generate(**inputs, max_new_tokens=5)\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    # timed generation\n",
    "    start = time.time()\n",
    "    outputs = model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS)\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "\n",
    "    latency = end - start\n",
    "    gen_tokens = outputs.size(1) - input_len\n",
    "    return latency, gen_tokens / latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e47de385-e5bb-4df0-aa2d-a7d2bf1cca63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_peak_mem_and_perplexity(model, tokenizer, text: str, device):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Tokenize `text` to tensors on `device` with labels=input_ids\n",
    "      - Reset peak GPU mem stats (if CUDA)\n",
    "      - Run model(**inputs) under torch.no_grad()\n",
    "      - Sync CUDA, read max_memory_allocated → MiB\n",
    "      - Compute loss → perplexity = exp(loss)\n",
    "      - Return (peak_mem_mib, perplexity)\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    torch.cuda.synchronize()\n",
    "    peak_mem_mib = torch.cuda.max_memory_allocated(device) / 1024**2\n",
    "    perplexity = math.exp(out.loss.item())\n",
    "    return peak_mem_mib, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a5d359b-a506-474e-95b9-3674721e4ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start():\n",
    "    # 1) FP32 on CPU\n",
    "    tokenizer, model_fp32 = load_and_save_fp32(MODEL_NAME, FP32_DIR)\n",
    "    size_fp32 = get_dir_size_mib(FP32_DIR)\n",
    "    print(f\"FP32 size on disk: {size_fp32:.1f} MiB\")\n",
    "\n",
    "    # 2) 8-bit quantized on GPU\n",
    "    model_8bit = load_and_save_8bit(MODEL_NAME, EIGHTBIT_DIR)\n",
    "    size_8bit = get_dir_size_mib(EIGHTBIT_DIR)\n",
    "    print(f\"8-bit size on disk: {size_8bit:.1f} MiB\\n\")\n",
    "\n",
    "    # 3) Latency & Throughput\n",
    "    cpu = torch.device(\"cpu\")\n",
    "    gpu = torch.device(\"cuda\") if torch.cuda.is_available() else None\n",
    "\n",
    "    lat32, thr32 = measure_latency_and_throughput(model_fp32, tokenizer, PROMPT, cpu)\n",
    "    print(f\"FP32 (CPU)    → Latency: {lat32:.3f}s, Throughput: {thr32:.1f} tok/s\")\n",
    "\n",
    "    if gpu:\n",
    "        lat8, thr8 = measure_latency_and_throughput(model_8bit, tokenizer, PROMPT, gpu)\n",
    "        print(f\"8-bit (GPU)   → Latency: {lat8:.3f}s, Throughput: {thr8:.1f} tok/s\\n\")\n",
    "\n",
    "    # 4) Memory & Perplexity (8-bit on GPU)\n",
    "    if gpu:\n",
    "        peak8, ppl8 = measure_peak_mem_and_perplexity(model_8bit, tokenizer, PERP_TEXT, gpu)\n",
    "        print(f\"8-bit Peak GPU mem: {peak8:.1f} MiB\")\n",
    "        print(f\"8-bit Perplexity  : {ppl8:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d51de3d-edd1-482e-8f90-1bf839398ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 size on disk: 4196.4 MiB\n",
      "8-bit size on disk: 1175.7 MiB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 (CPU)    → Latency: 9.270s, Throughput: 5.4 tok/s\n",
      "8-bit (GPU)   → Latency: 3.974s, Throughput: 12.6 tok/s\n",
      "\n",
      "8-bit Peak GPU mem: 1269.4 MiB\n",
      "8-bit Perplexity  : 4.674\n"
     ]
    }
   ],
   "source": [
    "start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe33dc6-c35d-481c-8b75-280b635ab9e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
