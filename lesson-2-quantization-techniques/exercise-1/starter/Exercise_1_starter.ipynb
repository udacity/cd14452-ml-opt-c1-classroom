{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "211ffccf-a551-412c-b027-ce4eab2ddd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c594e9b-fffb-4057-a51e-f4f4c545e48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── SETTINGS ────────────────────────────────────────────────────────────────\n",
    "MODEL_NAME     = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"   # or your chosen variant\n",
    "PROMPT         = \"…\"  # TODO: write a 30–50 word prompt for generation\n",
    "PERP_TEXT      = \"…\"  # TODO: write a ~100-token passage for perplexity\n",
    "MAX_NEW_TOKENS = 50\n",
    "FP32_DIR       = \"./model_fp32\"\n",
    "EIGHTBIT_DIR   = \"./model_8bit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ab8f5b6-5a6c-40e4-8a4a-f712a3f937e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── HELPERS ─────────────────────────────────────────────────────────────────\n",
    "def get_dir_size_mib(path: str) -> float:\n",
    "    \"\"\"\n",
    "    TODO: walk `path` and sum file sizes to return MiB.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb283064-bfc3-443b-a475-876be9ebed49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── LOADING & SAVING ─────────────────────────────────────────────────────────\n",
    "def load_and_save_fp32(model_name: str, save_dir: str):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Load tokenizer & FP32 model (torch_dtype=torch.float32)\n",
    "      - Force model onto CPU only (device_map={\"\": \"cpu\"})\n",
    "      - Set eval mode\n",
    "      - Save to `save_dir`\n",
    "      - Return (tokenizer, model)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "204abd72-c621-453e-864f-52dec5282e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_save_8bit(model_name: str, save_dir: str):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Create a BitsAndBytesConfig with load_in_8bit=True\n",
    "      - Load tokenizer & 8-bit model via quantization_config\n",
    "      - Use device_map=\"auto\" so 8-bit layers go to GPU\n",
    "      - Set eval mode\n",
    "      - Save to `save_dir`\n",
    "      - Return model\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f51774b8-787b-4eb4-9de8-857e9bd9c77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── METRICS ──────────────────────────────────────────────────────────────────\n",
    "def measure_latency_and_throughput(model, tokenizer, prompt: str, device):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Tokenize `prompt` to tensors on `device`\n",
    "      - Warm up with a short generate\n",
    "      - Time a full generate(max_new_tokens=MAX_NEW_TOKENS)\n",
    "      - Return (latency_s, tokens_per_second)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbd35823-fe3c-405f-b5a3-30f8fabc24fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_peak_mem_and_perplexity(model, tokenizer, text: str, device):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Tokenize `text` to tensors on `device` with labels=input_ids\n",
    "      - Reset peak GPU mem stats (if CUDA)\n",
    "      - Run model(**inputs) under torch.no_grad()\n",
    "      - Sync CUDA, read max_memory_allocated → MiB\n",
    "      - Compute loss → perplexity = exp(loss)\n",
    "      - Return (peak_mem_mib, perplexity)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce335b03-d729-4985-a910-ffb783941af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start():\n",
    "    # 1) FP32 on CPU\n",
    "    tokenizer, model_fp32 = load_and_save_fp32(MODEL_NAME, FP32_DIR)\n",
    "    size_fp32 = get_dir_size_mib(FP32_DIR)\n",
    "    print(f\"FP32 size on disk: {size_fp32:.1f} MiB\")\n",
    "\n",
    "    # 2) 8-bit quantized on GPU\n",
    "    model_8bit = load_and_save_8bit(MODEL_NAME, EIGHTBIT_DIR)\n",
    "    size_8bit = get_dir_size_mib(EIGHTBIT_DIR)\n",
    "    print(f\"8-bit size on disk: {size_8bit:.1f} MiB\\n\")\n",
    "\n",
    "    # 3) Latency & Throughput\n",
    "    cpu = torch.device(\"cpu\")\n",
    "    gpu = torch.device(\"cuda\") if torch.cuda.is_available() else None\n",
    "\n",
    "    lat32, thr32 = measure_latency_and_throughput(model_fp32, tokenizer, PROMPT, cpu)\n",
    "    print(f\"FP32 (CPU)    → Latency: {lat32:.3f}s, Throughput: {thr32:.1f} tok/s\")\n",
    "\n",
    "    if gpu:\n",
    "        lat8, thr8 = measure_latency_and_throughput(model_8bit, tokenizer, PROMPT, gpu)\n",
    "        print(f\"8-bit (GPU)   → Latency: {lat8:.3f}s, Throughput: {thr8:.1f} tok/s\\n\")\n",
    "\n",
    "    # 4) Memory & Perplexity (8-bit on GPU)\n",
    "    if gpu:\n",
    "        peak8, ppl8 = measure_peak_mem_and_perplexity(model_8bit, tokenizer, PERP_TEXT, gpu)\n",
    "        print(f\"8-bit Peak GPU mem: {peak8:.1f} MiB\")\n",
    "        print(f\"8-bit Perplexity  : {ppl8:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37a0cf7-553b-46a2-9324-189a5c7d1fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
