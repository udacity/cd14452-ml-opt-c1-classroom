{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb2b1242-8e58-4d60-82d1-3972b6186d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 00:00:41.315292: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-05 00:00:41.330496: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-08-05 00:00:41.351020: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-08-05 00:00:41.356649: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-08-05 00:00:41.370077: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-05 00:00:49 [WARNING][auto_accelerator.py:454] Auto detect accelerator: CUDA_Accelerator.\n",
      "2025-08-05 00:00:49 [WARNING][modeling_auto.py:807] please install transformers>=4.46 for quantizing Qwen2VLForConditionalGeneration.\n",
      "2025-08-05 00:00:49 [WARNING][modeling_auto.py:814] please install transformers>=4.46 for quantizing MllamaForConditionalGeneration.\n",
      "2025-08-05 00:00:49 [WARNING][modeling_auto.py:821] please install transformers>=4.46 for quantizing LlavaForConditionalGeneration.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import shutil\n",
    "import torch\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from datasets import load_dataset, DownloadConfig\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    EvalPrediction,\n",
    ")\n",
    "from optimum.intel import INCTrainer, INCModelForCausalLM\n",
    "from neural_compressor import QuantizationAwareTrainingConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afdb9262-9057-474c-a135-fcf214fd5ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ‚îÄ SETTINGS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "PROMPT = (\n",
    "    \"Over the next decade, sustainable energy solutions will revolutionize \"\n",
    "    \"global power grids, reducing carbon footprints and fostering resilient \"\n",
    "    \"communities through innovative storage and distribution technologies.\"\n",
    ")\n",
    "PERP_TEXT = (\n",
    "    \"Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast \"\n",
    "    \"to the natural intelligence displayed by humans and animals. Leading AI textbooks \"\n",
    "    \"define the field as the study of intelligent agents: any system that perceives \"\n",
    "    \"its environment and takes actions that maximize its chance of achieving its goals.\"\n",
    ")\n",
    "MAX_NEW_TOKENS = 50\n",
    "OUTPUT_DIR = \"qat_tinyllama\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c3d52ef-d54d-416f-9ac9-68088d41f182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_id: str):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - load AutoModelForCausalLM.from_pretrained(model_id)\n",
    "      - enable gradient checkpointing \n",
    "      - load AutoTokenizer.from_pretrained(model_id)\n",
    "      - set tokenizer.pad_token = tokenizer.eos_token, and model.config.pad_token_id\n",
    "      - return model, tokenizer\n",
    "    \"\"\"\n",
    "    # 1. Load the model\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "    # (Optional) save memory during training\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    # 2. Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "    # 3. Make sure we have a pad token\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9c7f232-bc3c-45a3-a97e-ebe0eaffa447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(tokenizer, block_size=8, train_size=1000, eval_size=500):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - load ‚Äúwikitext‚Äù, \"wikitext-2-raw-v1\" via load_dataset\n",
    "      - tokenize via tokenizer in blocks of `block_size`; set labels=input_ids\n",
    "      - select first train_size for train_ds, eval_size for eval_ds\n",
    "      - return train_ds, eval_ds\n",
    "    \"\"\"\n",
    "    # 1) Load the raw Wikitext-2 dataset\n",
    "    raw = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "    # 2) Tokenization + label prep\n",
    "    def _tokenize(examples):\n",
    "        out = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=block_size,\n",
    "        )\n",
    "        out[\"labels\"] = out[\"input_ids\"].copy()\n",
    "        return out\n",
    "\n",
    "    tokenized = raw.map(\n",
    "        _tokenize,\n",
    "        batched=True,\n",
    "        remove_columns=[\"text\"],\n",
    "    )\n",
    "\n",
    "    # 3) Select the subsets\n",
    "    train_ds = tokenized[\"train\"].select(range(train_size))\n",
    "    eval_ds  = tokenized[\"validation\"].select(range(eval_size))\n",
    "\n",
    "    return train_ds, eval_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "080245c8-386e-43c1-b949-fa2c6879d276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inc_trainer(model, tokenizer, train_ds, quant_config, output_dir):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - instantiate TrainingArguments for QAT: \n",
    "          ‚Ä¢ num_train_epochs=1\n",
    "          ‚Ä¢ per_device_train_batch_size=1\n",
    "          ‚Ä¢ gradient_accumulation_steps=4\n",
    "          ‚Ä¢ fp16=True\n",
    "          ‚Ä¢ optim=\"adamw_bnb_8bit\"\n",
    "          ‚Ä¢ evaluation_strategy=\"no\"\n",
    "          ‚Ä¢ save_strategy=\"epoch\"\n",
    "      - pass model, quantization_config, args, train_dataset, tokenizer, data_collator\n",
    "      - return INCTrainer instance\n",
    "    \"\"\"\n",
    "    # 1) Define the Hugging Face TrainingArguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        fp16=True,\n",
    "        optim=\"adamw_bnb_8bit\",\n",
    "        evaluation_strategy=\"no\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=50,              # optional: adjust as you like\n",
    "        save_total_limit=1,            # keep only the latest checkpoint\n",
    "    )\n",
    "\n",
    "    # 2) Instantiate the INCTrainer\n",
    "    trainer = INCTrainer(\n",
    "        model=model,\n",
    "        quantization_config=quant_config,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=default_data_collator,\n",
    "    )\n",
    "\n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75af9f3b-c3e2-406c-856d-a743aa5e2eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_compute_ppl_fn(pad_token_id: int):\n",
    "    \"\"\"\n",
    "    Returns a compute_metrics function that knows the pad_token_id.\n",
    "    \"\"\"\n",
    "    def compute_ppl(pred: EvalPrediction):\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "          - shift logits and labels by one, flatten\n",
    "          - compute CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "          - return {\"eval_perplexity\": exp(loss).item()}\n",
    "        \"\"\"\n",
    "        # 1) Unpack\n",
    "        logits = pred.predictions         # np array (batch, seq_len, vocab_size)\n",
    "        labels = pred.label_ids           # np array (batch, seq_len)\n",
    "\n",
    "        # 2) Shift so each token predicts the next one\n",
    "        shift_logits = logits[..., :-1, :]\n",
    "        shift_labels = labels[..., 1:]\n",
    "\n",
    "        # 3) Flatten\n",
    "        flat_logits = shift_logits.reshape(-1, shift_logits.shape[-1])\n",
    "        flat_labels = shift_labels.reshape(-1)\n",
    "\n",
    "        # 4) To torch\n",
    "        logits_t = torch.from_numpy(flat_logits)\n",
    "        labels_t = torch.from_numpy(flat_labels)\n",
    "\n",
    "        # 5) CE loss ignoring pad_token_id\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "        loss = loss_fct(logits_t, labels_t)\n",
    "\n",
    "        # 6) Return perplexity\n",
    "        return {\"eval_perplexity\": torch.exp(loss).item()}\n",
    "    return compute_ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1896d4b-50bd-4197-a100-2a424ab3a94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_qat_and_evaluate(trainer, eval_ds, tokenizer):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - call trainer.train()\n",
    "      - create INCTrainer for evaluation with compute_metrics=compute_ppl\n",
    "      - call eval_trainer.evaluate() on a small subset\n",
    "      - return eval metrics\n",
    "    \"\"\"\n",
    "    # 1) Train with QAT\n",
    "    trainer.train()\n",
    "\n",
    "    # 2) Small slice for eval\n",
    "    small_eval = eval_ds.select(range(min(len(eval_ds), 100)))\n",
    "\n",
    "    # 3) Build a ppl_fn that closes over pad_token_id\n",
    "    ppl_fn = make_compute_ppl_fn(tokenizer.pad_token_id)\n",
    "\n",
    "    # 4) Set up evaluation INCTrainer\n",
    "    eval_args = TrainingArguments(\n",
    "        output_dir=trainer.args.output_dir,\n",
    "        per_device_eval_batch_size=1,\n",
    "        fp16=True,\n",
    "        eval_strategy=\"no\",\n",
    "        save_strategy=\"no\",\n",
    "        logging_steps=50,\n",
    "    )\n",
    "    eval_trainer = INCTrainer(\n",
    "        model=trainer.model,\n",
    "        args=eval_args,\n",
    "        eval_dataset=small_eval,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=default_data_collator,\n",
    "        compute_metrics=ppl_fn,  # << use the closure here\n",
    "    )\n",
    "\n",
    "    # 5) Evaluate and return metrics\n",
    "    return eval_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c43f6335-f9fd-48c8-870d-df9a6da99969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_load_qat_model(trainer, output_dir):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - call trainer.save_model()\n",
    "      - load back via INCModelForCausalLM.from_pretrained(output_dir)\n",
    "      - return loaded_model\n",
    "    \"\"\"\n",
    "    # 1) Save to the specified directory\n",
    "    trainer.save_model(output_dir)\n",
    "\n",
    "    # 2) Load it back as an INCModelForCausalLM\n",
    "    loaded_model = INCModelForCausalLM.from_pretrained(output_dir)\n",
    "\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67f7f1f8-3f95-47c1-9299-826e281c6db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_latency_and_throughput(model, tokenizer, prompt: str, device, max_new_tokens=50):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_len = inputs[\"input_ids\"].size(1)\n",
    "\n",
    "    # warm-up\n",
    "    _ = model.generate(**inputs, max_new_tokens=5)\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    # timed generation\n",
    "    start = time.time()\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "\n",
    "    latency = end - start\n",
    "    gen_tokens = outputs.size(1) - input_len\n",
    "    return latency, gen_tokens / latency\n",
    "\n",
    "def measure_peak_mem_and_perplexity(model, tokenizer, text: str, device):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    torch.cuda.synchronize()\n",
    "    peak_mem_mib = torch.cuda.max_memory_allocated(device) / 1024**2\n",
    "    perplexity = math.exp(out.loss.item())\n",
    "    return peak_mem_mib, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26545b47-4068-45c7-8b15-1c9b568d3725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start():\n",
    "    # 1) Load model & tokenizer\n",
    "    model, tokenizer = load_model_and_tokenizer(MODEL_NAME)\n",
    "\n",
    "    # 2) Prepare datasets\n",
    "    train_ds, eval_ds = prepare_datasets(tokenizer)\n",
    "\n",
    "    # 3) QAT configuration\n",
    "    quant_config = QuantizationAwareTrainingConfig()\n",
    "\n",
    "    # 4) Create and run QAT trainer\n",
    "    qat_trainer = create_inc_trainer(model, tokenizer, train_ds, quant_config, OUTPUT_DIR)\n",
    "    metrics = run_qat_and_evaluate(qat_trainer, eval_ds, tokenizer)\n",
    "    print(f\"Final perplexity: {metrics['eval_perplexity']:.2f}\")\n",
    "\n",
    "    # 5) Save & load quantized model\n",
    "    qat_model = save_and_load_qat_model(qat_trainer, OUTPUT_DIR)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    qat_model.to(device)\n",
    "\n",
    "    # 6) Benchmarks\n",
    "    latency, throughput = measure_latency_and_throughput(qat_model, tokenizer, PROMPT, device)\n",
    "    print(f\"Latency     : {latency:.3f} s\")\n",
    "    print(f\"Throughput  : {throughput:.1f} tokens/s\")\n",
    "\n",
    "    peak_mem, ppl = measure_peak_mem_and_perplexity(qat_model, tokenizer, PERP_TEXT, device)\n",
    "    print(f\"Peak GPU memory     : {peak_mem:.1f} MiB\")\n",
    "    print(f\"Next-token perplexity: {ppl:.3f}\")\n",
    "    size_mb = sum(os.path.getsize(os.path.join(OUTPUT_DIR, f)) for f in os.listdir(OUTPUT_DIR)) / 1024**2\n",
    "\n",
    "    print(f\"Quantized model size : {size_mb:.2f} MB\")\n",
    "    print(f\"Average latency      : {latency*1000:.1f} ms\")\n",
    "    print(f\"Throughput           : {throughput:.1f} tokens/sec\")\n",
    "    if peak_mem is not None:\n",
    "        print(f\"Peak GPU memory      : {peak_mem:.1f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ca15c9c-e473-4d91-8f70-f12362ef6174",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 04:20, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.787100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.348500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.415500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.125900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.139800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final perplexity: 49.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency     : 1.084 s\n",
      "Throughput  : 46.1 tokens/s\n",
      "Peak GPU memory     : 10592.2 MiB\n",
      "Next-token perplexity: 10.600\n",
      "Quantized model size : 4200.37 MB\n",
      "Average latency      : 1084.0 ms\n",
      "Throughput           : 46.1 tokens/sec\n",
      "Peak GPU memory      : 10592.2 MB\n"
     ]
    }
   ],
   "source": [
    "start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c575f545-7858-4131-b727-3e4c8550a751",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
