{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d931c8f-6f00-4348-8165-e3f47035943e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 23:14:29.789858: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-04 23:14:29.804082: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-08-04 23:14:29.822090: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-08-04 23:14:29.827774: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-08-04 23:14:29.840933: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-04 23:14:37 [WARNING][auto_accelerator.py:454] Auto detect accelerator: CUDA_Accelerator.\n",
      "2025-08-04 23:14:37 [WARNING][modeling_auto.py:807] please install transformers>=4.46 for quantizing Qwen2VLForConditionalGeneration.\n",
      "2025-08-04 23:14:37 [WARNING][modeling_auto.py:814] please install transformers>=4.46 for quantizing MllamaForConditionalGeneration.\n",
      "2025-08-04 23:14:37 [WARNING][modeling_auto.py:821] please install transformers>=4.46 for quantizing LlavaForConditionalGeneration.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from datasets import load_dataset, DownloadConfig\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    EvalPrediction,\n",
    ")\n",
    "from optimum.intel import INCTrainer, INCModelForCausalLM\n",
    "from neural_compressor import QuantizationAwareTrainingConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1896b93-47bf-40fc-98ec-0acb5de8e1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── SETTINGS ────────────────────────────────────────────────────────────────\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "PROMPT = (\n",
    "    \"Over the next decade, sustainable energy solutions will revolutionize \"\n",
    "    \"global power grids, reducing carbon footprints and fostering resilient \"\n",
    "    \"communities through innovative storage and distribution technologies.\"\n",
    ")\n",
    "PERP_TEXT = (\n",
    "    \"Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast \"\n",
    "    \"to the natural intelligence displayed by humans and animals. Leading AI textbooks \"\n",
    "    \"define the field as the study of intelligent agents: any system that perceives \"\n",
    "    \"its environment and takes actions that maximize its chance of achieving its goals.\"\n",
    ")\n",
    "MAX_NEW_TOKENS = 50\n",
    "OUTPUT_DIR = \"qat_tinyllama\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486199cc-1dc1-4bac-9621-d8187e57f9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_id: str):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - load AutoModelForCausalLM.from_pretrained(model_id)\n",
    "      - enable gradient checkpointing if desired\n",
    "      - load AutoTokenizer.from_pretrained(model_id)\n",
    "      - set tokenizer.pad_token = tokenizer.eos_token, and model.config.pad_token_id\n",
    "      - return model, tokenizer\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878101e6-83f9-4463-be3b-f4f4fb3c7f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(tokenizer, block_size=8, train_size=1000, eval_size=500):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - load “wikitext”, \"wikitext-2-raw-v1\" via load_dataset\n",
    "      - tokenize via tokenizer in blocks of `block_size`; set labels=input_ids\n",
    "      - select first train_size for train_ds, eval_size for eval_ds\n",
    "      - return train_ds, eval_ds\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fff4f4-6473-4e98-a4ea-061c176a3707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inc_trainer(model, tokenizer, train_ds, quant_config, output_dir):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - instantiate TrainingArguments for QAT: \n",
    "          • num_train_epochs=1\n",
    "          • per_device_train_batch_size=1\n",
    "          • gradient_accumulation_steps=4\n",
    "          • fp16=True\n",
    "          • optim=\"adamw_bnb_8bit\"\n",
    "          • evaluation_strategy=\"no\"\n",
    "          • save_strategy=\"epoch\"\n",
    "      - pass model, quantization_config, args, train_dataset, tokenizer, data_collator\n",
    "      - return INCTrainer instance\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e90580d-dcb8-4f8c-8af2-9024ec799e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ppl(pred: EvalPrediction):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - shift logits and labels by one, flatten\n",
    "      - compute CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "      - return {\"eval_perplexity\": exp(loss).item()}\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12163aa-d701-4fb6-b833-0f6accf1065b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_qat_and_evaluate(trainer, eval_ds, tokenizer):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - call trainer.train()\n",
    "      - create INCTrainer for evaluation with compute_metrics=compute_ppl\n",
    "      - call eval_trainer.evaluate() on a small subset\n",
    "      - return eval metrics\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd27b67-a0d2-4881-a15c-509104d472d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_load_qat_model(trainer, output_dir):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - call trainer.save_model()\n",
    "      - load back via INCModelForCausalLM.from_pretrained(output_dir)\n",
    "      - return loaded_model\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aa5e2f-c140-4770-9979-0713f3296f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_latency_and_throughput(model, tokenizer, prompt: str, device, max_new_tokens=50):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_len = inputs[\"input_ids\"].size(1)\n",
    "\n",
    "    # warm-up\n",
    "    _ = model.generate(**inputs, max_new_tokens=5)\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    # timed generation\n",
    "    start = time.time()\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "\n",
    "    latency = end - start\n",
    "    gen_tokens = outputs.size(1) - input_len\n",
    "    return latency, gen_tokens / latency\n",
    "\n",
    "def measure_peak_mem_and_perplexity(model, tokenizer, text: str, device):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    torch.cuda.synchronize()\n",
    "    peak_mem_mib = torch.cuda.max_memory_allocated(device) / 1024**2\n",
    "    perplexity = math.exp(out.loss.item())\n",
    "    return peak_mem_mib, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a92216a-a3f6-4dea-b14e-f2d1c0ef054a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start():\n",
    "    # 1) Load model & tokenizer\n",
    "    model, tokenizer = load_model_and_tokenizer(MODEL_NAME)\n",
    "\n",
    "    # 2) Prepare datasets\n",
    "    train_ds, eval_ds = prepare_datasets(tokenizer)\n",
    "\n",
    "    # 3) QAT configuration\n",
    "    quant_config = QuantizationAwareTrainingConfig()\n",
    "\n",
    "    # 4) Create and run QAT trainer\n",
    "    qat_trainer = create_inc_trainer(model, tokenizer, train_ds, quant_config, OUTPUT_DIR)\n",
    "    metrics = run_qat_and_evaluate(qat_trainer, eval_ds, tokenizer)\n",
    "    print(f\"Final perplexity: {metrics['eval_perplexity']:.2f}\")\n",
    "\n",
    "    # 5) Save & load quantized model\n",
    "    qat_model = save_and_load_qat_model(qat_trainer, OUTPUT_DIR)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    qat_model.to(device)\n",
    "\n",
    "    # 6) Benchmarks\n",
    "    latency, throughput = measure_latency_and_throughput(qat_model, tokenizer, PROMPT, device)\n",
    "    print(f\"Latency     : {latency:.3f} s\")\n",
    "    print(f\"Throughput  : {throughput:.1f} tokens/s\")\n",
    "\n",
    "    peak_mem, ppl = measure_peak_mem_and_perplexity(qat_model, tokenizer, PERP_TEXT, device)\n",
    "    print(f\"Peak GPU memory     : {peak_mem:.1f} MiB\")\n",
    "    print(f\"Next-token perplexity: {ppl:.3f}\")\n",
    "    size_mb = sum(os.path.getsize(os.path.join(OUTPUT_DIR, f)) for f in os.listdir(OUTPUT_DIR)) / 1024**2\n",
    "\n",
    "    print(f\"Quantized model size : {size_mb:.2f} MB\")\n",
    "    print(f\"Average latency      : {latency*1000:.1f} ms\")\n",
    "    print(f\"Throughput           : {throughput:.1f} tokens/sec\")\n",
    "    if peak_mem is not None:\n",
    "        print(f\"Peak GPU memory      : {peak_mem:.1f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7633d33-9ba1-4f03-b9b7-814a41147b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
