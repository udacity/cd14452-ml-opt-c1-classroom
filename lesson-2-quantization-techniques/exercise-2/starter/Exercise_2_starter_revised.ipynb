{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb2b1242-8e58-4d60-82d1-3972b6186d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-03 23:43:00.992905: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-03 23:43:01.006372: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-09-03 23:43:01.023874: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-09-03 23:43:01.029380: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-09-03 23:43:01.041980: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-03 23:43:06 [WARNING][auto_accelerator.py:454] Auto detect accelerator: CUDA_Accelerator.\n",
      "2025-09-03 23:43:06 [WARNING][modeling_auto.py:807] please install transformers>=4.46 for quantizing Qwen2VLForConditionalGeneration.\n",
      "2025-09-03 23:43:06 [WARNING][modeling_auto.py:814] please install transformers>=4.46 for quantizing MllamaForConditionalGeneration.\n",
      "2025-09-03 23:43:06 [WARNING][modeling_auto.py:821] please install transformers>=4.46 for quantizing LlavaForConditionalGeneration.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import shutil\n",
    "import torch\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from datasets import load_dataset, DownloadConfig\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    EvalPrediction,\n",
    ")\n",
    "from optimum.intel import INCTrainer, INCModelForCausalLM\n",
    "from neural_compressor import QuantizationAwareTrainingConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afdb9262-9057-474c-a135-fcf214fd5ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── SETTINGS ────────────────────────────────────────────────────────────────\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "PROMPT = (\n",
    "    \"Over the next decade, sustainable energy solutions will revolutionize \"\n",
    "    \"global power grids, reducing carbon footprints and fostering resilient \"\n",
    "    \"communities through innovative storage and distribution technologies.\"\n",
    ")\n",
    "PERP_TEXT = (\n",
    "    \"Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast \"\n",
    "    \"to the natural intelligence displayed by humans and animals. Leading AI textbooks \"\n",
    "    \"define the field as the study of intelligent agents: any system that perceives \"\n",
    "    \"its environment and takes actions that maximize its chance of achieving its goals.\"\n",
    ")\n",
    "MAX_NEW_TOKENS = 50\n",
    "OUTPUT_DIR = \"qat_tinyllama\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c3d52ef-d54d-416f-9ac9-68088d41f182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_id: str):\n",
    "    # 1. Load the model\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "    # (Optional) save memory during training\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    # 2. Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "    # 3. Make sure we have a pad token\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9c7f232-bc3c-45a3-a97e-ebe0eaffa447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(tokenizer, block_size=8, train_size=1000, eval_size=500):\n",
    "    # 1) Load the raw Wikitext-2 dataset\n",
    "    raw = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "    # 2) Tokenization + label prep\n",
    "    def _tokenize(examples):\n",
    "        out = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=block_size,\n",
    "        )\n",
    "        out[\"labels\"] = out[\"input_ids\"].copy()\n",
    "        return out\n",
    "\n",
    "    tokenized = raw.map(\n",
    "        _tokenize,\n",
    "        batched=True,\n",
    "        remove_columns=[\"text\"],\n",
    "    )\n",
    "\n",
    "    # 3) Select the subsets\n",
    "    train_ds = tokenized[\"train\"].select(range(train_size))\n",
    "    eval_ds  = tokenized[\"validation\"].select(range(eval_size))\n",
    "\n",
    "    return train_ds, eval_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "080245c8-386e-43c1-b949-fa2c6879d276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inc_trainer(model, tokenizer, train_ds, quant_config, output_dir):\n",
    "    from transformers import TrainingArguments\n",
    "    from optimum.intel import INCTrainer\n",
    "    from transformers import default_data_collator\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        optim=\"adamw_bnb_8bit\",\n",
    "        eval_strategy=\"no\",          # <- replaces deprecated evaluation_strategy\n",
    "        save_strategy=\"no\",          # <- NO mid-training checkpoints\n",
    "        save_only_model=True,        # <- smaller, model-only saves when we do save\n",
    "        logging_steps=50,\n",
    "        save_total_limit=1,\n",
    "        report_to=[],                # (quiet logs; optional)\n",
    "    )\n",
    "\n",
    "    trainer = INCTrainer(\n",
    "        model=model,\n",
    "        quantization_config=quant_config,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=default_data_collator,\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75af9f3b-c3e2-406c-856d-a743aa5e2eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_compute_ppl_fn(pad_token_id: int):\n",
    "    \"\"\"\n",
    "    Returns a compute_metrics function that knows the pad_token_id.\n",
    "    \"\"\"\n",
    "    def compute_ppl(pred: EvalPrediction):\n",
    "        # 1) Unpack\n",
    "        logits = pred.predictions         # np array (batch, seq_len, vocab_size)\n",
    "        labels = pred.label_ids           # np array (batch, seq_len)\n",
    "\n",
    "        # 2) Shift so each token predicts the next one\n",
    "        shift_logits = logits[..., :-1, :]\n",
    "        shift_labels = labels[..., 1:]\n",
    "\n",
    "        # 3) Flatten\n",
    "        flat_logits = shift_logits.reshape(-1, shift_logits.shape[-1])\n",
    "        flat_labels = shift_labels.reshape(-1)\n",
    "\n",
    "        # 4) To torch\n",
    "        logits_t = torch.from_numpy(flat_logits)\n",
    "        labels_t = torch.from_numpy(flat_labels)\n",
    "\n",
    "        # 5) CE loss ignoring pad_token_id\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "        loss = loss_fct(logits_t, labels_t)\n",
    "\n",
    "        # 6) Return perplexity\n",
    "        return {\"eval_perplexity\": torch.exp(loss).item()}\n",
    "    return compute_ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1896d4b-50bd-4197-a100-2a424ab3a94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_qat_and_evaluate(trainer, eval_ds, tokenizer):\n",
    "    # 1) Train with QAT\n",
    "    trainer.train()\n",
    "\n",
    "    # 2) Small slice for eval\n",
    "    small_eval = eval_ds.select(range(min(len(eval_ds), 100)))\n",
    "\n",
    "    # 3) Build a ppl_fn that closes over pad_token_id\n",
    "    ppl_fn = make_compute_ppl_fn(tokenizer.pad_token_id)\n",
    "\n",
    "    # 4) Set up evaluation INCTrainer\n",
    "    eval_args = TrainingArguments(\n",
    "        output_dir=trainer.args.output_dir,\n",
    "        per_device_eval_batch_size=1,\n",
    "        fp16=True,\n",
    "        eval_strategy=\"no\",\n",
    "        save_strategy=\"no\",\n",
    "        logging_steps=50,\n",
    "    )\n",
    "    eval_trainer = INCTrainer(\n",
    "        model=trainer.model,\n",
    "        args=eval_args,\n",
    "        eval_dataset=small_eval,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=default_data_collator,\n",
    "        compute_metrics=ppl_fn,  # << use the closure here\n",
    "    )\n",
    "\n",
    "    # 5) Evaluate and return metrics\n",
    "    return eval_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c43f6335-f9fd-48c8-870d-df9a6da99969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_load_qat_model(trainer, output_dir):\n",
    "    # 1) Save to the specified directory\n",
    "    trainer.save_model(output_dir)\n",
    "\n",
    "    # 2) Load it back as an INCModelForCausalLM\n",
    "    loaded_model = INCModelForCausalLM.from_pretrained(output_dir)\n",
    "\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67f7f1f8-3f95-47c1-9299-826e281c6db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_latency_and_throughput(model, tokenizer, prompt: str, device, max_new_tokens=50):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_len = inputs[\"input_ids\"].size(1)\n",
    "\n",
    "    # warm-up\n",
    "    _ = model.generate(**inputs, max_new_tokens=5)\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    # timed generation\n",
    "    start = time.time()\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "\n",
    "    latency = end - start\n",
    "    gen_tokens = outputs.size(1) - input_len\n",
    "    return latency, gen_tokens / latency\n",
    "\n",
    "def measure_peak_mem_and_perplexity(model, tokenizer, text: str, device):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    torch.cuda.synchronize()\n",
    "    peak_mem_mib = torch.cuda.max_memory_allocated(device) / 1024**2\n",
    "    perplexity = math.exp(out.loss.item())\n",
    "    return peak_mem_mib, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26545b47-4068-45c7-8b15-1c9b568d3725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 13:46, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.397400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.348700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.351600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.072600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.096600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final perplexity: 55.14\n",
      "Latency     : 1.180 s\n",
      "Throughput  : 42.4 tokens/s\n",
      "Peak GPU memory     : 10660.4 MiB\n",
      "Next-token perplexity: 10.961\n",
      "Quantized model size : 4198.67 MB\n",
      "Average latency      : 1179.8 ms\n",
      "Throughput           : 42.4 tokens/sec\n",
      "Peak GPU memory      : 10660.4 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1) Load model & tokenizer\n",
    "model, tokenizer = load_model_and_tokenizer(MODEL_NAME)\n",
    "\n",
    "# 2) Prepare datasets\n",
    "train_ds, eval_ds = prepare_datasets(tokenizer)\n",
    "\n",
    "# 3) QAT configuration\n",
    "quant_config = QuantizationAwareTrainingConfig()\n",
    "\n",
    "# 4) Create and run QAT trainer\n",
    "qat_trainer = create_inc_trainer(model, tokenizer, train_ds, quant_config, OUTPUT_DIR)\n",
    "metrics = run_qat_and_evaluate(qat_trainer, eval_ds, tokenizer)\n",
    "print(f\"Final perplexity: {metrics['eval_perplexity']:.2f}\")\n",
    "\n",
    "# 5) Save & load quantized model\n",
    "qat_model = save_and_load_qat_model(qat_trainer, OUTPUT_DIR)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "qat_model.to(device)\n",
    "\n",
    "# 6) Benchmarks\n",
    "latency, throughput = measure_latency_and_throughput(qat_model, tokenizer, PROMPT, device)\n",
    "print(f\"Latency     : {latency:.3f} s\")\n",
    "print(f\"Throughput  : {throughput:.1f} tokens/s\")\n",
    "\n",
    "peak_mem, ppl = measure_peak_mem_and_perplexity(qat_model, tokenizer, PERP_TEXT, device)\n",
    "print(f\"Peak GPU memory     : {peak_mem:.1f} MiB\")\n",
    "print(f\"Next-token perplexity: {ppl:.3f}\")\n",
    "size_mb = sum(os.path.getsize(os.path.join(OUTPUT_DIR, f)) for f in os.listdir(OUTPUT_DIR)) / 1024**2\n",
    "\n",
    "print(f\"Quantized model size : {size_mb:.2f} MB\")\n",
    "print(f\"Average latency      : {latency*1000:.1f} ms\")\n",
    "print(f\"Throughput           : {throughput:.1f} tokens/sec\")\n",
    "if peak_mem is not None:\n",
    "    print(f\"Peak GPU memory      : {peak_mem:.1f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f2adf24-e9bc-441f-b92e-badfaf9570b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_ppl_cpu_texts(model, tokenizer, texts, max_length=64, batch_size=1):\n",
    "    import math, torch\n",
    "    model.eval().to(\"cpu\")\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    with torch.inference_mode():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            enc = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "            labels = enc[\"input_ids\"].clone()\n",
    "            # ignore pad positions in loss\n",
    "            pad_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "            attention = enc[\"attention_mask\"]\n",
    "            labels[attention == 0] = -100\n",
    "            out = model(**enc, labels=labels)\n",
    "            # HF loss is mean over non -100 positions; weight by #tokens to combine batches\n",
    "            n_tokens = (labels != -100).sum().item()\n",
    "            total_loss += float(out.loss) * max(1, n_tokens)\n",
    "            total_tokens += max(1, n_tokens)\n",
    "    return math.exp(total_loss / max(1, total_tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a9f8a4-4e19-4a4d-923f-32c397c4c94b",
   "metadata": {},
   "source": [
    "# Exercise 1 — Activation fake-quant sensitivity (CPU, per-layer, no training)\n",
    "\n",
    "Simulate QAT-style activation quantization one layer at a time with a forward pre-hook (8-bit symmetric per-tensor). Measure perplexity drift vs baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6577de13-a3ff-47b1-b11d-c7b1c2b1ff34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_fake_quant_sensitivity_cpu(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    texts=None,\n",
    "    max_length=64,\n",
    "    batch_size=1,\n",
    "    last_k_linear=8,   # test only the last K Linear layers to keep it light\n",
    "):\n",
    "    \"\"\"\n",
    "    TODO: For each of the last K nn.Linear layers:\n",
    "      - add a forward pre-hook that fake-quantizes the *input activation* to int8 (symmetric)\n",
    "      - run a tiny CPU perplexity eval\n",
    "      - remove the hook\n",
    "    Returns a DataFrame with baseline and per-layer PPL deltas.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afa1dd63-82b6-42bb-bd2a-729fd080bf49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer</th>\n",
       "      <th>ppl_with_act_fakequant</th>\n",
       "      <th>ppl_baseline</th>\n",
       "      <th>delta_ppl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>model.layers.21.mlp.up_proj</td>\n",
       "      <td>10.717280</td>\n",
       "      <td>10.961409</td>\n",
       "      <td>-0.244129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>model.layers.21.mlp.gate_proj</td>\n",
       "      <td>10.731192</td>\n",
       "      <td>10.961409</td>\n",
       "      <td>-0.230217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lm_head</td>\n",
       "      <td>10.904405</td>\n",
       "      <td>10.961409</td>\n",
       "      <td>-0.057004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>model.layers.21.self_attn.v_proj</td>\n",
       "      <td>10.955287</td>\n",
       "      <td>10.961409</td>\n",
       "      <td>-0.006121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>model.layers.21.self_attn.q_proj</td>\n",
       "      <td>10.959927</td>\n",
       "      <td>10.961409</td>\n",
       "      <td>-0.001482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>model.layers.21.self_attn.o_proj</td>\n",
       "      <td>10.968125</td>\n",
       "      <td>10.961409</td>\n",
       "      <td>0.006716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>model.layers.21.self_attn.k_proj</td>\n",
       "      <td>10.974499</td>\n",
       "      <td>10.961409</td>\n",
       "      <td>0.013091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>model.layers.21.mlp.down_proj</td>\n",
       "      <td>11.220519</td>\n",
       "      <td>10.961409</td>\n",
       "      <td>0.259110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              layer  ppl_with_act_fakequant  ppl_baseline  \\\n",
       "0       model.layers.21.mlp.up_proj               10.717280     10.961409   \n",
       "1     model.layers.21.mlp.gate_proj               10.731192     10.961409   \n",
       "2                           lm_head               10.904405     10.961409   \n",
       "3  model.layers.21.self_attn.v_proj               10.955287     10.961409   \n",
       "4  model.layers.21.self_attn.q_proj               10.959927     10.961409   \n",
       "5  model.layers.21.self_attn.o_proj               10.968125     10.961409   \n",
       "6  model.layers.21.self_attn.k_proj               10.974499     10.961409   \n",
       "7     model.layers.21.mlp.down_proj               11.220519     10.961409   \n",
       "\n",
       "   delta_ppl  \n",
       "0  -0.244129  \n",
       "1  -0.230217  \n",
       "2  -0.057004  \n",
       "3  -0.006121  \n",
       "4  -0.001482  \n",
       "5   0.006716  \n",
       "6   0.013091  \n",
       "7   0.259110  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_act_sens = activation_fake_quant_sensitivity_cpu(model, tokenizer, last_k_linear=8)\n",
    "df_act_sens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc487708-4d5c-46c6-a651-75651328c833",
   "metadata": {},
   "source": [
    "# Exercise 2 — CPU decoding strategy benchmark (no quant, no training)\n",
    "\n",
    "Benchmark latency & tokens/sec on CPU for common decoding setups (greedy / top-k / top-p / beam), across a few prompt lengths. Uses your model as-is; pure eval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "886a81f1-2448-41b3-b50a-a1ac1d924933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpu_decoding_strategy_benchmark(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    base_text=\"This is a short prompt for CPU decoding benchmark. \",\n",
    "    prompt_lengths=(16, 64, 256),\n",
    "    new_tokens=64,\n",
    "    runs=3,\n",
    "):\n",
    "    \"\"\"\n",
    "    TODO: On CPU, measure latency and throughput for different decoding strategies\n",
    "    across several prompt lengths.\n",
    "    Returns a DataFrame.\n",
    "    \"\"\"\n",
    "    import time, numpy as np, torch, pandas as pd\n",
    "    from copy import deepcopy\n",
    "\n",
    "    model.eval().to(\"cpu\")\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    strategies = {\n",
    "        \"greedy\": dict(do_sample=False),\n",
    "        \"topk\":   dict(do_sample=True, top_k=50, temperature=1.0),\n",
    "        \"topp\":   dict(do_sample=True, top_p=0.9, temperature=1.0),\n",
    "        \"beam4\":  dict(do_sample=False, num_beams=4, early_stopping=True),\n",
    "    }\n",
    "\n",
    "    def build_prompt(L):\n",
    "        ids = tokenizer(base_text, return_tensors=\"pt\")[\"input_ids\"][0].tolist()\n",
    "        if len(ids) < 4:\n",
    "            ids = (ids or [tokenizer.eos_token_id]) * 8\n",
    "        reps = max(1, (L + len(ids) - 1) // len(ids))\n",
    "        return tokenizer.decode(ids * reps)[: L]  # approximate length in tokens\n",
    "\n",
    "    rows = []\n",
    "    for L in prompt_lengths:\n",
    "        prompt = build_prompt(L)\n",
    "        enc = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        input_len = enc[\"input_ids\"].size(1)\n",
    "\n",
    "        for name, gen_kwargs in strategies.items():\n",
    "            # warmup\n",
    "            _ = model.generate(**enc, max_new_tokens=8, **gen_kwargs)\n",
    "            # timed runs\n",
    "            times = []\n",
    "            for _ in range(runs):\n",
    "                t0 = time.time()\n",
    "                _ = model.generate(**enc, max_new_tokens=new_tokens, **gen_kwargs)\n",
    "                times.append(time.time() - t0)\n",
    "            lat_ms = float(np.mean(times) * 1000.0)\n",
    "            toks_per_s = new_tokens / float(np.mean(times))\n",
    "            rows.append({\n",
    "                \"prompt_len_tokens(approx)\": L,\n",
    "                \"strategy\": name,\n",
    "                \"latency_ms\": lat_ms,\n",
    "                \"throughput_tokens_per_s\": toks_per_s,\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows).sort_values([\"prompt_len_tokens(approx)\", \"strategy\"]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e747eb1d-733d-4ef0-94a9-ecf21afc4c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpu_decoding_strategy_benchmark(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    base_text=\"This is a short prompt for CPU decoding benchmark. \",\n",
    "    prompt_lengths=(16, 64, 256),\n",
    "    new_tokens=64,\n",
    "    runs=3,\n",
    "):\n",
    "    \"\"\"\n",
    "    Measure CPU decoding latency & throughput across decoding strategies and prompt lengths.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame with columns:\n",
    "      ['prompt_len_tokens(approx)', 'strategy', 'latency_ms', 'throughput_tokens_per_s']\n",
    "\n",
    "    Student TODOs:\n",
    "      - Ensure the model is on CPU and in eval() mode.\n",
    "      - Define a few decoding strategies (greedy, top-k, top-p, beam).\n",
    "      - Build prompts that roughly match target token lengths.\n",
    "      - Warm up each strategy before timing.\n",
    "      - Time multiple runs and average.\n",
    "      - Return a tidy DataFrame.\n",
    "    \"\"\"\n",
    "    # --- Imports (keep local so function is self-contained) ---\n",
    "    import time\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import pandas as pd\n",
    "\n",
    "    # ---------- TODO 1: Put model in eval mode and move to CPU ----------\n",
    "    # Hints:\n",
    "    #   - Use model.eval()\n",
    "    #   - Use model.to(\"cpu\")\n",
    "    #   - Disable grads with torch.set_grad_enabled(False)\n",
    "    # YOUR CODE HERE\n",
    "    # model.eval()\n",
    "    # model.to(\"cpu\")\n",
    "    # torch.set_grad_enabled(False)\n",
    "\n",
    "    # ---------- TODO 2: Define decoding strategies ----------\n",
    "    # Provide a small set with different search behaviors:\n",
    "    #   - \"greedy\": no sampling\n",
    "    #   - \"topk\":   do_sample=True, top_k=50\n",
    "    #   - \"topp\":   do_sample=True, top_p=0.9\n",
    "    #   - \"beam4\":  beam search with num_beams=4 (no sampling)\n",
    "    # Keep temperature at 1.0 unless you want them to explore.\n",
    "    strategies = {\n",
    "    }\n",
    "\n",
    "    # ---------- Helper: approximate a prompt with ~L tokens ----------\n",
    "    # TODO 3: Implement build_prompt(L) so it returns a string whose tokenized\n",
    "    # length is approximately L. You can:\n",
    "    #   - tokenize base_text once to get a token list\n",
    "    #   - repeat it to exceed L\n",
    "    #   - decode back to string and (optionally) trim by tokens or characters\n",
    "    def build_prompt(L: int) -> str:\n",
    "        raise NotImplementedError(\"build_prompt(L) not implemented\")\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    # ---------- Main loop over prompt lengths ----------\n",
    "    for L in prompt_lengths:\n",
    "        # TODO 4: Build prompt and encode once on CPU\n",
    "        #   - Use tokenizer(prompt, return_tensors=\"pt\")\n",
    "        #   - Keep the encoded dict on CPU (no .to('cuda'))\n",
    "        prompt = None  # = build_prompt(L)\n",
    "        enc = None     # = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        # Optional: record the actual input length from enc[\"input_ids\"].size(1)\n",
    "        # input_len = enc[\"input_ids\"].size(1)\n",
    "\n",
    "        # ---------- Loop over decoding strategies ----------\n",
    "        for name, gen_kwargs in strategies.items():\n",
    "            # TODO 5: Warm-up (short generate) to trigger kernels/JIT\n",
    "            #   - e.g., max_new_tokens=8\n",
    "            # with torch.inference_mode():\n",
    "            #     _ = model.generate(**enc, max_new_tokens=8, **gen_kwargs)\n",
    "\n",
    "            # TODO 6: Timed runs (average over `runs`)\n",
    "            times = []\n",
    "\n",
    "            # TODO 7: Compute metrics\n",
    "\n",
    "            # TODO 8: Append a result row\n",
    "\n",
    "    # ---------- TODO 9: Return a tidy DataFrame sorted by prompt length then strategy ----------\n",
    "    # df = pd.DataFrame(rows).sort_values([\"prompt_len_tokens(approx)\", \"strategy\"]).reset_index(drop=True)\n",
    "    # return df\n",
    "    raise NotImplementedError(\"Return the DataFrame once all TODOs are implemented\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce861757-5519-4f58-9a1e-1a63b0a50ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_len_tokens(approx)</th>\n",
       "      <th>strategy</th>\n",
       "      <th>latency_ms</th>\n",
       "      <th>throughput_tokens_per_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>beam4</td>\n",
       "      <td>5062.797904</td>\n",
       "      <td>6.320616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>greedy</td>\n",
       "      <td>2567.549944</td>\n",
       "      <td>12.463243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>topk</td>\n",
       "      <td>2583.953261</td>\n",
       "      <td>12.384125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>topp</td>\n",
       "      <td>2620.415092</td>\n",
       "      <td>12.211806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>beam4</td>\n",
       "      <td>5132.568240</td>\n",
       "      <td>6.234695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>64</td>\n",
       "      <td>greedy</td>\n",
       "      <td>2692.276001</td>\n",
       "      <td>11.885854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>64</td>\n",
       "      <td>topk</td>\n",
       "      <td>2410.951018</td>\n",
       "      <td>13.272771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>64</td>\n",
       "      <td>topp</td>\n",
       "      <td>2791.317344</td>\n",
       "      <td>11.464121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>256</td>\n",
       "      <td>beam4</td>\n",
       "      <td>5807.430387</td>\n",
       "      <td>5.510182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>256</td>\n",
       "      <td>greedy</td>\n",
       "      <td>2800.542116</td>\n",
       "      <td>11.426359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>256</td>\n",
       "      <td>topk</td>\n",
       "      <td>2793.191195</td>\n",
       "      <td>11.456430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>256</td>\n",
       "      <td>topp</td>\n",
       "      <td>2317.993283</td>\n",
       "      <td>13.805044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    prompt_len_tokens(approx) strategy   latency_ms  throughput_tokens_per_s\n",
       "0                          16    beam4  5062.797904                 6.320616\n",
       "1                          16   greedy  2567.549944                12.463243\n",
       "2                          16     topk  2583.953261                12.384125\n",
       "3                          16     topp  2620.415092                12.211806\n",
       "4                          64    beam4  5132.568240                 6.234695\n",
       "5                          64   greedy  2692.276001                11.885854\n",
       "6                          64     topk  2410.951018                13.272771\n",
       "7                          64     topp  2791.317344                11.464121\n",
       "8                         256    beam4  5807.430387                 5.510182\n",
       "9                         256   greedy  2800.542116                11.426359\n",
       "10                        256     topk  2793.191195                11.456430\n",
       "11                        256     topp  2317.993283                13.805044"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cpu_decode = cpu_decoding_strategy_benchmark(model, tokenizer, prompt_lengths=(16, 64, 256), new_tokens=32, runs=2)\n",
    "df_cpu_decode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3d28f6-9ffb-414c-9495-5849de991ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
