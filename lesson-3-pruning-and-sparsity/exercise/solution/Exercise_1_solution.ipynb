{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d0e7ea7-e0bc-4f14-a4ad-9c2246094287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3919f56f-169a-42c5-b187-75144f2a1347",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME     = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "MLP_PRUNE_FRAC = 0.5       # fraction of inner neurons to prune\n",
    "MAX_NEW_TOKENS = 50\n",
    "PROMPT = (\n",
    "    \"Over the next decade, sustainable energy solutions will revolutionize \"\n",
    "    \"global power grids, reducing carbon footprints and fostering resilient \"\n",
    "    \"communities through innovative storage and distribution technologies.\"\n",
    ")\n",
    "PERP_TEXT = (\n",
    "    \"Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast \"\n",
    "    \"to the natural intelligence displayed by humans and animals. Leading AI textbooks \"\n",
    "    \"define the field as the study of intelligent agents: any system that perceives \"\n",
    "    \"its environment and takes actions that maximize its chance of achieving its goals.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53152b60-68e9-4e9c-a997-35fcb0c2334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name: str, device: torch.device):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Load AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "      - Load AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "      - Move model to `device` and set to .eval()\n",
    "      - Return tokenizer, model\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    # load model in FP16 for faster inference\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    # move to device and set to eval\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b41db383-dbff-49b1-81cc-0b8f1969173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_baseline(model: nn.Module, tokenizer, prompt: str, perp_text: str, device: torch.device):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Warm up & measure generation latency & throughput on `prompt`\n",
    "      - Measure peak GPU memory & perplexity on `perp_text`\n",
    "      - Print or return these baseline metrics\n",
    "    \"\"\"\n",
    "    # 1) Measure latency & throughput\n",
    "    latency, throughput = measure_latency_and_throughput(model, tokenizer, prompt, device)\n",
    "    # 2) Measure peak GPU memory & perplexity\n",
    "    peak_mem, perplexity = measure_peak_mem_and_perplexity(model, tokenizer, perp_text, device)\n",
    "\n",
    "    # 3) Print baseline metrics\n",
    "    print(f\"[Baseline] latency   = {latency:.3f}s\")\n",
    "    print(f\"[Baseline] throughput= {throughput:.1f} tok/s\")\n",
    "    print(f\"[Baseline] peak GPU  = {peak_mem:.1f} MiB\")\n",
    "    print(f\"[Baseline] perplexity= {perplexity:.3f}\")\n",
    "\n",
    "    # Return in case caller wants to use them programmatically\n",
    "    return {\n",
    "        \"latency\": latency,\n",
    "        \"throughput\": throughput,\n",
    "        \"peak_gpu_mem\": peak_mem,\n",
    "        \"perplexity\": perplexity\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eecc93ae-e73e-4136-9628-78f2fc609a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_mlp_rows_and_cols(model: nn.Module, prune_frac: float):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Move model to CPU\n",
    "      - For each layer in model.model.layers:\n",
    "          • Zero out `prune_frac` of rows in gate_proj and up_proj\n",
    "          • Zero out corresponding `prune_frac` of columns in down_proj\n",
    "      - Remove pruning reparameterizations\n",
    "    \"\"\"\n",
    "    # 1) Ensure we prune on CPU to avoid GPU OOM\n",
    "    model.cpu()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # 2) Iterate through each decoder layer’s MLP\n",
    "    for layer in model.model.layers:\n",
    "        gate = layer.mlp.gate_proj   # [inner, hidden]\n",
    "        up   = layer.mlp.up_proj     # [inner, hidden]\n",
    "        down = layer.mlp.down_proj   # [hidden, inner]\n",
    "\n",
    "        # 2a) Zero out rows in gate_proj and up_proj\n",
    "        for proj in (gate, up):\n",
    "            prune.ln_structured(\n",
    "                proj,\n",
    "                name=\"weight\",\n",
    "                amount=prune_frac,\n",
    "                n=1,\n",
    "                dim=0,           # prune entire rows\n",
    "            )\n",
    "            prune.remove(proj, \"weight\")\n",
    "\n",
    "        # 2b) Zero out corresponding columns in down_proj\n",
    "        prune.ln_structured(\n",
    "            down,\n",
    "            name=\"weight\",\n",
    "            amount=prune_frac,\n",
    "            n=1,\n",
    "            dim=1,               # prune entire columns\n",
    "        )\n",
    "        prune.remove(down, \"weight\")\n",
    "\n",
    "    # 3) Return the model (now with zeros in place)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fbd93d6-0cd5-42d5-a2d3-80671fc0c1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_mlp_blocks(model: nn.Module):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - For each layer in model.model.layers:\n",
    "          1) Identify kept neuron indices in gate_proj\n",
    "          2) Construct new nn.Linear modules for gate_proj, up_proj, down_proj\n",
    "             with reduced dimensions\n",
    "          3) Copy over weights and biases\n",
    "          4) Replace the old modules on the model\n",
    "    \"\"\"\n",
    "    for layer in model.model.layers:\n",
    "        # original modules (still on CPU, dtype=original)\n",
    "        old_gate = layer.mlp.gate_proj\n",
    "        old_up   = layer.mlp.up_proj\n",
    "        old_down = layer.mlp.down_proj\n",
    "\n",
    "        # discover surviving rows in gate_proj\n",
    "        Wg = old_gate.weight.data     # [inner_orig, hidden], dtype say torch.half\n",
    "        keep_idx = (Wg.abs().sum(dim=1) != 0).nonzero(as_tuple=False).view(-1)\n",
    "        inner_new = keep_idx.numel()\n",
    "        hidden    = Wg.size(1)\n",
    "        dtype     = Wg.dtype\n",
    "        device    = Wg.device\n",
    "\n",
    "        # helper to build a new Linear with the same dtype/device\n",
    "        def make_linear(in_f, out_f, bias, old_weight, old_bias=None):\n",
    "            nl = nn.Linear(in_f, out_f, bias=bias)\n",
    "            # init in correct dtype & device\n",
    "            nl.weight.data = old_weight.clone().to(device=device, dtype=dtype)\n",
    "            if bias and old_bias is not None:\n",
    "                nl.bias.data = old_bias.clone().to(device=device, dtype=dtype)\n",
    "            return nl\n",
    "\n",
    "        # rebuild gate_proj: hidden -> inner_new\n",
    "        new_gate = make_linear(\n",
    "            hidden, inner_new, \n",
    "            bias=(old_gate.bias is not None),\n",
    "            old_weight=old_gate.weight.data[keep_idx],\n",
    "            old_bias=old_gate.bias.data[keep_idx] if old_gate.bias is not None else None\n",
    "        )\n",
    "\n",
    "        # rebuild up_proj: hidden -> inner_new\n",
    "        new_up = make_linear(\n",
    "            hidden, inner_new,\n",
    "            bias=(old_up.bias is not None),\n",
    "            old_weight=old_up.weight.data[keep_idx],\n",
    "            old_bias=old_up.bias.data[keep_idx] if old_up.bias is not None else None\n",
    "        )\n",
    "\n",
    "        # rebuild down_proj: inner_new -> hidden\n",
    "        new_down = make_linear(\n",
    "            inner_new, hidden,\n",
    "            bias=(old_down.bias is not None),\n",
    "            old_weight=old_down.weight.data[:, keep_idx],\n",
    "            old_bias=old_down.bias.data if old_down.bias is not None else None\n",
    "        )\n",
    "\n",
    "        # swap in-place\n",
    "        layer.mlp.gate_proj = new_gate\n",
    "        layer.mlp.up_proj   = new_up\n",
    "        layer.mlp.down_proj = new_down\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fcba78d-36a9-4c2f-a7ab-8d1b78033154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_rebuilt(model: nn.Module, tokenizer, prompt: str, perp_text: str, device: torch.device):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Move rebuilt model to `device` & .eval()\n",
    "      - Re-measure latency, throughput, peak memory, perplexity\n",
    "      - Print or return these metrics\n",
    "    \"\"\"\n",
    "    # 1) Move to device and set to eval\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # 2) Measure latency & throughput\n",
    "    latency, throughput = measure_latency_and_throughput(\n",
    "        model, tokenizer, prompt, device\n",
    "    )\n",
    "\n",
    "    # 3) Measure peak GPU memory & perplexity\n",
    "    peak_mem, perplexity = measure_peak_mem_and_perplexity(\n",
    "        model, tokenizer, perp_text, device\n",
    "    )\n",
    "\n",
    "    # 4) Print results\n",
    "    print(f\"[Rebuilt] latency   = {latency:.3f}s\")\n",
    "    print(f\"[Rebuilt] throughput= {throughput:.1f} tok/s\")\n",
    "    print(f\"[Rebuilt] peak GPU  = {peak_mem:.1f} MiB\")\n",
    "    print(f\"[Rebuilt] perplexity= {perplexity:.3f}\")\n",
    "\n",
    "    # 5) Return for further use if needed\n",
    "    return {\n",
    "        \"latency\": latency,\n",
    "        \"throughput\": throughput,\n",
    "        \"peak_gpu_mem\": peak_mem,\n",
    "        \"perplexity\": perplexity\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df00ca42-c803-4d06-bc21-f50f4ef0ec22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_report_size(model: nn.Module, output_dir: str):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - model.save_pretrained(output_dir)\n",
    "      - Walk `output_dir` to sum file sizes (in MiB)\n",
    "      - Print the on-disk size\n",
    "    \"\"\"\n",
    "    # 1) Save\n",
    "    model.save_pretrained(output_dir)\n",
    "\n",
    "    # 2) Sum file sizes\n",
    "    total_bytes = 0\n",
    "    for root, _, files in os.walk(output_dir):\n",
    "        for fname in files:\n",
    "            total_bytes += os.path.getsize(os.path.join(root, fname))\n",
    "\n",
    "    # 3) Convert to MiB and print\n",
    "    size_mb = total_bytes / 1024**2\n",
    "    print(f\"[Rebuilt] on-disk size = {size_mb:.1f} MiB\")\n",
    "\n",
    "    return size_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b5d4e48-12d5-45d6-ad2f-1a32440b7ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_peak_mem_and_perplexity(model, tokenizer, text: str, device):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Tokenize `text` to tensors on `device` with labels=input_ids\n",
    "      - Reset peak GPU mem stats (if CUDA)\n",
    "      - Run model(**inputs) under torch.no_grad()\n",
    "      - Sync CUDA, read max_memory_allocated → MiB\n",
    "      - Compute loss → perplexity = exp(loss)\n",
    "      - Return (peak_mem_mib, perplexity)\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    torch.cuda.synchronize()\n",
    "    peak_mem_mib = torch.cuda.max_memory_allocated(device) / 1024**2\n",
    "    perplexity = math.exp(out.loss.item())\n",
    "    return peak_mem_mib, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9e38101-2b8b-4185-aa63-d7cae0db8594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_latency_and_throughput(model, tokenizer, prompt: str, device):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Tokenize `prompt` to tensors on `device`\n",
    "      - Warm up with a short generate\n",
    "      - Time a full generate(max_new_tokens=MAX_NEW_TOKENS)\n",
    "      - Return (latency_s, tokens_per_second)\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_len = inputs[\"input_ids\"].size(1)\n",
    "\n",
    "    # warm-up\n",
    "    _ = model.generate(**inputs, max_new_tokens=5)\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    # timed generation\n",
    "    start = time.time()\n",
    "    outputs = model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS)\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "\n",
    "    latency = end - start\n",
    "    gen_tokens = outputs.size(1) - input_len\n",
    "    return latency, gen_tokens / latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e22e2ca-c38d-4342-ba01-48a84c6b0196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start():\n",
    "    device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    tokenizer, model = load_model_and_tokenizer(MODEL_NAME, device)\n",
    "\n",
    "    # Baseline\n",
    "    measure_baseline(model, tokenizer, PROMPT, PERP_TEXT, device)\n",
    "\n",
    "    # Prune on CPU\n",
    "    prune_mlp_rows_and_cols(model, MLP_PRUNE_FRAC)\n",
    "\n",
    "    # Rebuild smaller MLPs\n",
    "    rebuild_mlp_blocks(model)\n",
    "\n",
    "    # Re-benchmark rebuilt model\n",
    "    measure_rebuilt(model, tokenizer, PROMPT, PERP_TEXT, device)\n",
    "\n",
    "    # Save & report on-disk size\n",
    "    save_and_report_size(model, \"llama_pruned_rebuilt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27440f2e-e9ef-474f-9275-dd1a588ac57e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Baseline] latency   = 1.071s\n",
      "[Baseline] throughput= 46.7 tok/s\n",
      "[Baseline] peak GPU  = 2125.4 MiB\n",
      "[Baseline] perplexity= 4.557\n",
      "[Rebuilt] latency   = 1.075s\n",
      "[Rebuilt] throughput= 46.5 tok/s\n",
      "[Rebuilt] peak GPU  = 1465.4 MiB\n",
      "[Rebuilt] perplexity= 428383.216\n",
      "[Rebuilt] on-disk size = 1372.2 MiB\n"
     ]
    }
   ],
   "source": [
    "start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a2f565-2c3f-4250-a85c-44b8d22f09e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
