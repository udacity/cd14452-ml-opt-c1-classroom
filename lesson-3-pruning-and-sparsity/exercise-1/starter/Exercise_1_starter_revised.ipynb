{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "851cb58b-1175-45d8-9113-9869ca6cfed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d3b226d-9e0d-4ae9-984d-4e00e8006d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME     = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "MLP_PRUNE_FRAC = 0.5       # fraction of inner neurons to prune\n",
    "MAX_NEW_TOKENS = 50\n",
    "PROMPT = (\n",
    "    \"Over the next decade, sustainable energy solutions will revolutionize \"\n",
    "    \"global power grids, reducing carbon footprints and fostering resilient \"\n",
    "    \"communities through innovative storage and distribution technologies.\"\n",
    ")\n",
    "PERP_TEXT = (\n",
    "    \"Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast \"\n",
    "    \"to the natural intelligence displayed by humans and animals. Leading AI textbooks \"\n",
    "    \"define the field as the study of intelligent agents: any system that perceives \"\n",
    "    \"its environment and takes actions that maximize its chance of achieving its goals.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d36546f-6bbe-4d65-b952-2520b94ab986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name: str, device: torch.device):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Load AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "      - Load AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "      - Move model to `device` and set to .eval()\n",
    "      - Return tokenizer, model\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39e97ed4-3031-452c-b3d9-7d79afa84e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_baseline(model: nn.Module, tokenizer, prompt: str, perp_text: str, device: torch.device):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Warm up & measure generation latency & throughput on `prompt`\n",
    "      - Measure peak GPU memory & perplexity on `perp_text`\n",
    "      - Print or return these baseline metrics\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d94de27-8bb4-4110-84f5-b9cac02dc045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_mlp_rows_and_cols(model: nn.Module, prune_frac: float):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Move model to CPU\n",
    "      - For each layer in model.model.layers:\n",
    "          • Zero out `prune_frac` of rows in gate_proj and up_proj\n",
    "          • Zero out corresponding `prune_frac` of columns in down_proj\n",
    "      - Remove pruning reparameterizations\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d85619ae-1a0f-44f2-99f1-cb8cb3d7bf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_mlp_blocks(model: nn.Module):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - For each layer in model.model.layers:\n",
    "          1) Identify kept neuron indices in gate_proj\n",
    "          2) Construct new nn.Linear modules for gate_proj, up_proj, down_proj\n",
    "             with reduced dimensions\n",
    "          3) Copy over weights and biases\n",
    "          4) Replace the old modules on the model\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79f5c2f5-9541-42e1-8389-dcd1b216add7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_rebuilt(model: nn.Module, tokenizer, prompt: str, perp_text: str, device: torch.device):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - Move rebuilt model to `device` & .eval()\n",
    "      - Re-measure latency, throughput, peak memory, perplexity\n",
    "      - Print or return these metrics\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b5cde41-77ee-4339-a06c-6fe825e6ad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_report_size(model: nn.Module, output_dir: str):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      - model.save_pretrained(output_dir)\n",
    "      - Walk `output_dir` to sum file sizes (in MiB)\n",
    "      - Print the on-disk size\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c3803f-9b33-42b3-a1cf-0891f0ea130b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start():\n",
    "    device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    tokenizer, model = load_model_and_tokenizer(MODEL_NAME, device)\n",
    "\n",
    "    # Baseline\n",
    "    measure_baseline(model, tokenizer, PROMPT, PERP_TEXT, device)\n",
    "\n",
    "    # Prune on CPU\n",
    "    prune_mlp_rows_and_cols(model, MLP_PRUNE_FRAC)\n",
    "\n",
    "    # Rebuild smaller MLPs\n",
    "    rebuild_mlp_blocks(model)\n",
    "\n",
    "    # Re-benchmark rebuilt model\n",
    "    measure_rebuilt(model, tokenizer, PROMPT, PERP_TEXT, device)\n",
    "\n",
    "    # Save & report on-disk size\n",
    "    save_and_report_size(model, \"llama_pruned_rebuilt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2824316e-ff26-4dfa-b1f4-f7d3e4c3e2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
